{"result":{"history_size":1,"folders":["assignment1"],"nr":380,"data":{"embed_links":[]},"created":"2024-11-17T17:08:24Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m3lup1zaw7a2uf","v":"all","type":"create","when":"2024-11-17T17:08:24Z","uid_a":"a_0"},{"anon":"no","uid":"ktq3zcvidvn4rg","data":"m3lvvf09gkd7p4","to":"m3lup1z2vah2ue","type":"i_answer","when":"2024-11-17T17:41:20Z"},{"anon":"stud","to":"m3lup1z2vah2ue","type":"followup","when":"2024-11-17T19:16:22Z","cid":"m3lz9mplfdr5do","uid_a":"a_0"},{"anon":"stud","to":"m3lup1z2vah2ue","type":"feedback","when":"2024-11-17T19:27:28Z","cid":"m3lznw9jiea2k8","uid_a":"a_1"},{"anon":"stud","to":"m3lup1z2vah2ue","type":"feedback","when":"2024-11-17T19:52:58Z","cid":"m3m0konj7qx12w","uid_a":"a_0"},{"anon":"stud","to":"m3lup1z2vah2ue","type":"feedback","when":"2024-11-17T19:54:23Z","cid":"m3m0mib6d8n2zd","uid_a":"a_1"},{"anon":"stud","to":"m3lup1z2vah2ue","type":"feedback","when":"2024-11-17T19:56:07Z","cid":"m3m0oqpt6sf73c","uid_a":"a_0"},{"anon":"no","uid":"ktq3zcvidvn4rg","to":"m3lup1z2vah2ue","type":"feedback","when":"2024-11-18T01:58:34Z","cid":"m3mdmuvuplm5w0"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"LFM Implementation in Pytorch","created":"2024-11-17T17:08:24Z","content":"<md>I have tried to implement a LFM in Pytorch using the model prof suggested in class: $$(\\alpha + \\beta_u + \\beta_i + \\gamma_u\\gamma_i)$$. I believe the model is working, since it is correctly decreasing loss over training and validation. However, I cannot seem to get even the validation loss to below 1.5 regardless of how I am tuning my hyperparameters (dimension of latent factors, learning rate, epochs, batch size, regularization constants for biases & latent factors). I am stuck now not knowing how to progress. Any tips would be greatly appreciated.</md>"}],"type":"question","tags":["assignment1","student"],"tag_good":[],"unique_views":273,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-17T17:41:20Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ktq3zcvidvn4rg","subject":"","created":"2024-11-17T17:41:20Z","content":"<p>Validation loss is something that you&#39;d have no control over. What you are doing sounds correct to me.</p>\n<p></p>\n<p>Generally speaking, just make sure your range of parameters (1) makes sense (e.g., 1k latent factors is probably too much even without having to experiment with them, and (2) covers a wide enough hyper-parameter range.</p>\n<p></p>\n<p>If you&#39;d like to absolutely make sure your code is working, you can construct synthetic dataset where e.g., user always give a rating of 5 and see if your model fits that.</p>\n<p></p>\n<p></p>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m3lvvf02zgy7p3","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<md>Thanks for the reply. I have made sure that my code is working by testing the model on the synthetic dataset (it converges after one epoch, batch size 50000).\n\nCould you please give some hints on to how to conduct hyperparameters? It seems that if I set latent factors to 5 (as suggested by the prof to be relatively on the \"complex\" side), no matter how I change learning rate (i tested values ranging 1e-2 to 1e-4), batch size (256 ~ 50k), and lambdas (from 0 to 1e-2) I still cannot decrease the MSE. Is there a way that I can heuristically get a more concise range of hyperparameters?</md>","created":"2024-11-17T19:16:22Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Im not an instructor, but something like this beyond intuitively guessing and using common patterns from research is pretty difficult. Majority of people actually use stuff like random search, grid search, and bayesian optimization. If you have a much more complex model I believe the current go tos are bayesian optimization as well as hyper banding. Instead I personally recommend modifying your training process in other ways that may improve the result much more than hyper parameter tuning.","created":"2024-11-17T19:27:28Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m3lznw9jiea2k8","d-bucket":"Yesterday","updated":"2024-11-17T19:27:28Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<md>Thanks for your reply. Do you have any suggestions? I was following the recommended model given by the professor and unsure how I can alter the training process.</md>","created":"2024-11-17T19:52:58Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m3m0konj7qx12w","d-bucket":"Yesterday","updated":"2024-11-17T19:52:58Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Um tbh from what I’m reading creating a solid latent factor model and including his tips from lecture should be enough to clear the full credit baseline. So I would doublecheck your implementation and review lecture at this point.","created":"2024-11-17T19:54:23Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"student","name":"Andrew Yang","endorser":{"l7qhog9kgaf36y":1667584353,"lr01hfr4wzn4fn":1710967315},"admin":false,"photo":null,"id":"l7uh7swykhe24e","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"ta","name":"Zhouhang Xie","endorser":{},"admin":true,"photo":null,"id":"ktq3zcvidvn4rg","photo_url":null,"us":false,"facebook_id":null}],"uid_a":"a_1","children":[],"tag_good_arr":["l7uh7swykhe24e","ktq3zcvidvn4rg"],"id":"m3m0mib6d8n2zd","updated":"2024-11-18T01:56:40Z","config":{}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<md>Thanks. I’ll make sure to double check</md>","created":"2024-11-17T19:56:07Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m3m0oqpt6sf73c","d-bucket":"Yesterday","updated":"2024-11-17T19:56:07Z","config":{"editor":"md"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"(Also, just FYI I think 50K sounds like a really large batch size than normal, we frequently see numbers like 36 - 2048-ish in practice, not that it definitely won&#39;t work though)","created":"2024-11-18T01:58:34Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ktq3zcvidvn4rg","children":[],"tag_good_arr":[],"id":"m3mdmuvuplm5w0","updated":"2024-11-18T01:58:34Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m3lz9mplfdr5do","updated":"2024-11-18T01:58:34Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":0,"id":"m3lup1z2vah2ue","config":{"editor":"md","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":5,"num_favorites":3,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731986700939,"default_anonymity":"no"},"error":null,"aid":"m3nw5vgd7oa21"}