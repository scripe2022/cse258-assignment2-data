{"result":{"history_size":2,"folders":["hw3"],"nr":264,"data":{"embed_links":[]},"created":"2024-11-07T00:42:18Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m36l2etipuc6v3","v":"all","type":"create","when":"2024-11-07T00:42:18Z","uid_a":"a_0"},{"anon":"stud","data":"m36l3pk7an614l","v":"all","type":"update","when":"2024-11-07T00:43:19Z","uid_a":"a_0"},{"anon":"no","uid":"ktq3zcvidvn4rg","data":"m36lvahv4zs3b5","to":"m36l2et8lt06v1","type":"i_answer","when":"2024-11-07T01:04:46Z"},{"anon":"stud","to":"m36l2et8lt06v1","type":"followup","when":"2024-11-07T01:19:05Z","cid":"m36mdpk8jc6nu","uid_a":"a_0"},{"anon":"no","uid":"ktq3zcvidvn4rg","to":"m36l2et8lt06v1","type":"feedback","when":"2024-11-07T17:33:08Z","cid":"m37l6cff5xj7ek"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"HW3 Q6 gradient descent and regularization","created":"2024-11-07T00:43:19Z","content":"<p>I&#39;m trying to implement the gradient descent version of the predictor but am running into a problem while calculating theta.</p>\n<p></p>\n<p>I mostly followed the implementation from the Coursera slides titled &#34;gradient descent in Python&#34; from Week 2 but made some adjustments to the dtheta[k] updating equation to include regularization. When I run the code without the regularization, it works fine, which leads me to believe I don&#39;t know how to properly include regularization. </p>\n<p></p>\n<p>Given the following equation from the LEC slides on recommender systems:</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl871grkwo481d2%2F0f7bdc601446eed07116f85927b708349192d4092a5ed804b8ff4944ae458920%2Fimage.png\" alt=\"image.png\" /></p>\n<p>I proceeded to add &#34;&#43; 2*lamb*theta[k]&#34; to the existing update equation, but this results in theta[0] eventually updating to -(inf) and then all values of theta immediately become &#34;nan&#34;. I tried removing the X[i][k] multiplier from the error to make it exactly match the derivative above but that also didn&#39;t help and didn&#39;t seem to be the problem. </p>\n<p></p>\n<p>What am I doing wrong? Is there another way I&#39;m supposed to implement regularization and use the regularization parameter?   </p>"},{"anon":"stud","uid_a":"a_0","subject":"HW3 Q6 gradient descent and regularization","created":"2024-11-07T00:42:18Z","content":"<p>I&#39;m trying to implement the gradient descent version of the predictor but am running into a problem while calculating theta.</p>\n<p></p>\n<p>I mostly followed the implementation from the Coursera slides titled &#34;gradient descent in Python&#34; from Week 2 but made some adjustments to the dtheta[k] updating equation to include regularization. When I run the code without the regularization, it works fine, which leads me to believe I don&#39;t know how to properly include regularization. </p>\n<p></p>\n<p>Given the following equation from slides:</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl871grkwo481d2%2F0f7bdc601446eed07116f85927b708349192d4092a5ed804b8ff4944ae458920%2Fimage.png\" alt=\"image.png\" /></p>\n<p>I proceeded to add &#34;&#43; 2*lamb*theta[k]&#34; to the existing update equation, but this results in theta[0] eventually updating to -(inf) and then all values of theta immediately become &#34;nan&#34;. I tried removing the X[i][k] multiplier from the error to make it exactly match the derivative above but that also didn&#39;t help and didn&#39;t seem to be the problem. </p>\n<p></p>\n<p>What am I doing wrong? Is there another way I&#39;m supposed to implement regularization and use the regularization parameter?   </p>"}],"type":"question","tags":["hw3","student"],"tag_good":[],"unique_views":245,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-07T01:04:46Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ktq3zcvidvn4rg","subject":"","created":"2024-11-07T01:04:46Z","content":"<p>In case it helps: it&#39;d be easier to use modern machine learning frameworks (PyTorch/Tensorflow) that does automatic gradient upadtes!</p>\n<p></p>\n<p>See workbook for class for some examples (tensorflow): <a href=\"https://cseweb.ucsd.edu/~jmcauley/pml/code/chap5.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://cseweb.ucsd.edu/~jmcauley/pml/code/chap5.html</a></p>\n<p></p>\n<p>In general the homework shouldn&#39;t push you to worry too much about gradient updates beyond AutoGrad optimizers.</p>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m36lvahof653b4","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"I saw in other posts that there are issues regarding Tensorflow, is that no longer the case?","created":"2024-11-07T01:19:05Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"That is still the case, but my guess is the change required to make that code block work is minimal. ","created":"2024-11-07T17:33:08Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ktq3zcvidvn4rg","children":[],"tag_good_arr":[],"id":"m37l6cff5xj7ek","updated":"2024-11-07T17:33:08Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m36mdpk8jc6nu","updated":"2024-11-07T17:33:10Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m36l2et8lt06v1","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731986556584,"default_anonymity":"no"},"error":null,"aid":"m3nw2s2l54g1ns"}