{"result":{"history_size":1,"folders":["other"],"nr":107,"data":{"embed_links":[]},"created":"2024-10-21T00:49:23Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2iau1afia163b","v":"all","type":"create","when":"2024-10-21T00:49:23Z","uid_a":"a_0"},{"anon":"stud","to":"m2iau1a7nl63a","type":"followup","when":"2024-10-21T02:18:48Z","cid":"m2ie10l4owh3wt","uid_a":"a_1"},{"anon":"stud","to":"m2iau1a7nl63a","type":"feedback","when":"2024-10-21T02:34:32Z","cid":"m2iel9naxxu2pt","uid_a":"a_1"},{"anon":"no","uid":"ln0s9p097co3hc","data":"m2lc3qez3dv2xh","to":"m2iau1a7nl63a","type":"i_answer","when":"2024-10-23T03:48:14Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"One-Hot-Encoding","created":"2024-10-21T00:49:23Z","content":"<p>I remember in class, we talked about the case of one-hot-encoding, where if we have $$k$$ categories, we&#39;ll expand that feature out to $$k-1$$ features of 0s and a single 1. For example, if we had {male,female,other,not specified}, we used [0,0,0], [1,0,0], [0,1,0], [0,0,1]. The professor stressed the importance of not doing a feature vector of length $$k$$ because that would just be redundant.</p>\n<p></p>\n<p>However, right now I&#39;m also taking another ML class (CSE 151A), and that professor says that one-hot-encoding is like the following:</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl7uh6ra5917de%2F365b8c0a2a325f7a6f946f2c390fdc5fea5f89b68392e3b546217b4e74db61c7%2Fimage.png\" alt=\"image.pngNaN\" /> This seems to be redundant according to what was said in 158. I asked the 151 prof about this contradiction, and he said the following:</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl7uh6ra5917de%2F1045adf1bba3c570068a07d7ab11469b156818aed05651170e54d9da7c499bac%2Fimage.png\" alt=\"image.pngNaN\" /></p>\n<p>To be honest, I don&#39;t fully understand everything the 151 prof is saying. Could I perhaps get some clarification on the discrepancies between our class&#39;s one-hot-encoding versus the other class&#39;s one-hot-encoding?</p>"}],"type":"question","tags":["other","student"],"tag_good":[{"role":"student","name":"Eric Huang","endorser":{},"admin":false,"photo":null,"id":"l8c8ox501wl3i","photo_url":null,"us":false,"facebook_id":null}],"unique_views":163,"children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"I am also a bit unsure, since my previous understanding (I&#39;ve taken 152a, 152b, 151a) of one hot encoding is also consistent with what your CSE151A professor says.","created":"2024-10-21T02:18:47Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>Actually seems like it is just a different version of OHE used for regression:</p>\n<p></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Multicollinearity#Resolution\" target=\"_blank\" rel=\"noopener noreferrer\">https://en.wikipedia.org/wiki/Multicollinearity#Resolution</a></p>\n<p></p>\n<blockquote>\n<p>Similarly, including a <a href=\"https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29\">dummy variable</a> for every category (e.g., summer, autumn, winter, and spring) as well as an intercept term will result in perfect collinearity. This is known as the dummy variable trap</p>\n</blockquote>","created":"2024-10-21T02:34:32Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[{"role":"ta","name":"Sean O'Brien","endorser":{},"admin":true,"photo":null,"id":"ln0s9p097co3hc","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_1","children":[],"tag_good_arr":["ln0s9p097co3hc"],"id":"m2iel9naxxu2pt","d-bucket":"Yesterday","updated":"2024-10-22T18:50:08Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m2ie10l4owh3wt","d-bucket":"Yesterday","updated":"2024-10-22T18:50:17Z","config":{"editor":"rte"}},{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-23T03:48:14Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0s9p097co3hc","subject":"","created":"2024-10-23T03:48:14Z","content":"<md>Student answer below is correct. For the sake of regression, adding an extra dimension makes points collinear. For nonlinear functions (e.g. neural networks) typically we don't care about this and so it's common to encode one dimension for each category even though it is redundant. This leads to a discrepancy in practice.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2lc3qetdmr2xg","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["l8c8ox501wl3i"],"no_answer":0,"id":"m2iau1a7nl63a","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731986360277,"default_anonymity":"no"},"error":null,"aid":"m3nvyklkdk7178"}