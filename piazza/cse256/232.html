{"result":{"history_size":1,"folders":["pa2"],"nr":232,"data":{"embed_links":[]},"created":"2024-10-23T21:10:51Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2mdcjy9xfq11y","v":"all","type":"create","when":"2024-10-23T21:10:51Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2mf18nx6mw1n5","to":"m2mdcjy1kot11x","type":"i_answer","when":"2024-10-23T21:58:02Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Encoder","created":"2024-10-23T21:10:51Z","content":"&#34;Encoder Implementation: Your first task is to implement the transformer encoder following the hyperparameters defined inmain.py. The output of the encoder is the sequence of embeddings for each word in the inputsentence. To provide the embeddings to the classifier, use the mean of the embeddings across thesequence dimension. Later, in Part 3, you can experiment with different ways of providing theembeddings, such as using the[CLS]token&#34;<br /><br />for this encoder part, does it mean the whole encoder block(including n_heads attention) or just the positional encoder?<br />thanks"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":162,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-23T21:58:02Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-23T21:58:02Z","content":"<md>You need to implement the whole encoder block.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2mf18nrcwb1n4","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2mdcjy1kot11x","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990235031,"default_anonymity":"no"},"error":null,"aid":"m3ny9mdmjj7vb"}