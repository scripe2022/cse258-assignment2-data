{"result":{"history_size":1,"folders":["pa2"],"nr":307,"data":{"embed_links":[]},"created":"2024-10-30T20:38:27Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2wc9unv3mo1oy","v":"all","type":"create","when":"2024-10-30T20:38:27Z","uid_a":"a_0"},{"anon":"stud","data":"m2wdtmuzexs2th","to":"m2wc9un6s881ox","type":"s_answer","when":"2024-10-30T21:21:50Z","uid_a":"a_0"},{"anon":"stud","to":"m2wc9un6s881ox","type":"followup","when":"2024-10-30T21:22:14Z","cid":"m2wdu5xsnqe3md","uid_a":"a_0"},{"anon":"stud","data":"m2wduovov3m4dy","type":"s_answer_update","when":"2024-10-30T21:22:39Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2wf7s7fch24eu","to":"m2wc9un6s881ox","type":"i_answer","when":"2024-10-30T22:00:49Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2wf98oino168y","type":"i_answer_update","when":"2024-10-30T22:01:57Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"How to translate &#34;attention is all you need&#34; encoder for classifier?","created":"2024-10-30T20:38:27Z","content":"<p>Hi,</p>\n<p></p>\n<p>I understand that I will first convert a sentence into it&#39;s embedding (64 dim positional &#43; vocab based) to get a 16 x 32 x 64 dim tensor as input (batch size x sentence len x embedding dim). This is passed onto the 2 different attention heads which return the attention matrix * V result (16 x 32 x 32) as embedding dim is halved due to presence of 2 heads. In multihead these outputs are concatenated to form 16 x 32 x 64 tensor again.</p>\n<p></p>\n<p>I add the output of multi head attention block to initial x (residual), take the mean across all words in each sentence to get 16 x 64 tensor (each sentence represented by mean of the embeddings of it&#39;s words) and this is then passed into the feed forward classifier. The output of the classifier is 16 x 3 after softmax (probability of being each of 3 classes for each sentence in the batch).</p>\n<p></p>\n<p>main input is pre-normed before multi head attention block and feed forward classifier.Â </p>\n<p></p>\n<p>My questions:</p>\n<p>1. The attention is all you need paper adds a residual connection to this final output, but this has been reduced to 3 dims so how do we add the residual?</p>\n<p>2. Does this seem like the correct flow of the code? Especially in terms of the dimensions of x at each stage?</p>\n<p>3. For the pre-LN, do we add the LN(x) as residual or plain x? for example, multi head attention gets LN(x) as input, then the output gets a residual that is passed into feed forward classifier. Is this residual also pre-LN?</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":151,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-30T21:21:50Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"","created":"2024-10-30T21:22:39Z","content":"resubmitted in follow up discussions"},{"anon":"stud","uid_a":"a_0","subject":"","created":"2024-10-30T21:21:50Z","content":"Also as a follow, if there are supposed to be 4 layers in the transformer block does that mean 4 encoder blocks, where each encoder block is the multi head attention -&gt; feedforward classifier block as per the paper? because then the output of each block is the softmax (3 probabilities) for each sentence, so I&#39;m not sure how this would feed into the next block?"}],"type":"s_answer","tag_endorse_arr":[],"children":[],"id":"m2wdtmur1wm2tf","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Also as a follow, if there are supposed to be 4 layers in the transformer block does that mean 4 encoder blocks, where each encoder block is the multi head attention -&gt; feedforward classifier block as per the paper? because then the output of each block is the softmax (3 probabilities) for each sentence, so I&#39;m not sure how this would feed into the next block?","created":"2024-10-30T21:22:14Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"no_answer":1,"id":"m2wdu5xsnqe3md","updated":"2024-10-30T21:22:14Z","config":{"editor":"rte"}},{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-30T22:00:49Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-30T22:01:57Z","content":"<md>I think you need to understand the flow first. You should average tensors across all words in the sentence at the end of the encoder (i.g., average the final output (16, 32, 64) of the encoder). See @245\n\nFix this first, then you'll get the other questions along.</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-30T22:00:49Z","content":"<md>I think you need to understand the flow first. You average tensors across all words in the sentence at the end of the encoder. See @245\n\nFix this first, then you'll get the other questions along.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2wf7s7amqy4et","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2wc9un6s881ox","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990313517,"default_anonymity":"no"},"error":null,"aid":"m3nybaxvwkw6rr"}