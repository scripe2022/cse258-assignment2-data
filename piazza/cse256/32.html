{"result":{"history_size":2,"folders":["pa1"],"nr":32,"data":{"embed_links":[]},"created":"2024-10-09T22:37:08Z","bucket_order":3,"no_answer_followup":3,"change_log":[{"anon":"stud","data":"m22g9lkj39w73p","v":"private","type":"create","when":"2024-10-09T22:37:08Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m22j3sbq2pb5zt","to":"m22g9lkc5yr73o","type":"i_answer","when":"2024-10-09T23:56:36Z"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m22g9lkc5yr73o","type":"followup","when":"2024-10-09T23:56:56Z","cid":"m22j47z9iwi6ob"},{"anon":"stud","to":"m22g9lkc5yr73o","type":"feedback","when":"2024-10-10T00:09:21Z","cid":"m22jk6gehf415i","uid_a":"a_0"},{"anon":"stud","data":"m22jl1smzdl2j6","v":"all","type":"update","when":"2024-10-10T00:10:01Z","uid_a":"a_0"},{"anon":"stud","to":"m22g9lkc5yr73o","type":"followup","when":"2024-10-14T00:02:05Z","cid":"m28928xn6b94n7","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m22g9lkc5yr73o","type":"feedback","when":"2024-10-14T00:17:53Z","cid":"m289mk7s56528h"},{"anon":"stud","to":"m22g9lkc5yr73o","type":"feedback","when":"2024-10-14T06:17:07Z","cid":"m28mgjtdv271lg","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m22g9lkc5yr73o","type":"feedback","when":"2024-10-14T06:45:11Z","cid":"m28ngmvt7cs2z7"},{"anon":"no","uid":"m182yszkkec4c4","to":"m22g9lkc5yr73o","type":"feedback","when":"2024-10-17T19:48:19Z","cid":"m2dpraws3d14ia"},{"anon":"stud","to":"m22g9lkc5yr73o","type":"followup","when":"2024-10-17T23:00:23Z","cid":"m2dwmb2nykd50s","uid_a":"a_2"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m22g9lkc5yr73o","type":"feedback","when":"2024-10-18T00:29:53Z","cid":"m2dztekt5505tc"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"BPE expected accuracy","created":"2024-10-10T00:10:01Z","content":"<p>So I was just testing my bpe accuracy but I am hardly cracking 50%. Is there a minimum expected accuracy?<br />Also are there any common bugs that could be my problem.</p>\n<p>Thanks :)</p>"},{"anon":"stud","uid_a":"a_0","subject":"BPE expected accuracy","created":"2024-10-09T22:37:08Z","content":"<p>So I was just testing my bpe accuracy but I am hardly cracking 50%. Is there a minimum expected accuracy?<br />Also are there any common bugs that could be my problem.</p>\n<p>Thanks :)</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[{"role":"student","name":"Madurya Suresh","endorser":{},"admin":false,"photo":null,"id":"m182yu2idsy4f6","photo_url":null,"us":false,"facebook_id":null}],"unique_views":245,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-09T23:56:36Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Nikhil Gautam","endorser":{},"admin":false,"photo":null,"id":"m182yqdoa2644c","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-09T23:56:36Z","content":"For Part 2 (BPE), if your dev accuracy is below 70%, it’s highly possible there could be bugs in your implementation. Additionally, checking out this <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE&amp;ab_channel=AndrejKarpathy\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube tutorial</a> on BPE tokenizer might help clarify things and guide you through the process."}],"type":"i_answer","tag_endorse_arr":["m182yqdoa2644c"],"children":[],"id":"m22j3sbmn8s5zs","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Also, would you mind making this post public instead of private (you can choose to remain anonymous to your classmates)? Others may have similar questions, and it could be helpful for them as well. Thanks!","created":"2024-10-09T23:56:56Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid":"ln0md59uz9w3kd","children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"By all means","created":"2024-10-10T00:09:21Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[{"role":"ta","name":"Po-Chun Wu","endorser":{},"admin":true,"photo":null,"id":"ln0md59uz9w3kd","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_0","children":[],"tag_good_arr":["ln0md59uz9w3kd"],"id":"m22jk6gehf415i","updated":"2024-10-10T00:10:35Z","config":{}}],"tag_good_arr":[],"no_answer":1,"id":"m22j47z9iwi6ob","updated":"2024-10-10T00:09:21Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>for my BPE, the dev accuracy has been low, around 55-60%, no matter how I changed the vocabulary size (276-500), and embedding length (50-300), number of hidden layers (1-3) . I also implemented the decoding algorithm to check that my encoding algorithm is correct. And since after encoding then decoding it, I got the same string as the input string, I assume that the BPE is correctly implemented.</p>\n<p></p>\n<p>Do you have any other suggestions on what I can do to make my model perform better?</p>\n<p></p>\n<p>Thank you!</p>","created":"2024-10-14T00:02:05Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Perhaps try a larger vocabulary size—take a look at the GloVe vocabulary from Part 1. To achieve similar results, you might want to use similar vocabulary sizes. Also, it may take a few minutes depending on the time efficiency of your code, but there&#39;s no need to train BPE for more than 20 minutes.","created":"2024-10-14T00:17:53Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":["m182ygrlgf83d9"],"id":"m289mk7s56528h","updated":"2024-10-14T00:20:40Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>Thank you for providing the direction! I have looked up the vocabulary size for the GloVe library, and tried the size of 6000-16000, and the embedding length was 50-300, but I am still stuck around 55-60% for the dev accuracy. (the training accuracy can achieve around 95% after 100 epochs in these cases)</p>\n<p></p>\n<p>Does this mean that there should be a bug in my BPE? (But after using my algorithm to encode then decode a string, I got the same string as the input string.) Or are there other directions I should try out?</p>\n<p></p>\n<p>Thank you very much for your help!</p>","created":"2024-10-14T06:17:07Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m28mgjtdv271lg","updated":"2024-10-14T06:17:07Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>It seems like there might be bugs elsewhere in the code. I recommend printing out some intermediate values (e.g., input to the forward function, embeddings, average vectors, and layer outputs) to see if they make sense and align with your expectations.</p>\n<p></p>\n<p>Sometimes small issues are hard to spot, but with patience and careful debugging, you&#39;ll get there. Good luck!</p>","created":"2024-10-14T06:45:11Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":["m182ygrlgf83d9"],"id":"m28ngmvt7cs2z7","updated":"2024-10-15T05:46:19Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>Hi @Po-Chun Wu, I don&#39;t understand the rationale behind your approach. It seems that using very large vocabulary sizes defeats the core purpose of BPE. Based on what I understood from lectures and elsewhere, the idea behind BPE is that,</p>\n<p>1. It allows for a much smaller vocabulary size, thereby speeding up training times and freeing up computational resources.</p>\n<p>2. It allows you to better handle out-of-vocabulary words by breaking them into smaller subwords (which would have otherwise been treated as UNK in word-level vocabulary). For eg. the word &#34;unhappiness&#34; might be split into &#34;un,&#34; &#34;happi,&#34; and &#34;ness.&#34; These subwords are likely to be more common than the word &#39;unhappiness&#39; itself.</p>\n<p></p>\n<p>Now, if you were to do too many merges and increase the vocabulary size to lengths comparable to GloVe, your subword vocabulary starts morphing into a word-level vocabulary as the code starts over merging subwords into entire words. Then you slowly start losing all the benefits that BPE was bringing to the table.</p>\n<p>1. Large vocabulary - no computational advantage</p>\n<p>2. As overmerging occurs, most words are merged into full words, and the model’s ability to generalize and break down rare or unknown words (out-of-vocabulary words) is diminished</p>\n<p></p>\n<p>So, I&#39;m not sure that trading off these benefits for accuracy makes sense</p>","created":"2024-10-17T19:48:19Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"student","name":"Sarvani Kunapareddy","endorser":{},"admin":false,"photo":"a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","id":"m182yor466x3zz","photo_url":"https://cdn-uploads.piazza.com/photos/m182yor466x3zz/a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","published":true,"us":false,"facebook_id":null},{"role":"student","name":"Mayank Jain","endorser":{"lr6p20ijvfq6cy":1707263336,"lqtw1z4zol5ta":1710803157,"lu0hqdarqgca8":1714098279,"global":1715052269},"admin":false,"photo":null,"id":"ln0saam519h5bc","photo_url":null,"us":false,"facebook_id":null},{"role":"student","name":"Kuber Shahi","endorser":{},"admin":false,"photo":null,"id":"m182yq2hwpk43u","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"m182yszkkec4c4","children":[],"tag_good_arr":["m182yor466x3zz","ln0saam519h5bc","m182yq2hwpk43u"],"id":"m2dpraws3d14ia","updated":"2024-10-18T07:14:23Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m28928xn6b94n7","d-bucket":"Yesterday","updated":"2024-10-17T19:48:19Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"I am testing my BPE model and I am not able to achieve a dev accuracy of over 60%. I have been debugging to check my tokenizer and all other functions but I am not finding any issue. Do we need to achieve a certain accuracy to get full credit for number 2?","created":"2024-10-17T23:00:23Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_2","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Partial credit will still be given based on your implementation and report, even if you don&#39;t achieve 70% dev accuracy for Part 2 BPE.","created":"2024-10-18T00:29:53Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m2dztekt5505tc","updated":"2024-10-18T00:29:53Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m2dwmb2nykd50s","updated":"2024-10-18T00:29:53Z","config":{"editor":"rte"}}],"tag_good_arr":["m182yu2idsy4f6"],"no_answer":0,"id":"m22g9lkc5yr73o","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":5,"num_favorites":3,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990031369,"default_anonymity":"no"},"error":null,"aid":"m3ny598cdnu7jp"}