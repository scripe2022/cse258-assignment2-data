{"result":{"history_size":1,"folders":["pa2"],"nr":321,"data":{"embed_links":[]},"created":"2024-10-31T22:16:01Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2xv760tspf7ny","v":"all","type":"create","when":"2024-10-31T22:16:01Z","uid_a":"a_0"},{"anon":"no","uid":"m182yt8a22e4cr","data":"m2xvojmuc3573k","to":"m2xv760n3pk7nx","type":"s_answer","when":"2024-10-31T22:29:31Z"},{"anon":"stud","to":"m2xv760n3pk7nx","type":"followup","when":"2024-10-31T22:46:05Z","cid":"m2xw9u8lyyh4rw","uid_a":"a_0"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Doubt in PA2","created":"2024-10-31T22:16:01Z","content":"<p>The utilities function sanity_check has the following line of code,</p>\n<div>\n<div>total_prob_over_rows = torch.sum(attn_map[0], dim=1)<br /><br />But, this seems to be incorrect since the attn_map would be a 32*32 matrix and we do not have a dim=1 if we extract the first row. Hence, I changed it to this and the code seems to work as expected,</div>\n</div>\n<div></div>\n<div>\n<div>total_prob_over_rows = torch.sum(attn_map, dim=-1)<br /><br />Could you let me know if I this could be done?</div>\n</div>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Yufei Weng","endorser":{},"admin":false,"photo":null,"id":"k164ecv1yll6tf","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":166,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-31T22:29:31Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"ta","name":"Bosung Kim","endorser":{},"admin":true,"photo":null,"id":"ku2mpmjms7n645","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"m182yt8a22e4cr","subject":"","created":"2024-10-31T22:29:31Z","content":"<md>If I'm understanding the `sanity_check` code correctly, `attn_map` is meant to be a (1, 32, 32) tensor, where the dimensions correspond to the batches, rows, and columns, respectively.\nThis is why the for loop starts by removing the batch dimension (dimension index 0), creating a (32, 32) numpy array for the attention heatmap.\n\nNote however that the squeezed tensor is stored to a new variable `att_map` (no \"n\"), so `attn_map` remains a (1, 32, 32) tensor when the row sums are computed.\nThus, `attn_map[0]` selects the first (and only) batch, and dimension index 1 corresponds to the rows of the attention matrix.</md>"}],"type":"s_answer","tag_endorse_arr":["m182ygrlgf83d9","ku2mpmjms7n645"],"children":[],"id":"m2xvojmow2e73j","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Oh yeah, Thanks!","created":"2024-10-31T22:46:05Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"no_answer":1,"id":"m2xw9u8lyyh4rw","updated":"2024-10-31T22:46:05Z","config":{"editor":"rte"}}],"tag_good_arr":["k164ecv1yll6tf","kfsi52ar6572xo"],"no_answer":0,"id":"m2xv760n3pk7nx","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990328687,"default_anonymity":"no"},"error":null,"aid":"m3nybmn6d0934r"}