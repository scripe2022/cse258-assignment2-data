{"result":{"history_size":3,"folders":["pa2"],"nr":311,"data":{"embed_links":[]},"created":"2024-10-31T00:37:07Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"no","uid":"lxmbc110rzxfi","data":"m2wksrpkxz9qo","v":"all","type":"create","when":"2024-10-31T00:37:07Z"},{"anon":"stud","data":"m2wmrhw1c1o108","v":"all","type":"update","when":"2024-10-31T01:32:06Z","uid_a":"a_0"},{"anon":"no","uid":"lxmbc110rzxfi","data":"m2wmrq3dgj21bq","v":"all","type":"update","when":"2024-10-31T01:32:17Z"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m2wpshbxn3d5zc","to":"m2wksrpcyqqqn","type":"i_answer","when":"2024-10-31T02:56:51Z"},{"anon":"no","uid":"m182yt8a22e4cr","data":"m2wpwp1ussc6wv","to":"m2wksrpcyqqqn","type":"s_answer","when":"2024-10-31T03:00:08Z"},{"anon":"stud","to":"m2wksrpcyqqqn","type":"followup","when":"2024-10-31T16:16:40Z","cid":"m2xid1vyn793iw","uid_a":"a_1"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2wksrpcyqqqn","type":"feedback","when":"2024-10-31T18:36:32Z","cid":"m2xncxg4tk524c"}],"bucket_name":"Today","history":[{"anon":"no","uid":"lxmbc110rzxfi","subject":"Feedforward Classifier vs Encoder Feedforward","created":"2024-10-31T01:32:17Z","content":"Just wanted to clarify that the Feedforward classifier network is separate from the feedforward network that is a part of the transformer encoder layers."},{"anon":"stud","uid_a":"a_0","subject":"Feedforward Classifier vs Encoder Feedforward","created":"2024-10-31T01:32:06Z","content":"Just wanted to clarify that the Feedforward classifier network is separate from the feedforward network that is a part of the transformer encoder layers."},{"anon":"no","uid":"lxmbc110rzxfi","subject":"Feedforward Classifier vs Encoder Feedforward","created":"2024-10-31T00:37:07Z","content":"Just wanted to clarify that the Feedforward classifier network is separate from the feedforward network that is a part of the transformer encoder layers."}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":150,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-31T02:56:51Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-31T02:56:51Z","content":"Yeah, you are right. The feed forward network is a part of the Encoder layer and the feed forward classifier is the final block which comes after the Encoder part.Â "}],"type":"i_answer","tag_endorse_arr":["kfsi52ar6572xo"],"children":[],"id":"m2wpshbq6dy5za","config":{},"is_tag_endorse":false},{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-31T03:00:08Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"m182yt8a22e4cr","subject":"","created":"2024-10-31T03:00:08Z","content":"<md>Yes, you should also have a feedforward classifier that processes the output of the encoder:\n> For this part, you will train a transformer encoder __with a feedforward neural network classifier on top of it__, end-to-end.\n\nSee also @243 and @265</md>"}],"type":"s_answer","tag_endorse_arr":[],"children":[],"id":"m2wpwp1rioa6wu","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>I think I am getting confused by the contradiction in @265.</p>\n<p></p>\n<p><code>&#34;You only need one the feedforward classifier for the last Transformer layer. Average vectors over sequence length, e.g. the output of last Transformer layer: (16, 32, 64) then average across the sentence to get (16, 64). this is your sentence embedding for the classification task.&#34;</code></p>\n<p></p>\n<p>Does that mean that each of the encoder blocks (which does multi head attention) also need a feedforward linear layer? Or are we only implementing ONE feedforward layer which is the classifier after the 4 layers of multi head attention?</p>\n<p></p>\n<p>Also, if we ARE implementing the feedfoward linear layers after each multi head attention in the encoder block, I&#39;m a little confused about the dimensions.</p>\n<p>Are the input dims, hidden and output only for the last feed forward classifier (which is what I assumed)? If yes, what would be the dimensions of the feedforward layer after each multi head attention in each encoder layer?</p>\n<p>In Andrej&#39;s video he used 4 * embedding dim after the multi head attention in his encoder but I wasn&#39;t sure what we should do IF we also need to implement those in each encoder layer...</p>","created":"2024-10-31T16:16:40Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Each encoder blocks have a feedforward layer (blue block in the figure):\n\n![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fku2mpmjms7n645%2F477bb44bc647e7e5aeb0d0caa99963db785ff0870b2a00eb3479535143b033d5%2Fimage.png)\n\n> In Andrej's video he used 4 \\* embedding dim after the multi head attention \n\n--> this is correct.\n\nThe only difference between Encoder and Decoder in this PA is `masking`</md>","created":"2024-10-31T18:36:32Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2xncxg4tk524c","updated":"2024-10-31T18:36:32Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":1,"id":"m2xid1vyn793iw","updated":"2024-10-31T18:36:32Z","config":{"editor":"rte"}}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":0,"id":"m2wksrpcyqqqn","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":5,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990318566,"default_anonymity":"no"},"error":null,"aid":"m3nybeu3mha6y6"}