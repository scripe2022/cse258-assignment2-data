{"result":{"history_size":7,"folders":["pa2"],"nr":283,"data":{"embed_links":[]},"created":"2024-10-27T21:20:45Z","bucket_order":0,"no_answer_followup":5,"change_log":[{"anon":"no","uid":"ku2mpmjms7n645","data":"m2s3goyh8cb5q3","v":"all","type":"create","when":"2024-10-27T21:20:45Z"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"followup","when":"2024-10-28T00:03:03Z","cid":"m2s99etp2c661n","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-10-29T04:46:30Z","cid":"m2tyts92vefyg"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"followup","when":"2024-11-01T00:03:24Z","cid":"m2xz19gh94h54w","uid_a":"a_1"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-01T00:43:46Z","cid":"m2y0h6hpqh36z4","uid_a":"a_1"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2y461a0hxqx8","v":"all","type":"update","when":"2024-11-01T02:27:04Z"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-01T02:28:28Z","cid":"m2y47tjg6et3o1"},{"anon":"no","uid":"m182yt8a22e4cr","to":"m2s3goy9ckz5q2","type":"followup","when":"2024-11-01T14:43:01Z","cid":"m2yugh00xzx49g"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2yyezgijce23w","v":"all","type":"update","when":"2024-11-01T16:33:50Z"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-01T16:34:09Z","cid":"m2yyfdglh8p32n"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"followup","when":"2024-11-01T17:44:00Z","cid":"m2z0x7d659q3ef","uid_a":"a_1"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2z2jtqpts1g4","v":"all","type":"update","when":"2024-11-01T18:29:35Z"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-01T18:34:25Z","cid":"m2z2q2209j95mi"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T08:20:08Z","cid":"m2zw7xicobv6lj","uid_a":"a_1"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T08:31:33Z","cid":"m2zwmlu1u9l2pm","uid_a":"a_1"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"followup","when":"2024-11-02T17:51:08Z","cid":"m30gm8onlx91wa","uid_a":"a_2"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T18:45:47Z","cid":"m30ikielgx54yz"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T18:51:40Z","cid":"m30is3a8v8w3lo","uid_a":"a_2"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T18:53:45Z","cid":"m30iurdy53uhh","uid_a":"a_2"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T18:54:21Z","cid":"m30ivj5mdt94sv"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T19:54:26Z","cid":"m30l0szp6ub11x","uid_a":"a_2"},{"anon":"stud","to":"m2s3goy9ckz5q2","type":"feedback","when":"2024-11-02T23:30:51Z","cid":"m30sr4jpxoh4ng","uid_a":"a_1"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m30u1a4fox82bt","v":"all","type":"update","when":"2024-11-03T00:06:45Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m30u52rddhq79n","v":"all","type":"update","when":"2024-11-03T00:09:42Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m328i044dv94aq","v":"all","type":"update","when":"2024-11-03T23:39:26Z"}],"bucket_name":"Pinned","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"PA2 Clarification","created":"2024-11-03T23:39:26Z","content":"<ol><li>In Part 2, you only use Decoder. Encoder is not used in Part 2.</li></ol>\n<p>The decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project. Also see @247</p>\n<p></p>\n<ol start=\"2\"><li>In Part 2, too low perplexity means your causal masking is not correctly implemented.</li></ol>\n<p>Make sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test.</p>\n<p></p>\n<ol start=\"3\"><li>\n<p>In Part 1, encoder performance can be not stable due the the place of <code>LayerNorm</code>. There are two places to put the layer norm: pre-LN and post-LN, you can check out this <a href=\"https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx\">post</a> for details. Post-LN can lead to the instability.</p>\n<p></p>\n</li><li>\n<p>In Part 2, we found that if you put the nn.LayerNorm right before the <code>lm_head</code> layer (following the Kaparthy’s tutorial code) you could get much lower perplexity than the numbers in the instruction. This is the correct implementation, and we will not deduct any points because of this.</p>\n<p></p>\n</li><li>\n<p>In part 2, you need to report a total of 8 numbers</p>\n<p></p>\n</li></ol>\n<ul><li>iter 100 (on the training set)</li><li>iter 200 (on the training set)</li><li>iter 300 (on the training set)</li><li>iter 400 (on the training set)</li><li>iter 500 (on the training set)</li></ul>\n<p>For 3 test sets, final perplexity after 500 iters</p>\n<p></p>\n<ul><li>iter 500 (on the Obama)</li><li>iter 500 (on the H. Bush)</li><li>iter 500 (on the W. Bush)</li></ul>\n<p>(It is also acceptable with results at 200th iteration)</p>\n<p></p>\n<p></p>\n<p>6. Acceptable perplexity range (could be updated later)</p>\n<ul><li></li><li>Train Perplexity: 80~200<br />Obama Perplexity:  200~450  <br />HBush Perplexity:  200~500<br />WBush Perplexity:  200~550</li></ul>\n<p></p>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"PA2 Clarification","created":"2024-11-03T00:09:42Z","content":"<ol><li>In Part 2, you only use Decoder. Encoder is not used in Part 2.</li></ol>\n<p>The decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project. Also see @247</p>\n<p></p>\n<ol start=\"2\"><li>In Part 2, too low perplexity means your causal masking is not correctly implemented.</li></ol>\n<p>Make sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test.</p>\n<p></p>\n<ol start=\"3\"><li>\n<p>In Part 1, encoder performance can be not stable due the the place of <code>LayerNorm</code>. There are two places to put the layer norm: pre-LN and post-LN, you can check out this <a href=\"https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx\">post</a> for details. Post-LN can lead to the instability.</p>\n<p></p>\n</li><li>\n<p>In Part 2, we found that if you put the nn.LayerNorm right before the <code>lm_head</code> layer (following the Kaparthy’s tutorial code) you could get much lower perplexity than the numbers in the instruction. This is the correct implementation, and we will not deduct any points because of this.</p>\n<p></p>\n</li><li>\n<p>In part 2, you need to report a total of 8 numbers</p>\n<p></p>\n</li></ol>\n<ul><li>iter 100 (on the training set)</li><li>iter 200 (on the training set)</li><li>iter 300 (on the training set)</li><li>iter 400 (on the training set)</li><li>iter 500 (on the training set)</li></ul>\n<p>For 3 test sets, final perplexity after 500 iters</p>\n<p></p>\n<ul><li>iter 500 (on the Obama)</li><li>iter 500 (on the H. Bush)</li><li>iter 500 (on the W. Bush)</li></ul>\n<p>(It is also acceptable with results at 200th iteration)</p>\n<p></p>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"PA2 Clarification","created":"2024-11-03T00:06:45Z","content":"<md>1. In Part 2, you only use Decoder. Encoder is not used in Part 2.\n\nThe decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project. Also see @247\n\n2. In Part 2, too low perplexity means your causal masking is not correctly implemented.\n\nMake sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test.\n\n3. In Part 1, encoder performance can be not stable due the the place of `LayerNorm`. There are two places to put the layer norm: pre-LN and post-LN, you can check out this [post](https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx) for details. Post-LN can lead to the instability.\n\n\n4.  In Part 2, we found that if you put the nn.LayerNorm right before the `lm_head` layer (following the Kaparthy’s tutorial code) you could get much lower perplexity than the numbers in the instruction. This is the correct implementation, and we will not deduct any points because of this.\n\n5. In part 3, you need to report a total of 8 numbers\n\n*   iter 100 (on the training set)\n*   iter 200 (on the training set)\n*   iter 300 (on the training set)\n*   iter 400 (on the training set)\n*   iter 500 (on the training set)\n\nFor 3 test sets, final perplexity after 500 iters\n\n*   iter 500 (on the Obama)\n*   iter 500 (on the H. Bush)\n*   iter 500 (on the W. Bush)\n\n(It is also acceptable with results at 200th iteration)</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"PA2 Clarification","created":"2024-11-01T18:29:35Z","content":"<md>1. In Part 2, you only use Decoder. Encoder is not used in Part 2.\n\nThe decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project. Also see @247\n\n2. In Part 2, too low perplexity means your causal masking is not correctly implemented.\n\nMake sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test.\n\n3. In Part 1, encoder performance can be not stable due the the place of `LayerNorm`. There are two places to put the layer norm: pre-LN and post-LN, you can check out this [post](https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx) for details. Post-LN can lead to the instability.\n\n\n4.  In Part 2, we found that if you put the nn.LayerNorm right before the `lm_head` layer (following the Kaparthy’s tutorial code) you could get much lower perplexity than the numbers in the instruction. This is the correct implementation, and we will not deduct any points because of this.\n\n5. In part 3, you need to report a total of 8 numbers\n\n*   iter 100 (on the training set)\n*   iter 200 (on the training set)\n*   iter 300 (on the training set)\n*   iter 400 (on the training set)\n*   iter 500 (on the training set)\n\nFor 3 test sets, final perplexity after 200 iters\n\n*   iter 200 (on the Obama)\n*   iter 200 (on the H. Bush)\n*   iter 200 (on the W. Bush)</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"PA2 Clarification","created":"2024-11-01T16:33:50Z","content":"<md>1. In Part 2, you only use Decoder. Encoder is not used in Part 2.\n\nThe decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project. Also see @247\n\n2. In Part 2, too low perplexity means your causal masking is not correctly implemented.\n\nMake sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test.\n\n3. In Part 1, encoder performance can be not stable due the the place of `LayerNorm`. There are two places to put the layer norm: pre-LN and post-LN, you can check out this [post](https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx) for details. Post-LN can lead to the instability.\n\n\n4.  In Part 2, we found that if you put the nn.LayerNorm right before the `lm_head` layer (following the Kaparthy’s tutorial code) you could get much lower perplexity than the numbers in the instruction. This is the correct implementation, and we will not deduct any points because of this.\n\n5. In part 3, you need to report a total of 8 numbers\n\n*   iter 100 (on the training set)\n*   iter 200 (on the training set)\n*   iter 300 (on the training set)\n*   iter 400 (on the training set)\n*   iter 500 (on the training set)\n\nFor 3 test sets, final perplexity after 500 iters\n\n*   iter 200 (on the Obama)\n*   iter 200 (on the H. Bush)\n*   iter 200 (on the W. Bush)</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"PA2 Clarification","created":"2024-11-01T02:27:04Z","content":"<md>1. In Part 2, you only use Decoder. Encoder is not used in Part 2.\n\nThe decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project. Also see @247\n\n2. In Part 2, too low perplexity means your causal masking is not correctly implemented.\n\nMake sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test.\n\n3. In Part 1, encoder performance can be not stable due the the place of `LayerNorm`. There are two places to put the layer norm: pre-LN and post-LN, you can check out this [post](https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx) for details. Post-LN can lead to the instability.\n\n\n4.  In Part 2, we found that if you put the nn.LayerNorm right before the `lm_head` layer (following the Kaparthy’s tutorial code) you could get much lower perplexity than the numbers in the instruction. This is the correct implementation, and we will not deduct any points because of this.\n\n5. In part 3, you need to report a total of 8 numbers\n\n*   iter 100 (on the training set)\n*   iter 200 (on the training set)\n*   iter 300 (on the training set)\n*   iter 400 (on the training set)\n*   iter 500 (on the training set)\n\nFor 3 test sets, final perplexity after 500 iters\n\n*   iter 500 (on the Obama)\n*   iter 500 (on the H. Bush)\n*   iter 500 (on the W. Bush)</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"PA2 Clarification","created":"2024-10-27T21:20:45Z","content":"<md>1. In Part 2, you only use Decoder. Encoder is not used in Part 2.\n\nThe decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project. Also see @247\n\n2. In Part 2, too low perplexity means your causal masking is not correctly implemented.\n\nMake sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test.\n\n3. In Part 1, encoder performance can be not stable due the the place of `LayerNorm`. There are two places to put the layer norm: pre-LN and post-LN, you can check out this [post](https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx) for details. Post-LN can lead to the instability.\n\n\n4.  In Part 2, we found that if you put the nn.LayerNorm right before the `lm_head` layer (following the Kaparthy’s tutorial code) you could get much lower perplexity than the numbers in the instruction. This is the correct implementation, and we will not deduct any points because of this.</md>"}],"type":"note","tags":["instructor-note","pa2","pin"],"tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Akshaya Thenkarai Lakshminarasimhan","endorser":{},"admin":false,"photo":"a279cbe3-5f08-48df-8ab5-60a1ba1c8994_200.jpg","id":"m182yjodigv3la","photo_url":"https://cdn-uploads.piazza.com/photos/m182yjodigv3la/a279cbe3-5f08-48df-8ab5-60a1ba1c8994_200.jpg","published":true,"us":false,"facebook_id":null},{"role":"student","name":"Madurya Suresh","endorser":{},"admin":false,"photo":null,"id":"m182yu2idsy4f6","photo_url":null,"us":false,"facebook_id":null},{"role":"student","name":"Sai Siri Chinta","endorser":{},"admin":false,"photo":null,"id":"m182yohq6nu3yz","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Rami Altai","endorser":{},"admin":false,"photo":null,"id":"lml4g3u9lv41ee","photo_url":null,"us":false,"facebook_id":null}],"unique_views":328,"children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"4. So how low is fine? I am getting 200s for bush and other datasets.","created":"2024-10-28T00:03:03Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[{"role":"student","name":"Zheng Zeng","endorser":{},"admin":false,"photo":null,"id":"l4orh8d3v0754v","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>200 is good</md>","created":"2024-10-29T04:46:30Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2tyts92vefyg","updated":"2024-10-29T04:46:30Z","config":{"editor":"md"}}],"tag_good_arr":["l4orh8d3v0754v"],"no_answer":1,"id":"m2s99etp2c661n","updated":"2024-10-29T04:46:30Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>After implementing pre-LN, I am still getting different accuracies every time, but the difference is around 1-2%. Is this acceptable for the assignment? Or do we have to make sure the accuracy is exactly the same all the time?</p>\n<p></p>\n<p>Thank you!</p>","created":"2024-11-01T00:03:24Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Rami Altai","endorser":{},"admin":false,"photo":null,"id":"lml4g3u9lv41ee","photo_url":null,"us":false,"facebook_id":null}],"uid_a":"a_1","children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"also, for the perplexity, there&#39;s also a bit difference in each run even if I used pre-LN. I wonder what is the acceptable range for the difference. Thank you!","created":"2024-11-01T00:43:46Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m2y0h6hpqh36z4","updated":"2024-11-01T00:43:46Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>1-2% difference is okay! We understand there is a randomness in experiments, you don't need to worry about that.\n\nYes, it is acceptable too.</md>","created":"2024-11-01T02:28:28Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"student","name":"Rami Altai","endorser":{},"admin":false,"photo":null,"id":"lml4g3u9lv41ee","photo_url":null,"us":false,"facebook_id":null}],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":["lml4g3u9lv41ee"],"id":"m2y47tjg6et3o1","updated":"2024-11-06T07:49:25Z","config":{"editor":"md"}}],"tag_good_arr":["kfsi52ar6572xo","lml4g3u9lv41ee"],"no_answer":1,"id":"m2xz19gh94h54w","updated":"2024-11-06T07:49:29Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<md>> For 3 test sets, final perplexity after 500 iters\n\nThe `main.py` file sets `eval_iters` to 200:\n```py\neval_iters = 200  # Number of iterations to evaluate perplexity on the test set\n```\n\nso shouldn't the final perplexity for the test sets be after 200 iters, not 500 iters?</md>","created":"2024-11-01T14:43:01Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"m182yt8a22e4cr","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Updated. Thanks for the heads up!</md>","created":"2024-11-01T16:34:08Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2yyfdglh8p32n","updated":"2024-11-01T16:34:08Z","config":{"editor":"md"}}],"tag_good_arr":["m182ygrlgf83d9"],"no_answer":1,"id":"m2yugh00xzx49g","updated":"2024-11-01T16:34:08Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>Sorry I don&#39;t quite understand point 5,</p>\n<p>&#34;&#34;&#34;</p>\n<p>For 3 test sets, final perplexity after 500 iters</p>\n<ul><li>iter 200 (on the Obama)</li><li>iter 200 (on the H. Bush)</li><li>iter 200 (on the W. Bush)</li></ul>\n<p>&#34;&#34;&#34;</p>\n<p></p>\n<p>isn&#39;t the perplexity final after running 500 iterations? If so, how can we get the final perplexity for the decoder if we just finish running 200 iterations?</p>\n<p></p>\n<p>Thanks for clarifying!</p>","created":"2024-11-01T17:44:00Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Sorry, I updated: 500 -> 200\n\nAs mentioned in main.py, the test set should be evaluated on the 200-th iter\n\n\n![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fku2mpmjms7n645%2F933ecb6f9977486c8ebc6a82db12e055081ec2f7451e9e771ba2f56e1bfd6abe%2Fimage.png)\n\nyou can do something like this\n\n```\nfor i, (xb, yb) in enumerate(train_LM_loader):\n    if i == 200:\n       compute_perplexity(your_model, test_data_loader)\n```</md>","created":"2024-11-01T18:34:25Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2z2q2209j95mi","updated":"2024-11-01T18:34:25Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>Hi, I think I am a bit confused about the definition of eval_interval and eval_iters. According to the comment of eval_interval (which is 100 in this case), it seems that it should be used here:</p>\n<pre>\n<code>for i, (xb, yb) in enumerate(train_LM_loader):\n    if (i &#43; 1) % eval_interval:\n\t\t\t compute_perplexity(your_model, training_data)\n       compute_perplexity(your_model, test_data_loader)</code></pre>\n<p>because eval_interval is &#34;how often to evaluate train and test perplexity during training&#34;</p>\n<p></p>\n<p>and eval_iters should probably be used in compute_perplexity() function because the function literally has the keyword parameter called &#34;eval_iter&#34;. And it&#39;s used here:</p>\n<pre>\ndef compute_perplexity(decoderLMmodel, data_loader, eval_iters=100):\n    decoderLMmodel.eval()\n    losses = []\n    for X, Y in data_loader:\n        ...\n        if len(losses) &gt;= eval_iters: # here\n            break\n\t\t...\n\n    decoderLMmodel.train()\n    return perplexity</pre>\n<p>because eval_iters is &#34;Number of iterations to evaluate perplexity on the test set&#34;, which is exactly what it is doing in the compute_perplexity.</p>\n<div>\n<div></div>\nIs my understanding correct?</div>\n<p></p>","created":"2024-11-02T08:20:08Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m2zw7xicobv6lj","updated":"2024-11-02T08:20:08Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>The assignment instruction says, &#34;In your report, include the final perplexity, as well as the perplexity after every 100 iterations, and after completing all 500 iterations. Additionally, report the number of parameters in your decoder.&#34;</p>\n<p></p>\n<p>I think this contradicts with point 5. Point 5 seems to ask us to return one perplexity for each of the test set (so total is 3), while the homework instruction asks us to return five perplexity (defined by interval = 100 and max_iters is 500), for each of the test set (so total is 15). Can you clarify on how many perplexities we should report for part 2.4?</p>\n<p></p>\n<p>Thank you!</p>","created":"2024-11-02T08:31:33Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[{"role":"student","name":"Nikita Kachappilly","endorser":{},"admin":false,"photo":null,"id":"m182yixnjz43iv","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_1","children":[],"tag_good_arr":["m182yixnjz43iv"],"id":"m2zwmlu1u9l2pm","updated":"2024-11-02T17:28:27Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>8 or 15 both would be good enough.\n\nFor this,\n> the perplexity after every 100 iterations \n\nwe just want to see your overall training process is correct or not.\n\nThere's no restriction, you can put all the results on the report as much as you can.</md>","created":"2024-11-02T18:54:21Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m30ivj5mdt94sv","updated":"2024-11-02T18:54:21Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>Hi, I wonder whether my understanding mentioned in the second comment of the thread is correct:</p>\n<p></p>\n<p>Hi, I think I am a bit confused about the definition of eval_interval and eval_iters. According to the comment of eval_interval (which is 100 in this case), it seems that it should be used here:</p>\n<pre>\n<code>for i, (xb, yb) in enumerate(train_LM_loader):\n    if (i &#43; 1) % eval_interval:\n\t\t\t compute_perplexity(your_model, training_data)\n       compute_perplexity(your_model, test_data_loader)</code></pre>\n<p>because eval_interval is &#34;how often to evaluate train and test perplexity during training&#34;</p>\n<p></p>\n<p>and eval_iters should probably be used in compute_perplexity() function because the function literally has the keyword parameter called &#34;eval_iter&#34;. And it&#39;s used here:</p>\n<pre>\ndef compute_perplexity(decoderLMmodel, data_loader, eval_iters=100):\n    decoderLMmodel.eval()\n    losses = []\n    for X, Y in data_loader:\n        ...\n        if len(losses) &gt;= eval_iters: # here\n            break\n\t\t...\n\n    decoderLMmodel.train()\n    return perplexity</pre>\n<p>because eval_iters is &#34;Number of iterations to evaluate perplexity on the test set&#34;, which is exactly what it is doing in the compute_perplexity.</p>\n<p></p>\n<p>Thank you!</p>","created":"2024-11-02T23:30:51Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"CHERYL STANLEY","endorser":{},"admin":false,"photo":null,"id":"m10qfbzbhan2hx","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_1","children":[],"tag_good_arr":["m182ygrlgf83d9","m10qfbzbhan2hx"],"id":"m30sr4jpxoh4ng","updated":"2024-11-03T10:42:24Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m2z0x7d659q3ef","updated":"2024-11-02T23:30:51Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"What perplexities should we be expecting for the test data with AliBi implemented correctly?","created":"2024-11-02T17:51:08Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid_a":"a_2","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>There is no expected numbers for Part 3, but should be a little better or similar to Part 1/Part 2. Too lower performance could be regarded wrong implementation.</md>","created":"2024-11-02T18:45:47Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m30ikielgx54yz","updated":"2024-11-02T18:45:47Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>I am getting single digit perplexities for the test sets. Is this considered too low performance?<br /><br /></p>\n<div>Iteration 100, Loss: 6.8094, Perplexity: 906.32</div>\n<div>Iteration 200, Loss: 5.9198, Perplexity: 372.33</div>\n<div>Iteration 300, Loss: 5.3615, Perplexity: 213.04</div>\n<div>Iteration 400, Loss: 4.9389, Perplexity: 139.62</div>\n<div>Iteration 500, Loss: 4.5416, Perplexity: 93.84</div>\n<div></div>\n<div>Training on speechesdataset/test_LM_obama.txt...</div>\n<div>Iteration 100, Loss: 4.4440, Perplexity: 85.11</div>\n<div>Iteration 200, Loss: 3.0694, Perplexity: 21.53</div>\n<div>Perplexity on speechesdataset/test_LM_obama.txt after training: 13.04</div>\n<div></div>\n<div>Training on speechesdataset/test_LM_wbush.txt...</div>\n<div>Iteration 100, Loss: 4.2566, Perplexity: 70.57</div>\n<div>Iteration 200, Loss: 2.7856, Perplexity: 16.21</div>\n<div>Perplexity on speechesdataset/test_LM_wbush.txt after training: 9.36</div>\n<div></div>\n<div>Training on speechesdataset/test_LM_hbush.txt...</div>\n<div>Iteration 100, Loss: 4.2496, Perplexity: 70.08</div>\n<div>Iteration 200, Loss: 2.6717, Perplexity: 14.46</div>\n<div>Perplexity on speechesdataset/test_LM_hbush.txt after training: 8.19</div>","created":"2024-11-02T18:51:40Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_2","children":[],"tag_good_arr":[],"id":"m30is3a8v8w3lo","updated":"2024-11-02T18:51:40Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Just to clarify, we should run the test set after the 200th iteration after training? Just wondering why don&#39;t we run the test set after the 500th training iteration?","created":"2024-11-02T18:53:45Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_2","children":[],"tag_good_arr":["m182ygrlgf83d9"],"id":"m30iurdy53uhh","updated":"2024-11-02T19:01:56Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Ignore my question on &#34;I am getting single digit perplexities for the test sets. Is this considered too low performance?&#34;. I was further training it on the test set for 200 iterations each after training it for 500 iterations. That is why my perplexities were so low.","created":"2024-11-02T19:54:26Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_2","children":[],"tag_good_arr":[],"id":"m30l0szp6ub11x","updated":"2024-11-02T19:54:26Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m30gm8onlx91wa","updated":"2024-11-02T19:54:26Z","config":{"editor":"rte"}}],"tag_good_arr":["m182ygrlgf83d9","m182yjodigv3la","m182yu2idsy4f6","m182yohq6nu3yz","lml4g3u9lv41ee"],"id":"m2s3goy9ckz5q2","config":{"is_announcement":1,"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":5,"num_favorites":9,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990290772,"default_anonymity":"no"},"error":null,"aid":"m3nyatcqm1vcj"}