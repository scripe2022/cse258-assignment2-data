{"result":{"history_size":3,"folders":["pa1"],"nr":158,"data":{"embed_links":[]},"created":"2024-10-17T08:23:50Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2d1b228sfm6up","v":"private","type":"create","when":"2024-10-17T08:23:50Z","uid_a":"a_0"},{"anon":"stud","data":"m2d1bsutnd9iy","v":"all","type":"update","when":"2024-10-17T08:24:25Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2df9uonuhs384","to":"m2d1b221rc56uo","type":"i_answer","when":"2024-10-17T14:54:48Z"},{"anon":"stud","data":"m2ditmk8mdz6","v":"all","type":"update","when":"2024-10-17T16:34:10Z","uid_a":"a_0"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m2dn5zfs50636v","type":"i_answer_update","when":"2024-10-17T18:35:45Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Question about PA 1 Q3","created":"2024-10-17T16:34:10Z","content":"<p>I have a few questions for PA1 Q3:</p>\n<p></p>\n<p>1. Is log-likelihood of training data the same as the data likelihood?</p>\n<p></p>\n<p>2. For 3(b) isn&#39;t probabilities P(y | the) the same for both &#34;cat&#34; and &#34;dog&#34; since context vectors for them are the same (0, 1), so why will there be &#34;probabilities within 0.01 of the optimum&#34;? Am I missing something?</p>\n<p></p>\n<p>3. What are &#34;training examples&#34;? Is it like &#34;context=y|word=x&#34; e.g (context=&#34;cat&#34;|word=&#34;the&#34;)?</p>\n<p></p>\n<p>4. Does &#34;dimensionality of the word embedding space d = 2&#34; means the vector will be size of 2 like (1, 0)?</p>\n<p></p>\n<p>Thank you for your time.</p>"},{"anon":"stud","uid_a":"a_0","subject":"Question about PA 1 Q#","created":"2024-10-17T08:24:25Z","content":"<p>I have a few questions for PA1 Q3:</p>\n<p></p>\n<p>1. Is log-likelihood of training data the same as the data likelihood?</p>\n<p></p>\n<p>2. For 3(b) isn&#39;t probabilities P(y | the) the same for both &#34;cat&#34; and &#34;dog&#34; since context vectors for them are the same (0, 1), so why will there be &#34;probabilities within 0.01 of the optimum&#34;? Am I missing something?</p>\n<p></p>\n<p>3. What are &#34;training examples&#34;? Is it like &#34;context=y|word=x&#34; e.g (context=&#34;cat&#34;|word=&#34;the&#34;)?</p>\n<p></p>\n<p>4. Does &#34;dimensionality of the word embedding space d = 2&#34; means the vector will be size of 2 like (1, 0)?</p>\n<p></p>\n<p>Thank you for your time.</p>"},{"anon":"stud","uid_a":"a_0","subject":"Question about PA 1 Q#","created":"2024-10-17T08:23:50Z","content":"<p>I have a few questions for PA1 Q3:</p>\n<p></p>\n<p>1. Is log-likelihood of training data the same as the data likelihood?</p>\n<p></p>\n<p>2. For 3(b) isn&#39;t probabilities P(y | the) the same for both &#34;cat&#34; and &#34;dog&#34; since context vectors for them are the same (0, 1), so why will there be &#34;probabilities within 0.01 of the optimum&#34;? Am I missing something?</p>\n<p></p>\n<p>3. What are &#34;training examples&#34;? Is it like &#34;context=y|word=x&#34; e.g (context=&#34;cat&#34;|word=&#34;the&#34;)?</p>\n<p></p>\n<p>4. Does &#34;dimensionality of the word embedding space d = 2&#34; means the vector will be size of 2 like (1, 0)?</p>\n<p></p>\n<p>Thank you for your time.</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":152,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-17T14:54:48Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-17T18:35:45Z","content":"<p>Great questions!<br /><br />1: Yes, the log-likelihood of training data refers to the likelihood of the data under your model, expressed in logarithmic terms. So, it&#39;s essentially the same as the data likelihood but expressed in log form to simplify calculations, especially when dealing with products of probabilities.</p>\n<p></p>\n<p>2 &amp; 3: I can&#39;t give a direct answer without revealing part of the solution. I recommend reviewing the probability equation in the skip-gram model and trying out some vector assignments to get a better sense. Also, if you are sure of the solution you obtained, you can write down the steps and explain your answer. Partial credits will be given for reasonable explanation even if the final answer is incorrect. Trust your answer!</p>\n<p></p>\n<p>4: Yes.</p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-17T14:54:48Z","content":"<p>Great questions!<br /><br />1: Yes, the log-likelihood of training data refers to the likelihood of the data under your model, expressed in logarithmic terms. So, it&#39;s essentially the same as the data likelihood but expressed in log form to simplify calculations, especially when dealing with products of probabilities.</p>\n<p></p>\n<p>2 &amp; 3: I can&#39;t give a direct answer without revealing part of the solution. I recommend reviewing the probability equation in the skip-gram model and trying out some vector assignments to get a better sense. Also, if you are sure of the solution you obtained, you can write down the steps and explain your answer. Partial credits will be given for reasonable explanation even if the final answer is incorrect. Trust your answer!</p>\n<p></p>\n<p>4: Yes.</p>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2df9uogamj383","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2d1b221rc56uo","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990171726,"default_anonymity":"no"},"error":null,"aid":"m3ny89j591f4hb"}