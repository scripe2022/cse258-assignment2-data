{"result":{"history_size":1,"folders":["pa2"],"nr":291,"data":{"embed_links":[]},"created":"2024-10-29T04:02:41Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2tx9fjobng1aq","v":"all","type":"create","when":"2024-10-29T04:02:41Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2txxahtvh146m","to":"m2tx9fjhjxh1ap","type":"i_answer","when":"2024-10-29T04:21:14Z"},{"anon":"stud","to":"m2tx9fjhjxh1ap","type":"followup","when":"2024-10-29T05:28:22Z","cid":"m2u0bm6i3owz6","uid_a":"a_1"},{"anon":"no","uid":"m182yt8a22e4cr","to":"m2tx9fjhjxh1ap","type":"feedback","when":"2024-10-29T09:01:42Z","cid":"m2u7xyurva16yt"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"part 2: decoder","created":"2024-10-29T04:02:41Z","content":"I have finished part 1 of the project and am trying to begin part 2. Any tips or videos to watch to better understand how to begin the decoder? Is it very similar to the encoder.Â "}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":140,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-29T04:21:14Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Sarvani Kunapareddy","endorser":{},"admin":false,"photo":"a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","id":"m182yor466x3zz","photo_url":"https://cdn-uploads.piazza.com/photos/m182yor466x3zz/a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-29T04:21:14Z","content":"<md>The video in the instruction is actually for Part 2. The same architecture without masking and lm_head is the Transformer encoder.</md>"}],"type":"i_answer","tag_endorse_arr":["m182yor466x3zz"],"children":[],"id":"m2txxahov246l","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"What do you mean LM_head? We have Attention Heads in the Encoder as well right?","created":"2024-10-29T05:28:22Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>In Karpathy's implementation of his `GPTLanguageModel` class, he follows the encoder blocks with a layer norm and a final linear layer (which he named `lm_head`).</md>","created":"2024-10-29T09:01:42Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"m182yt8a22e4cr","children":[],"tag_good_arr":[],"id":"m2u7xyurva16yt","updated":"2024-10-29T09:01:42Z","config":{"editor":"md"}}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":1,"id":"m2u0bm6i3owz6","updated":"2024-11-05T18:20:20Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m2tx9fjhjxh1ap","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990297087,"default_anonymity":"no"},"error":null,"aid":"m3nyay9e6rjkb"}