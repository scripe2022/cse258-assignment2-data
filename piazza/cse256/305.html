{"result":{"history_size":1,"folders":["pa2"],"nr":305,"data":{"embed_links":[]},"created":"2024-10-30T18:57:30Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"no","uid":"m182yi7s5mu3gz","data":"m2w8o0yzsng74d","v":"all","type":"create","when":"2024-10-30T18:57:30Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2wdnczroce3rm","to":"m2w8o0ypybr74b","type":"i_answer","when":"2024-10-30T21:16:57Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2we4a6dc8d38a","type":"i_answer_update","when":"2024-10-30T21:30:06Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"m182yi7s5mu3gz","subject":"PA2 Part 2","created":"2024-10-30T18:57:30Z","content":"<p>My decoder training results are shown below:<br /><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fm182yi7s5mu3gz%2F796080276c78e4ce2426749197e29dd66cadb35ad790ba036551bf57769a5d3d%2Fimage.png\" alt=\"image.png\" width=\"345\" height=\"443\" /><br />As the training perplexity reaches 100s level. The test perplexity doesn&#39;t seems to be in the &#34;300s and 400s&#34; range as specified in the document.Â </p>\n<p></p>\n<p>I believe I implemented the masks correctly, as the attn map shown below: (looks reasonable)</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fm182yi7s5mu3gz%2F5896ddbe48ddcb925939a64da42d6bd3c7fd577a209f38deb80577485ca4cbf5%2Fattention_map_1.png\" alt=\"attention_map_1.png\" width=\"412\" height=\"309\" /></p>\n<p></p>\n<p>Is there anything I may get wrong to result in this? Or it should just be a random deviation from the target results?</p>\n<p></p>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Sarvani Kunapareddy","endorser":{},"admin":false,"photo":"a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","id":"m182yor466x3zz","photo_url":"https://cdn-uploads.piazza.com/photos/m182yor466x3zz/a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","published":true,"us":false,"facebook_id":null}],"unique_views":144,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-30T21:16:57Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-30T21:30:06Z","content":"<md>I think your implementation is okay seeing the loss and training perplexity. If you want to improve a bit there are some tricks you can try: [pre-LayerNorm](https://stackoverflow.com/questions/77864704/annotated-transformer-why-x-dropoutsublayerlayernormx)  and 4 in @283 would help improving perplexity.</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-30T21:16:57Z","content":"<md>I think your implementation is okay seeing the loss and training perplexity. If you want to improve a bit there are some tricks you can try: 3 and 4 in @283 would help improving perplexity.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2wdnczjzo53rl","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["m182yor466x3zz"],"no_answer":0,"id":"m2w8o0ypybr74b","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990310988,"default_anonymity":"no"},"error":null,"aid":"m3nyb8zk11j10k"}