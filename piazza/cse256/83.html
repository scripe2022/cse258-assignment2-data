{"result":{"history_size":1,"folders":["pa1"],"nr":83,"data":{"embed_links":[]},"created":"2024-10-14T03:53:00Z","bucket_order":3,"no_answer_followup":2,"change_log":[{"anon":"stud","data":"m28hb7hxw1x2dz","v":"all","type":"create","when":"2024-10-14T03:53:00Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m28hnh0bwqc2pc","to":"m28hb7hrjw72dy","type":"i_answer","when":"2024-10-14T04:02:32Z"},{"anon":"stud","to":"m28hb7hrjw72dy","type":"followup","when":"2024-10-14T05:40:56Z","cid":"m28l60dr25d53a","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m28hb7hrjw72dy","type":"feedback","when":"2024-10-14T06:38:42Z","cid":"m28n8b0ij8z2yg"},{"anon":"no","uid":"m182yszkkec4c4","to":"m28hb7hrjw72dy","type":"followup","when":"2024-10-15T01:21:11Z","cid":"m29rbtfayep3t0"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m28hb7hrjw72dy","type":"feedback","when":"2024-10-15T01:58:58Z","cid":"m29soekvvm81rp"},{"anon":"stud","to":"m28hb7hrjw72dy","type":"feedback","when":"2024-10-15T04:03:36Z","cid":"m29x4oo767i4vd","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m28hb7hrjw72dy","type":"feedback","when":"2024-10-15T04:05:31Z","cid":"m29x75agpng56k"},{"anon":"no","uid":"m182yszkkec4c4","to":"m28hb7hrjw72dy","type":"feedback","when":"2024-10-19T22:46:48Z","cid":"m2gr0j8ihscv0"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m28hb7hrjw72dy","type":"feedback","when":"2024-10-19T23:07:47Z","cid":"m2grrj14grepn"},{"anon":"no","uid":"m182yszkkec4c4","to":"m28hb7hrjw72dy","type":"feedback","when":"2024-10-20T00:36:50Z","cid":"m2guy202t3w3f0"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"PA1: Q2 Num_Merges vs Vocab_Size","created":"2024-10-14T03:53:00Z","content":"<p>Hi,</p>\n<p></p>\n<p>I am currently implementing PA1: Q2. The question asks us to &#34;Experiment with different vocabulary sizes&#34;. In the BPE algorithm, we can adjust the number of BPE merge operations. If we increase the num_merges, it allows for a larger vocabulary since there are fewer subword tokens. There is also a vocab_size variable that can be used to decide on the number of unique tokens and the size of the embedding matrix (Please correct me if I am wrong). Should we experiment with only one of the variables or both? </p>\n<p></p>\n<p>Thanks!</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":193,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-14T04:02:32Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Melvyn Nam Qiang Tan","endorser":{"lfvsigi4w4t2pr":1686173608},"admin":false,"photo":null,"id":"kfo6ps0kgod3hx","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Ruoyu Hou","endorser":{},"admin":false,"photo":null,"id":"kdqq29shx1h5dn","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-14T04:02:32Z","content":"<div>\n<div>\n<div>\n<div>\n<div>\n<p>The final vocabulary size equals the initial vocabulary size plus the number of merge operations. For further clarification, please refer to the original <a href=\"https://arxiv.org/pdf/1508.07909\" target=\"_blank\" rel=\"noopener noreferrer\">BPE paper</a>.</p>\n<p>Please let us know if you have further questions.</p>\n</div>\n</div>\n</div>\n</div>\n</div>"}],"type":"i_answer","tag_endorse_arr":["kfo6ps0kgod3hx","kdqq29shx1h5dn"],"children":[],"id":"m28hnh056at2pb","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Would directly calculating the length of the merged subword group achieve the same effect?","created":"2024-10-14T05:40:56Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>Sorry, I don&#39;t quite understand what you mean by &#34;achieve the same effect.&#34;</p>\n<p></p>\n<p>In BPE, adjusting the number of merges and vocabulary size is essentially the same thing, as they are highly related.</p>","created":"2024-10-14T06:38:42Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m28n8b0ij8z2yg","updated":"2024-10-14T06:38:42Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"What I am thinking is to get all types of subwords after doing the merge. Would that number be the vocabulary size?","created":"2024-10-15T04:03:36Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m29x4oo767i4vd","updated":"2024-10-15T04:03:36Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"I think so.","created":"2024-10-15T04:05:31Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[{"role":"student","name":"Hengzhou Li","endorser":{},"admin":false,"photo":null,"id":"m182ytxamho4es","photo_url":null,"us":false,"facebook_id":null}],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":["m182ytxamho4es"],"id":"m29x75agpng56k","updated":"2024-10-15T04:57:31Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m28l60dr25d53a","updated":"2024-10-15T04:05:31Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"While that may be true in the initial merge stages, the final vocabulary doesn&#39;t always equal initial vocab &#43; merges. In the later stages of BPE, merges start to affect fewer and fewer words. Once common pairs have been merged, the remaining merges may not create an entirely new subword but may reinforce existing subwords. Additionally, some merges might collapse into larger units, reducing the overall count of distinct subwords. I noticed that after around 500 merges, the final vocab is smaller than sum of the initial vocab &#43; merges.","created":"2024-10-15T01:21:11Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"m182yszkkec4c4","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>We might have different understandings of &#34;subword&#34; and &#34;vocabulary.&#34; Please feel free to share if you have any different ideas or interpretations. Thanks!</p>\n<p></p>\n<p>Reference: BPE paper  (<a href=\"https://arxiv.org/pdf/1508.07909\">https://arxiv.org/pdf/1508.07909</a>)</p>\n<p></p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fln0md59uz9w3kd%2Fe4afbd232dc36d983bf0a41654a4fa7e29a4c2ea5b5b07bfeb7b4bfdb9d43c00%2FBPE.png\" alt=\"BPE.png\" /></p>","created":"2024-10-15T01:58:58Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m29soekvvm81rp","updated":"2024-10-15T01:58:58Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>That still does not make sense. If you start out with a set of 100 words, doing a million merges will not yield more than a million subwords. By its very nature, the BPE algorithm slowly converges a vocabulary of characters into a vocabulary of words. In between the two states, you get different sizes of vocabularies of subwords based on how many merges you perform. For some large number of merges, M, the character vocabulary will eventually take the form of the word level vocabulary. Any subsequent merges will add exactly zero new subwords.</p>\n<p>For any dataset you run BPE on, the size of subword vocabulary increases at first with the number of merges and then eventually decreases as the number of merges becomes too large. This would be the reality in practice. I believe, the paper mentions that equation only as a first-order approximation (of a more complex mathematical equation that takes effect in practice) and should not be taken literally. Most articles on BPE have simply adopted this equation as a tradition, but it is mathematically imprecise to use an &#39;=&#39; sign. It&#39;s an approximation. Let me know your thoughts. It&#39;s been an interesting discussion. Thanks! :)</p>\n<p></p>\n<p></p>","created":"2024-10-19T22:46:48Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"m182yszkkec4c4","children":[],"tag_good_arr":[],"id":"m2gr0j8ihscv0","d-bucket":"Yesterday","updated":"2024-10-19T22:46:48Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>I see your point that strictly speaking, we shouldn&#39;t use the equal sign.</p>\n<p></p>\n<p>However, in practice, we typically won&#39;t have such a large vocabulary size. Even models like GPT-4 have a vocabulary size of around 100K.</p>\n<p></p>\n<p>This is why we need sub-word tokenization; its goal is to strike a balance between character-level and word-level tokenization.</p>\n<p></p>\n<p>Additionally, as the vocabulary size grows, we experience diminishing returns, so in practice, people often stop merging after a certain point.</p>","created":"2024-10-19T23:07:47Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m2grrj14grepn","d-bucket":"Yesterday","updated":"2024-10-19T23:07:47Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>Okay, I see the disconnect now. The Basic BPE requires you to retain every version of the subword as it progresses through subsequent merges from a character to a subword and eventually to a word (after many merges).</p>\n<p></p>\n<p>I am only retaining a version of the subword as long as it is not redundant, i.e., the larger version of the subword can adequately explain the words in the dataset without losing granularity. This is a valid approach, and it is called pruning.</p>\n<p></p>\n<p>For example, lets say my dataset is just {low, lowering}.</p>\n<p>Initial: {l, o, w, e, r, i, n, g}</p>\n<p>After 3 merges,</p>\n<p>Basic BPE: {l, o, w, e, r, i, n, g, lo, low, lowe} (final vocab = initial &#43; merges = 8 &#43; 3 = 11, holds true)</p>\n<p>BPE with Pruning: {r, i, n, g, low, lowe} (final vocab = 6 != initial &#43; merges, does not hold true)</p>\n<p></p>\n<p>The dataset can be (adequately) represented as:</p>\n<p>low -&gt; low</p>\n<p>lowering -&gt; lowe &#43; r &#43; i &#43; n &#43; g</p>\n<p></p>\n<p>Another example:</p>\n<p>lets say my dataset is just {low, lowering, lost}.</p>\n<p>Initial: {l, o, w, e, r, i, n, g, s, t}</p>\n<p>After 3 merges,</p>\n<p>Basic BPE: {l, o, w, e, r, i, n, g, s, t, lo, low, lowe} (final vocab = initial &#43; merges = 10 &#43; 3 = 13, holds true)</p>\n<p>BPE with Pruning: {r, i, n, g, s, t, lo, low, lowe} (final vocab = 9 != initial &#43; merges, does not hold true)</p>\n<p></p>\n<p>The dataset can be (adequately) represented as:</p>\n<p>low -&gt; low</p>\n<p>lowering -&gt; lowe &#43; r &#43; i &#43; n &#43; g</p>\n<p>lost -&gt; lo &#43; s &#43; t</p>\n<p></p>\n<p>Pruning allows you to reduce the size of the vocabulary further, providing computational benefits. Tokenization will anyway choose the largest matching subword to break up a word. So for the original word, the smaller subword versions do not matter. For other similar words, this does not significantly reduce accuracy compared to basic BPE because, in a large enough dataset, most individual characters and common subword pairs will be accounted for, allowing adequate granularity for unseen words. Unseen characters and subwords will be taken care of by [UNK]</p>\n<p></p>\n<p>With this approach, I achieved a significant reduction in vocab size <em>without </em>sacrificing accuracy on the test set (which achieved a ~75% accuracy).</p>","created":"2024-10-20T00:36:50Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"ta","name":"Po-Chun Wu","endorser":{},"admin":true,"photo":null,"id":"ln0md59uz9w3kd","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"m182yszkkec4c4","children":[],"tag_good_arr":["ln0md59uz9w3kd"],"id":"m2guy202t3w3f0","updated":"2024-10-20T03:35:07Z","config":{"editor":"rte"}}],"tag_good_arr":["m182ygrlgf83d9"],"no_answer":1,"id":"m29rbtfayep3t0","updated":"2024-10-20T00:36:50Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m28hb7hrjw72dy","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990086964,"default_anonymity":"no"},"error":null,"aid":"m3ny6g4ob975ia"}