{"result":{"history_size":1,"folders":["pa1"],"nr":21,"data":{"embed_links":[]},"created":"2024-10-07T01:51:27Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"no","uid":"l4orh8d3v0754v","data":"m1ycvxa2qt3468","v":"all","type":"create","when":"2024-10-07T01:51:27Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m1ye4rzlzo2ml","to":"m1ycvx9wj12467","type":"i_answer","when":"2024-10-07T02:26:19Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"l4orh8d3v0754v","subject":"part 1 writeup","created":"2024-10-07T01:51:27Z","content":"<p>Hello, for part 1 of the writeup, should we experiment with those and describe the changes:</p>\n<ul><li>Varying the number of layers?</li><li>Changing the hidden layer sizes?</li><li>Trying different sources of embeddings (e.g., 50d or 300d)?</li><li>Adding dropout layers (after the embeddings or after the hidden layers)?</li></ul>\n<p>Are these adjustments aimed at finding the best configuration? Also, what does &#34;initialization&#34; refer to at the end of part one? Is it refers to some initialization like &#34;He initialization&#34;?</p>\n<p>Thank you!</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":231,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-07T02:26:19Z","bucket_order":3,"tag_endorse":[{"role":"ta","name":"Akash Saranathan","endorser":{},"admin":true,"photo":null,"id":"lmpga1m6ftd62m","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-07T02:26:19Z","content":"<p>For Part 1 of the writeup, yes, you should experiment with those adjustments and briefly describe the changes and results.</p>\n<p></p>\n<p>Regarding &#34;initialization,&#34; it depends on which part you’re referring to:</p>\n<ul><li><strong>1a)</strong> It’s about <strong>weight initialization</strong>, such as &#34;He initialization&#34; or other strategies for initializing the model’s weights.</li><li><strong>1b)</strong> It refers to <strong>word embedding initialization</strong>, like using random embeddings or pre-trained embeddings (e.g., GloVe).</li></ul>"}],"type":"i_answer","tag_endorse_arr":["lmpga1m6ftd62m"],"children":[],"id":"m1ye4rzf6ikmk","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m1ycvx9wj12467","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990022536,"default_anonymity":"no"},"error":null,"aid":"m3ny52f1grx2f9"}