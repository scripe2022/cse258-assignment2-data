{"result":{"history_size":1,"folders":["pa2"],"nr":394,"data":{"embed_links":[]},"created":"2024-11-05T09:36:57Z","bucket_order":3,"no_answer_followup":2,"change_log":[{"anon":"stud","data":"m349a9suau31d9","v":"all","type":"create","when":"2024-11-05T09:36:57Z","uid_a":"a_0"},{"anon":"stud","to":"m349a9snf4v1d8","type":"followup","when":"2024-11-05T10:15:51Z","cid":"m34aoaszwbd372","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m34pe4952xp1ko","to":"m349a9snf4v1d8","type":"i_answer","when":"2024-11-05T17:07:50Z"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m349a9snf4v1d8","type":"feedback","when":"2024-11-05T17:08:30Z","cid":"m34peyjycgq2k6"},{"anon":"stud","to":"m349a9snf4v1d8","type":"followup","when":"2024-11-05T17:27:29Z","cid":"m34q3dypwpj7ds","uid_a":"a_0"},{"anon":"stud","to":"m349a9snf4v1d8","type":"feedback","when":"2024-11-05T17:27:36Z","cid":"m34q3iszpqu7l0","uid_a":"a_0"},{"anon":"no","uid":"m182yt8a22e4cr","to":"m349a9snf4v1d8","type":"feedback","when":"2024-11-05T18:00:49Z","cid":"m34ra8zinb66h4"},{"anon":"stud","to":"m349a9snf4v1d8","type":"feedback","when":"2024-11-06T20:09:39Z","cid":"m36bbryupcu1nf","uid_a":"a_0"},{"anon":"no","uid":"m182yt8a22e4cr","to":"m349a9snf4v1d8","type":"feedback","when":"2024-11-06T20:12:32Z","cid":"m36bfhgvbpj7au"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Perpexility","created":"2024-11-05T09:36:57Z","content":"<p>I have doubt in the Decoder, as I have also implemented the mask and getting this results. is this wrong ?</p>\n<p></p>\n<p>Loading data and creating tokenizer ...<br />Vocabulary size is 5755<br />Iteration 0/500, Loss: 0.009219241328537464<br />Iteration 100/500, Loss: 0.007982319220900536<br />Iteration 200/500, Loss: 0.0<br />Iteration 300/500, Loss: 0.00775659317150712<br />Iteration 400/500, Loss: 0.016221217811107635<br />Perpexility of HBush<br />Perplexity after 100 iterations: 1.0044960975646973<br />Perplexity after 200 iterations: 1.0044102668762207<br />1.0044690370559692<br />Perpexility of WBush<br />Perplexity after 100 iterations: 1.004272699356079<br />Perplexity after 200 iterations: 1.004342794418335<br />1.004328727722168<br />Perpexility of Obama<br />Perplexity after 100 iterations: 1.003969669342041<br />Perplexity after 200 iterations: 1.004077434539795<br />Perplexity after 300 iterations: 1.004055380821228<br />1.0040981769561768</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":160,"children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>In Part 3. if we modify only architectural changes and use any other Positional Encoding and discuss it&#39;s result, will that be sufficient ?</p>\n<p></p>\n<p>or do we have to implement all 4 modifications ?</p>","created":"2024-11-05T10:15:51Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>tryingÂ other Positional Encoding is sufficient.</md>","created":"2024-11-05T17:08:30Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m34peyjycgq2k6","d-bucket":"Yesterday","updated":"2024-11-05T17:08:30Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Thanks.","created":"2024-11-05T17:27:36Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m34q3iszpqu7l0","d-bucket":"Yesterday","updated":"2024-11-05T17:27:36Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m34aoaszwbd372","d-bucket":"Yesterday","updated":"2024-11-05T17:27:36Z","config":{"editor":"rte"}},{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-05T17:07:50Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-05T17:07:50Z","content":"<md>Perplexities are too low, this means your masking implementation is wrong. \nPlease take a look your masking is correctly working.\nCorrect implementation should show the plots something like this: [@299](299)</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m34pe48zdtc1km","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>My Attention Matrix is perfectly fine, as I covered the future Tokens with lower triangular. attaching the Heat Map below.</p>\n<p></p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fm182yphehwi41z%2Fc881792e973138b40f11df97e33e82570124a8d00f633537007fbd75013e1979%2Fimage.png\" alt=\"image.png\" /></p>","created":"2024-11-05T17:27:29Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Your attention heatmap is wrong. Look carefully: your mask misses the diagonal above the main diagonal, so your model is able to look one token ahead.</md>","created":"2024-11-05T18:00:49Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"m182yt8a22e4cr","children":[],"tag_good_arr":[],"id":"m34ra8zinb66h4","d-bucket":"Yesterday","updated":"2024-11-05T18:01:04Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"What do you think would have gone wrong, any suggestions ?","created":"2024-11-06T20:09:39Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m36bbryupcu1nf","updated":"2024-11-06T20:09:39Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Like I already said, your mask is wrong. The lecture slides give an example of what the mask should look like. Your perplexity should be much more reasonable after fixing that.</md>","created":"2024-11-06T20:12:32Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"m182yt8a22e4cr","children":[],"tag_good_arr":[],"id":"m36bfhgvbpj7au","updated":"2024-11-06T20:12:32Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":1,"id":"m34q3dypwpj7ds","updated":"2024-11-06T20:12:32Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m349a9snf4v1d8","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990399807,"default_anonymity":"no"},"error":null,"aid":"m3nyd5ioq1y5zj"}