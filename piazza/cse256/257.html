{"result":{"history_size":1,"folders":["pa2"],"nr":257,"data":{"embed_links":[]},"created":"2024-10-26T00:37:24Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2pflvin3h5hm","v":"all","type":"create","when":"2024-10-26T00:37:24Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2pou49mbzd7nm","to":"m2pflvigh9bhl","type":"i_answer","when":"2024-10-26T04:55:45Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Encode layer structure","created":"2024-10-26T00:37:24Z","content":"What’s the structure of encode layer? After we get sum of the word and positional embeddings"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Rahul Sharma Nemmani","endorser":{},"admin":false,"photo":null,"id":"jml95ci0otv4w9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":133,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-26T04:55:45Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-26T04:55:45Z","content":"<md>After you get sum of the word and positional embeddings, then you need to implement Transformer layers with multi-head attention. Each Transformer encoder layer has multi head attention, layer norm and feed forward layer. See the slides or 'Attention is all you need' paper for more details</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2pou49ffp7nl","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["jml95ci0otv4w9"],"no_answer":0,"id":"m2pflvigh9bhl","config":{"has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990266740,"default_anonymity":"no"},"error":null,"aid":"m3nyaaufsal32v"}