{"result":{"history_size":1,"folders":["pa1"],"nr":40,"data":{"embed_links":[]},"created":"2024-10-10T18:37:38Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m23n5fyybd0h5","v":"all","type":"create","when":"2024-10-10T18:37:38Z","uid_a":"a_0"},{"anon":"no","uid":"lmr7yhnqdjb6ul","data":"m23u8pl6nxhw4","to":"m23n5fypwyuh4","type":"i_answer","when":"2024-10-10T21:56:08Z"},{"anon":"stud","to":"m23n5fypwyuh4","type":"followup","when":"2024-10-13T10:00:14Z","cid":"m27ezm5t6sf3rm","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m23n5fypwyuh4","type":"feedback","when":"2024-10-13T19:23:03Z","cid":"m27z3edzzp150y"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Seed for Embedded Layer and CrossEntropyLoss","created":"2024-10-10T18:37:38Z","content":"Hi,\n\nRegarding the loss calculation, it seems that the PA writeup says that we have to use CrossEntropyLoss and NLLoss. However, the forward function used the softMax function and returns it in the BOW model. Why should we return these losses compared the softmax losses if thats what the BOW model did? Additionally, sometimes when I run the DAN model I get 50% accuracy but other times I run it I get 70% accuracy. I&#39;m confused on why this is happening. How should I do debug this? It seems that the shape of my data looks correct."}],"type":"question","tags":["pa1","student"],"tag_good":[{"role":"student","name":"Rahul Sharma Nemmani","endorser":{},"admin":false,"photo":null,"id":"jml95ci0otv4w9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":174,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-10T21:56:08Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"lmr7yhnqdjb6ul","subject":"","created":"2024-10-10T21:56:08Z","content":"<p>Hi, for the first question, the use of Softmax. The softmax function is an activation function that converts the raw digits to probabilities in (0, 1) for each class, instead of returning a softmax loss. Then, we pass the predicted probability to CrossEntropyLoss or NLLoss to get the loss we need.</p>\n<p></p>\n<p>For the second question, I think the problem should derive from the initialization. If we get a bad initialization for the model weight or embedding, the model will probably get stuck into local minima, which leads to low accuracy. This is a normal case, and probably not the problem of your implementation. I think you can try to explore some initialization methods if you want. Or just report the times when you get 70% accuracy and explain the reason you think that leads to the 50% accuracy sometimes.</p>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m23u8pl03gfw3","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Sorry, it&#39;s not super clear from your response, but would we need to change the log_softmax function on the output of the network (from the BOW model) in the DAN model if we are still using NLLoss? To me, it doesn&#39;t seem like this is necessary, but I just wanted to confirm.","created":"2024-10-13T10:00:14Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>1. The spec says you can use a built-in loss function like <code>NLLLoss</code> or <code>CrossEntropyLoss</code>—you don’t need to implement both.</p>\n<p></p>\n<p>2. If you&#39;re using <code>NLLLoss</code>, you&#39;ll need to ensure the output from your model is passed through <code>log_softmax</code>. If you&#39;re using <code>CrossEntropyLoss</code>, no need to change it, as it combines <code>log_softmax</code> internally.</p>","created":"2024-10-13T19:23:03Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m27z3edzzp150y","updated":"2024-10-13T19:23:03Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m27ezm5t6sf3rm","updated":"2024-10-13T19:23:03Z","config":{"editor":"rte"}}],"tag_good_arr":["jml95ci0otv4w9"],"no_answer":0,"id":"m23n5fypwyuh4","config":{"editor":"plain","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990041487,"default_anonymity":"no"},"error":null,"aid":"m3ny5h1eoko32x"}