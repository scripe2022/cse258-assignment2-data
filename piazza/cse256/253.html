{"result":{"history_size":1,"folders":["pa2"],"nr":253,"data":{"embed_links":[]},"created":"2024-10-25T21:34:02Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2p922xjekj7am","v":"all","type":"create","when":"2024-10-25T21:34:02Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2pmtlevonet9","to":"m2p922x9ty07ak","type":"i_answer","when":"2024-10-26T03:59:21Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2pmu5e6rx21vx","type":"i_answer_update","when":"2024-10-26T03:59:47Z"},{"anon":"stud","to":"m2p922x9ty07ak","type":"followup","when":"2024-10-26T17:02:17Z","cid":"m2qesfzzryq374","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2p922x9ty07ak","type":"feedback","when":"2024-10-26T21:38:59Z","cid":"m2qooagzin76qa"},{"anon":"no","uid":"ktyq6r4zhsm3ih","to":"m2p922x9ty07ak","type":"feedback","when":"2024-10-27T03:43:24Z","cid":"m2r1oxlyih11nl"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Sanity check return attention map","created":"2024-10-25T21:34:02Z","content":"Hi, I&#39;m a little lost on how to return the attention map from our encoder, in order to generate the plots of attention matrices. Could you give me some hints on this? Thank you!"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Rahul Sharma Nemmani","endorser":{},"admin":false,"photo":null,"id":"jml95ci0otv4w9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Madurya Suresh","endorser":{},"admin":false,"photo":null,"id":"m182yu2idsy4f6","photo_url":null,"us":false,"facebook_id":null},{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":166,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-26T03:59:21Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Xiang Li","endorser":{},"admin":false,"photo":null,"id":"kde1ftqmtgu2pa","photo_url":null,"us":false,"facebook_id":null},{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Madurya Suresh","endorser":{},"admin":false,"photo":null,"id":"m182yu2idsy4f6","photo_url":null,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-26T03:59:47Z","content":"<md>Have you already implemented the Transformer model ?\n\nFirst, make sure you return the attention maps along with the models output\n\n```\n_,  attn_maps = self.model(input_tensor)\n```\n\n`attn_maps` here is the list of attention maps (tensors) from each Transformer layer. \n\nIn this loop\n\n```\n# Visualize and save the attention maps\n        for j, attn_map in enumerate(attn_maps):\n            att_map = attn_map.squeeze(0).detach().cpu().numpy()  # Remove batch dimension and convert to NumPy array\n\n            # Check if the attention probabilities sum to 1 over rows\n            total_prob_over_rows = torch.sum(attn_map[0], dim=1)\n            if torch.any(total_prob_over_rows < 0.99) or torch.any(total_prob_over_rows > 1.01):\n                print(\"Failed normalization test: probabilities do not sum to 1.0 over rows\")\n                print(\"Total probability over rows:\", total_prob_over_rows.numpy())\n```\n\n `attn_map[0]` is 2-D tensor of (32, 32). \n\nYou can modify indices or shape of tensors if you think its hard to fit your output into the provided code.</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-26T03:59:21Z","content":"<md>Have you already implemented the Transformer model ?\n\nFirst, make sure you return the attention maps along with the models output\n\n```\n_,  attn_maps = self.model(input_tensor)\n```\n\n`attn_maps` here is the list of attention maps from each Transformer layer. \n\nIn this loop\n\n```\n# Visualize and save the attention maps\n        for j, attn_map in enumerate(attn_maps):\n            att_map = attn_map.squeeze(0).detach().cpu().numpy()  # Remove batch dimension and convert to NumPy array\n\n            # Check if the attention probabilities sum to 1 over rows\n            total_prob_over_rows = torch.sum(attn_map[0], dim=1)\n            if torch.any(total_prob_over_rows < 0.99) or torch.any(total_prob_over_rows > 1.01):\n                print(\"Failed normalization test: probabilities do not sum to 1.0 over rows\")\n                print(\"Total probability over rows:\", total_prob_over_rows.numpy())\n```\n\n `attn_map[0]` is 2-D tensor of (32, 32). \n\nYou can modify indices or shape of tensors if you think its hard to fit your output into the provided code.</md>"}],"type":"i_answer","tag_endorse_arr":["kde1ftqmtgu2pa","m182ygrlgf83d9","m182yu2idsy4f6"],"children":[],"id":"m2pmtleqsmat8","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Thank you for replying to me! I think I&#39;m more confused on how to return the attention map along with our model output, like how to get the attention map while we compute our model output. Could you help me with this? Thank you!","created":"2024-10-26T17:02:17Z","bucket_order":5,"bucket_name":"This week","type":"followup","tag_good":[{"role":"student","name":"Madurya Suresh","endorser":{},"admin":false,"photo":null,"id":"m182yu2idsy4f6","photo_url":null,"us":false,"facebook_id":null}],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>You can make a list and store intermediate attention matrices. But the detail of implementation is actually part of the assignment, so please come up with your way to return attention maps</md>","created":"2024-10-26T21:38:59Z","bucket_order":8,"bucket_name":"Week 10/20 - 10/26","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2qooagzin76qa","updated":"2024-10-26T21:38:59Z","config":{"editor":"md"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Is it not possible to simply use the provided utilities.sanity_check() function after we finish training?","created":"2024-10-27T03:43:24Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ktyq6r4zhsm3ih","children":[],"tag_good_arr":[],"id":"m2r1oxlyih11nl","updated":"2024-10-27T03:43:24Z","config":{"editor":"rte"}}],"tag_good_arr":["m182yu2idsy4f6"],"no_answer":1,"id":"m2qesfzzryq374","updated":"2024-11-03T01:31:57Z","config":{"editor":"rte"}}],"tag_good_arr":["jml95ci0otv4w9","m182ygrlgf83d9","m182yu2idsy4f6","kfsi52ar6572xo"],"no_answer":0,"id":"m2p922x9ty07ak","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990261686,"default_anonymity":"no"},"error":null,"aid":"m3nya6y329g53t"}