{"result":{"history_size":1,"folders":["pa2"],"nr":286,"data":{"embed_links":[]},"created":"2024-10-28T18:54:19Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2tdo8qjem13ih","v":"all","type":"create","when":"2024-10-28T18:54:19Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2th4na5llutx","to":"m2tdo8qclag3ig","type":"i_answer","when":"2024-10-28T20:31:04Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2th6qo2lf44j3","type":"i_answer_update","when":"2024-10-28T20:32:41Z"},{"anon":"no","uid":"m182yt8a22e4cr","data":"m2tj6ycsut44rv","to":"m2tdo8qclag3ig","type":"s_answer","when":"2024-10-28T21:28:51Z"},{"anon":"no","uid":"m182yt8a22e4cr","data":"m2tjh2oky7pto","type":"s_answer_update","when":"2024-10-28T21:36:43Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Code Similar to Andrej&#39;s tutorial","created":"2024-10-28T18:54:19Z","content":"<p>Hi,</p>\n<p></p>\n<p>I am trying to follow the flow of class slides and the &#34;Attention is all you need&#34; paper. I also took a look at this <a href=\"https://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks\" target=\"_blank\" rel=\"noopener noreferrer\">post</a>Â as well as Andrej&#39;s video. However, after watching the video most of the arch I can think of is very similar to Andrej&#39;s video, with some of the modules (like calculating attention, feedforward layers, multihead layer) seem to end up being almost exactly the same. in post @252 the TAs said not to copy code repos or modules. I am implementing them myself but since the underlying idea is the same everywhere it ended up looking almost exactly like Andrej&#39;s layout.</p>\n<p></p>\n<p>Would this be acceptable since it&#39;s not a copy paste of his repo, if though it ends up being the same?</p>\n<p></p>\n<p>reference: https://piazza.com/class/m182y0mqvor2k7/post/252</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":181,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-28T20:31:04Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-28T20:32:41Z","content":"<md>Hi, it would be okay if your code looks similar to Andrej's or that blog post, and I think most codes are very similar each other since we implement the same model. Even you follow the tutorial code, I believe you will need to come up with some methods and implement by your self in Part 3.</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-28T20:31:04Z","content":"<md>Hi, it would be okay if your code looks similar to Andrej's or that blog post, and I think most codes are very similar each other since we implement the same model. Even you follow the tutorial code, I guess you need to implement some parts by your self in Part 3.</md>"}],"type":"i_answer","tag_endorse_arr":["kfsi52ar6572xo"],"children":[],"id":"m2th4n9yc0otw","config":{"editor":"md"},"is_tag_endorse":false},{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-28T21:28:51Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"m182yt8a22e4cr","subject":"","created":"2024-10-28T21:36:43Z","content":"<md>Even if you do follow Andrej's tutorial, your implementation won't be *completely* identical to his if you modify his code to follow this PA's instructions. For example:\n- Andrej adds dropout layers to his attention heads, but PA2 doesn't ask us to do that for our implementations. I asked a TA in OH and was told that dropout layers are optional.\n- The feed-forward layers in his encoder blocks has 2 linear layers with a hidden layer size of `4 * n_embed`, but PA2 doesn't specify this. I don't see why you couldn't choose a different number of layers or a different hidden layer size.\n- As mentioned in @280, there's also the question of pre-normalization vs post-normalization for the encoder blocks.\n\nDepending on what choices you make regarding implementation details like this, your implementation won't be identical to Andrej's.</md>"},{"anon":"no","uid":"m182yt8a22e4cr","subject":"","created":"2024-10-28T21:28:51Z","content":"<md>Even if you do follow Andrej's tutorial, your implementation won't be *completely* identical to his if you modify his code to follow this PA's instructions. For example:\n- Andrej adds dropout layers to his attention heads, but PA2 doesn't ask us to do that for our implementations. I asked a TA in OH and was told that dropout layers are optional.\n- The feed-forward layers in his encoder blocks has 2 linear layers with a hidden layer size of `4 * n_embed`, but PA2 doesn't specify this. I don't see why you couldn't choose a different number of layers or a different hidden layer size.\n- As mentioned in another Piazza post, there's also the question of pre-normalization vs post-normalization for the encoder blocks.\n\nDepending on what choices you make regarding implementation details like this, your implementation won't be identical to Andrej's.</md>"}],"type":"s_answer","tag_endorse_arr":[],"children":[],"id":"m2tj6ycphry4ru","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":0,"id":"m2tdo8qclag3ig","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990293300,"default_anonymity":"no"},"error":null,"aid":"m3nyavc7q9yp8"}