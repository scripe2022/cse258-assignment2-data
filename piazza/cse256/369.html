{"result":{"history_size":2,"folders":["pa2"],"nr":369,"data":{"embed_links":[]},"created":"2024-11-04T02:48:45Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"no","uid":"m182yxkiqd84os","data":"m32f9gtk5u36ce","v":"all","type":"create","when":"2024-11-04T02:48:45Z"},{"anon":"stud","data":"m32he20v7wk4we","to":"m32f9gtcvas6cc","type":"s_answer","when":"2024-11-04T03:48:18Z","uid_a":"a_0"},{"anon":"no","uid":"m182yxkiqd84os","data":"m32j11vjwbf3en","v":"all","type":"update","when":"2024-11-04T04:34:11Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"m182yxkiqd84os","subject":"part 2 overfitting","created":"2024-11-04T04:34:11Z","content":"<p>Hi, the perplexity of my train txt goes down from 400s to 100s, which looks correct. But for all three test txt, the perplexity goes up from 400s to 3000s, I&#39;m really confused about this. My decoder is almost the same as my encoder except I add mask attention and use a linear layer to project the size to (B, T, vocab_size). I calculate the loss by flattening the result to (B*T, vocab_size) and the target. I thought it was overfitting and I tried dropout and different ways to init the model, but none of them worked. Is it the problem of my model? Or I did my loss function calculation wrong?</p>\n<p></p>"},{"anon":"no","uid":"m182yxkiqd84os","subject":"part 2 overfitting","created":"2024-11-04T02:48:45Z","content":"Hi, the perplexity of my train txt goes down from 400s to 100s, which looks correct. But for all three test txt, the perplexity goes up from 400s to 3000s, I&#39;m really confused about this. My decoder is almost the same as my encoder except I add mask attention. I thought it was overfitting and I tried dropout and different ways to init the model, but none of them worked. Is it the problem of my model?"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":140,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-04T03:48:18Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"","created":"2024-11-04T03:48:18Z","content":"the only difference between your encoder and decoder is that you are masking in the decoder. What are other differences?"}],"type":"s_answer","tag_endorse_arr":[],"children":[],"id":"m32he20onjq4wd","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m32f9gtcvas6cc","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990371852,"default_anonymity":"no"},"error":null,"aid":"m3nycjy5hm54kg"}