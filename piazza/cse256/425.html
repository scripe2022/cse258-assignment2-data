{"result":{"history_size":1,"folders":["pa2"],"nr":425,"data":{"embed_links":[]},"created":"2024-11-07T00:41:50Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m36l1t3sqg57by","v":"all","type":"create","when":"2024-11-07T00:41:50Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m36q430c4521j8","to":"m36l1t3ixyh7bx","type":"i_answer","when":"2024-11-07T03:03:34Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Sparse attention doesn&#39;t reduce training time by a ton","created":"2024-11-07T00:41:50Z","content":"<p>I’m trying to implement sparse attention but it seems it doesn’t improve the overhead by a lot?</p>\n<p></p>\n<p>The decoder training time with sparse attention of <code>0.15*block_size</code> is nearly the same as the one without sparse attention.</p>\n<p></p>\n<p>Meanwhile, the one with sparse attention gets wose perplexity (which make sense as it is not seeing as much as the tokens).</p>\n<p></p>\n<p>My question is: is this normal? From my understanding our block_size is only 32 while the ChatGPT uses 128k. Will it show time difference when you increase the block size? (I tried 512 but only have 2% difference, ~6s). Or is it because I am training on CPU and I may see better difference on GPU?</p>\n<p></p>\n<p>The below is the attention map, which I think it limit to 4 tokens.</p>\n<p></p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fm182ypevmgf41p%2F71399437d2a68c7d168053eee67c71fdb4ef6c5cc2523a9b0f46ec63129bf8ab%2FCleanShot_2024-11-06_at_16.36.13_2x.png\" alt=\"CleanShot_2024-11-06_at_16.36.13_2x.png\" /></p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":120,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-07T03:03:34Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Yunhao Jiang","endorser":{},"admin":false,"photo":null,"id":"m182ypevmgf41p","photo_url":null,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-07T03:03:34Z","content":"<md>Yes, sparse attention's benefits become more evident with larger sequences--32 is too small to see its impact.\n\nSince model does not \"see\" the entire previous tokens compared to the normal attention, it is possible you get worse perplexity.\n\nI think you can put these observations together and write down in report for your analysis.</md>"}],"type":"i_answer","tag_endorse_arr":["m182ypevmgf41p"],"children":[],"id":"m36q4306u1t1j5","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m36l1t3ixyh7bx","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990428964,"default_anonymity":"no"},"error":null,"aid":"m3nyds0ofvx2x3"}