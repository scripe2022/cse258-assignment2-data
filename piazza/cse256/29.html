{"result":{"history_size":3,"folders":["pa1"],"nr":29,"data":{"embed_links":[]},"created":"2024-10-08T20:27:35Z","bucket_order":3,"no_answer_followup":2,"change_log":[{"anon":"stud","data":"m20w74kmh2j1n0","v":"private","type":"create","when":"2024-10-08T20:27:35Z","uid_a":"a_0"},{"anon":"stud","data":"m20wgc6czzf5jf","v":"private","type":"update","when":"2024-10-08T20:34:44Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2104995qqd8v","to":"m20w74kecw81mz","type":"i_answer","when":"2024-10-08T22:17:19Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2105jpamse1j8","type":"i_answer_update","when":"2024-10-08T22:18:19Z"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m20w74kecw81mz","type":"followup","when":"2024-10-09T05:44:08Z","cid":"m21g2v4gvu477a"},{"anon":"stud","to":"m20w74kecw81mz","type":"feedback","when":"2024-10-09T05:54:17Z","cid":"m21gfxdpbfw7gx","uid_a":"a_0"},{"anon":"stud","data":"m21gglyhmxzkc","v":"all","type":"update","when":"2024-10-09T05:54:49Z","uid_a":"a_0"},{"anon":"stud","to":"m20w74kecw81mz","type":"followup","when":"2024-10-15T19:08:39Z","cid":"m2atgli8mlu3v","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m20w74kecw81mz","type":"feedback","when":"2024-10-16T00:51:45Z","cid":"m2b5pthacmvre"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"-1 word indices and padding tensors","created":"2024-10-09T05:54:49Z","content":"<p>Hello!<br /><br />I noticed that some word indices are equal to -1 (set in the index_of() utility function). This is also throwing an index out of range error for me since there&#39;s no embedding corresponding to -1 in the word embeddings. I also noticed that the embeddings start from index 2, (index 0 and 1 are all zeroes). Could you please clarify why the embeddings are structured that way?</p>\n<p></p>\n<p>To work around this, I&#39;m currently setting all the -1 values to 0 since a valid index has an embedding of all zeroes. However since I don&#39;t fully understand the purpose of the -1 indices, I wanted to check if this workaround might have any other repercussions. <br /><br />Moreover, since we&#39;ve been asked to provide a list of indices as input to the model rather than the embeddings themselves, I&#39;m running into a problem where each tensor is of a different size within a batch due to differing sentence lengths. I&#39;ve been trying to pad it but due to the already present -1 indices, I&#39;m still running into a problem. Could you please advise how to correctly tackle this issue? Are we expected to write a custom collate_fn?</p>\n<p></p>\n<p>Thank you!</p>"},{"anon":"stud","uid_a":"a_0","subject":"-1 word indices and padding tensors","created":"2024-10-08T20:34:44Z","content":"<p>Hello!<br /><br />I noticed that some word indices are equal to -1 (set in the index_of() utility function). This is also throwing an index out of range error for me since there&#39;s no embedding corresponding to -1 in the word embeddings. I also noticed that the embeddings start from index 2, (index 0 and 1 are all zeroes). Could you please clarify why the embeddings are structured that way?</p>\n<p></p>\n<p>To work around this, I&#39;m currently setting all the -1 values to 0 since a valid index has an embedding of all zeroes. However since I don&#39;t fully understand the purpose of the -1 indices, I wanted to check if this workaround might have any other repercussions. <br /><br />Moreover, since we&#39;ve been asked to provide a list of indices as input to the model rather than the embeddings themselves, I&#39;m running into a problem where each tensor is of a different size within a batch due to differing sentence lengths. I&#39;ve been trying to pad it but due to the already present -1 indices, I&#39;m still running into a problem. Could you please advise how to correctly tackle this issue? Are we expected to write a custom collate_fn?</p>\n<p></p>\n<p>Thank you!</p>"},{"anon":"stud","uid_a":"a_0","subject":"-1 word indices and padding tensors","created":"2024-10-08T20:27:35Z","content":"<p>Hello!<br /><br />I noticed that some word indices are equal to -1 (set in the index_of() utility function). This is also throwing an index out of range error for me since there&#39;s no embedding corresponding to -1 in the word embeddings. I also noticed that the embeddings start from index 2, (index 0 and 1 are all zeroes). Could you please clarify why the embeddings are structured that way?<br /><br />Moreover, since we&#39;ve been asked to provide a list of indices as an input to the model rather than the embeddings themselves, I&#39;m running into a problem where each tensor is of a different size within a batch due to differing sentence lengths. I&#39;ve been trying to pad it but due to the already present -1 indices, I&#39;m still running into a problem. Could you please advise how I can possibly tackle this issue?</p>\n<p></p>\n<p>Thank you!</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":223,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-08T22:17:19Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-08T22:18:19Z","content":"<p>Please take a look at <code>sentiment_data.py</code>:</p>\n<ol><li>\n<p>The <code>-1</code>&#39;s will be mapped to the <code>&lt;UNK&gt;</code> token, as written in the <code>get_embedding()</code> function.</p>\n</li><li>\n<p>The first two indices are reserved for <code>&lt;PAD&gt;</code> (padding) and <code>&lt;UNK&gt;</code> (unknown token), as implemented in the <code>read_word_embeddings()</code> function.</p>\n</li><li>\n<p>Yep, you may need to use a custom <code>collate_fn</code> or other padding methods to handle varying sentence lengths.</p>\n</li></ol>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-08T22:17:19Z","content":"<p>Please take a look at <code>sentiment_data.py</code>:</p>\n<ol><li>\n<p>The <code>-1</code>&#39;s will be mapped to the <code>&lt;UNK&gt;</code> token, as written in the <code>get_embedding()</code> function.</p>\n</li><li>\n<p>The first two indices are reserved for <code>&lt;PAD&gt;</code> (padding) and <code>&lt;UNK&gt;</code> (unknown token), as implemented in the <code>read_word_embeddings()</code> function.</p>\n</li><li>\n<p>You may need to write a custom <code>collate_fn</code> or other padding methods to handle varying sentence lengths.</p>\n</li></ol>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2104991pkq8u","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Also, would you mind making this post public instead of private (you can choose to remain anonymous to your classmates)? Others may have similar questions, and it could be helpful for them as well. Thanks!","created":"2024-10-09T05:44:08Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid":"ln0md59uz9w3kd","children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Yes, of course, I can do that. Thank you for clarifying my doubts!","created":"2024-10-09T05:54:17Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[{"role":"ta","name":"Po-Chun Wu","endorser":{},"admin":true,"photo":null,"id":"ln0md59uz9w3kd","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_0","children":[],"tag_good_arr":["ln0md59uz9w3kd"],"id":"m21gfxdpbfw7gx","updated":"2024-10-09T05:57:43Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m21g2v4gvu477a","updated":"2024-10-09T05:54:17Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Are we allowed to map the -1 values to 0 in the collate_fn or do we have to handle this differently?","created":"2024-10-15T19:08:39Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"It depends on your implementation of other parts, so I cannot give you a firm answer. But it sounds reasonable to assign 0 to the word embedding vector for an unknown token, as in <code>sentiment_data.py</code>. If you know what you are doing, just trust yourself!","created":"2024-10-16T00:51:45Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m2b5pthacmvre","updated":"2024-10-16T00:51:45Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m2atgli8mlu3v","updated":"2024-10-16T00:51:45Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m20w74kecw81mz","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990027582,"default_anonymity":"no"},"error":null,"aid":"m3ny56b6l427hc"}