{"result":{"history_size":1,"folders":["pa2"],"nr":368,"data":{"embed_links":[]},"created":"2024-11-04T02:19:23Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"no","uid":"k0wsczc7hho5w2","data":"m32e7pa4n3o36j","v":"all","type":"create","when":"2024-11-04T02:19:23Z"},{"anon":"stud","data":"m32hhmpnw9l28z","to":"m32e7p9yygj36i","type":"s_answer","when":"2024-11-04T03:51:05Z","uid_a":"a_0"},{"anon":"stud","to":"m32e7p9yygj36i","type":"followup","when":"2024-11-06T00:37:48Z","cid":"m355grtuuec5qs","uid_a":"a_1"},{"anon":"no","uid":"k0wsczc7hho5w2","to":"m32e7p9yygj36i","type":"feedback","when":"2024-11-06T00:42:37Z","cid":"m355mykqojg6m2"}],"bucket_name":"Today","history":[{"anon":"no","uid":"k0wsczc7hho5w2","subject":"Strange Part 2 Attention Map","created":"2024-11-04T02:19:23Z","content":"<p>Is anyone getting a uniformly distributed attention map (in the rows) for part 2? Any idea about possible causes? But I am getting perplexity in the right range....</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fk0wsczc7hho5w2%2F4ed979c64e6cd4318409491a9a2fe330af4d9a7616e9eb865adf8561ec83fd29%2Fimage.png\" alt=\"image.png\" /></p>\n<p></p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":141,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-04T03:51:05Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"","created":"2024-11-04T03:51:05Z","content":"this means the scores of i&#39;th token with respect to its previous tokens (j&#39;s) are same. Are you averaging the scores? if the input sentence is unique, there should be some variation in the lower traingle."}],"type":"s_answer","tag_endorse_arr":[],"children":[],"id":"m32hhmpimu228y","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"I too am getting this. What could be the reason?","created":"2024-11-06T00:37:48Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>I make two modifications to obtain a normal attention map:</p>\n<p>1. I directly used one batch of training LM loader rather using the `text` defined in main.py to calculate the attention map</p>\n<p>2. I draw the attention map directly when they are created, rather than passing them all way out from different classes, to avoid potential bugs</p>","created":"2024-11-06T00:42:37Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"k0wsczc7hho5w2","children":[],"tag_good_arr":[],"id":"m355mykqojg6m2","updated":"2024-11-06T00:42:37Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m355grtuuec5qs","updated":"2024-11-06T00:42:37Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m32e7p9yygj36i","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990370587,"default_anonymity":"no"},"error":null,"aid":"m3nyciz1k2x2tc"}