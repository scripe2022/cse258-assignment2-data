{"result":{"history_size":1,"folders":["pa2"],"nr":231,"data":{"embed_links":[]},"created":"2024-10-22T22:22:27Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"no","uid":"lmpga1ix7ii629","data":"m2l0gshs51f7nw","v":"all","type":"create","when":"2024-10-22T22:22:27Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2ldegtlfqr5zx","to":"m2l0gshjxkq7nv","type":"i_answer","when":"2024-10-23T04:24:34Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1ix7ii629","subject":"PA2: Confusable but interesting found about the training of GPT like transformer","created":"2024-10-22T22:22:27Z","content":"<p>For the second part of PA2, at first I used a complex predictor after decoder to map resentation vector back to vocabulary index. I found that the training perplexity would get to aroud 100 just after 100 epochs and at this time the valid perplexity could reach about 300 or 400. After this, the training perplexity still went down, which may be 20 or around at last, while the val perplexity, however, bacame larger and larger (over 1000). I thought there are some problems with my masking. But I double checked that and nothing wrong. So I switch to a simple predictor with only one layer. The training seems to be right. After 500 epochs, I could get around 100 training perlexity and  300 test perplexity. However, if I continue, extend max iter_num to 1000 or larger, similar thing occurs: The training perplexity went down to below 20 (though slowly) and after some middle epoch the val perplexity started growing up (also slowly). </p>\n<p>So, why is it.  Is this just a kind of ovefitting or is there something wrong with my implementation? </p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":177,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-23T04:24:34Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-23T04:24:34Z","content":"<md>Yes, that is overfitting.\n\n\"After 500 epochs, I could get around 100 training perplexity and 300 test perplexity\" -> this seems your implementation is okay.\n\nOver-training always causes overfitting. Please use 500 iterations following the instructions.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2ldegtfy4g5zw","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2l0gshjxkq7nv","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990233770,"default_anonymity":"no"},"error":null,"aid":"m3ny9len8aa3q0"}