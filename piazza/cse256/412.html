{"result":{"history_size":4,"folders":["pa2"],"nr":412,"data":{"embed_links":[]},"created":"2024-11-06T11:08:38Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m35s00xz1s6mk","v":"all","type":"create","when":"2024-11-06T11:08:38Z","uid_a":"a_0"},{"anon":"stud","data":"m35s0fpphkf7ji","v":"all","type":"update","when":"2024-11-06T11:08:57Z","uid_a":"a_0"},{"anon":"stud","data":"m35tbmsl8pijg","v":"all","type":"update","when":"2024-11-06T11:45:39Z","uid_a":"a_0"},{"anon":"stud","data":"m35ted2m9ed5oc","v":"all","type":"update","when":"2024-11-06T11:47:46Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m365i11q9j6l7","to":"m35s00xp3xp6mj","type":"i_answer","when":"2024-11-06T17:26:33Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Dropout after softmax?","created":"2024-11-06T11:47:46Z","content":"<md>I was refering to Andrzej Karpathy’s tutorial and its repo (https://github.com/karpathy/ng-video-lecture/) on building my implementation for PA2. His implementation of attention head goes like:\n```\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        ...\n    def forward(self, x):\n        ...\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        ...\n```\nWhere he first masks out future attention scores (affinities) and then normalizes each row and applys dropout layer. Is this implementation reasonble?\\\nWith non-zero dropout rate, it breaks the rule that attention scores should add up to 1 and causes sanity tests to fail. Swapping the last two lines does not seem to have a major performance impact on the first two tasks of PA2 (except failing sanity tests).\n\nMy version looks like this:\n```python\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 \n        wei = self.dropout(wei) \n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n```\nAs to pass the sanity test I have to apply softmax last. This alone, however, brings problems. If I don't further swap the ***`dropout`*** line and the ***`masked_fill`*** line, I may end up dropping a ***`-inf`*** entry. Is this still a correct way for implementing attention heads with dropout?</md>"},{"anon":"stud","uid_a":"a_0","subject":"Dropout after softmax?","created":"2024-11-06T11:45:39Z","content":"<md>I was refering to Andrzej Karpathy’s tutorial and its repo (https://github.com/karpathy/ng-video-lecture/) on building my implementation for PA2. His implementation of attention head goes like:\n```\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        ...\n    def forward(self, x):\n        ...\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        ...\n```\nWhere he first masks out future attention scores (affinities) and then normalizes each row and applys dropout layer. Is this implementation reasonble?\\\nWith non-zero dropout rate, it breaks the rule that attention scores should add up to 1 and causes sanity tests to fail. Swapping the last two lines does not seem to have a major performance impact on the first two tasks of PA2 (except failing sanity tests).\n\nMy version looks like this:\n```python\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 \n        wei = self.dropout(wei) \n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n```\nAs to pass the sanity test I have to apply softmax last - this brings problems if I don't further swap the `dropout` line and the `masked_fill` line since we may end up dropping a `-inf` entry. Is this the correct way for implementing dropout?</md>"},{"anon":"stud","uid_a":"a_0","subject":"Dropout after softmax?","created":"2024-11-06T11:08:57Z","content":"<md>I was refering to Andrzej Karpathy's tutorial and its repo (https://github.com/karpathy/ng-video-lecture/) on building my implementation for PA2. His implementation of attention head goes like:\n```python\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        ...\n    def forward(self, x):\n        ...\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        ...\n```\nWhere he normalizes attention scores (affinities) before applying the dropout layer. Is this implementation reasonble? With non-zero dropout rate, it breaks the rule that attention scores should  add up to 1 and causes sanity tests to fail. Swapping the two lines does not seem to have a major performance impact on the first two tasks of PA2 (except failing sanity tests).</md>"},{"anon":"stud","uid_a":"a_0","subject":"Dropout after softmax?","created":"2024-11-06T11:08:38Z","content":"<md>I was refering to Andrzej Karpathy's tutorial and its repo (https://github.com/karpathy/ng-video-lecture/) on building my implementation for PA2. I see in his implementation of attention head that goes like:\n```python\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        ...\n    def forward(self, x):\n        ...\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        ...\n```\nWhere he normalizes attention scores (affinities) before applying the dropout layer. Is this implementation reasonble? With non-zero dropout rate, it breaks the rule that attention scores should  add up to 1 and causes sanity tests to fail. Swapping the two lines does not seem to have a major performance impact on the first two tasks of PA2 (except failing sanity tests).</md>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":132,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-06T17:26:33Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-06T17:26:33Z","content":"<md>Hi, you don't need to use dropout in the Head class in this assignment.\n\nThe first way is correct implementation, but we need to sanity check so its okay not using drop out.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m365i11kyhil6","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":0,"id":"m35s00xp3xp6mj","config":{"editor":"md","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990417516,"default_anonymity":"no"},"error":null,"aid":"m3nydj6nzrj48t"}