{"result":{"history_size":1,"folders":["pa2"],"nr":299,"data":{"embed_links":[]},"created":"2024-10-29T22:08:58Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"no","uid":"m182ylsvjo43qg","data":"m2v02e8xz69v3","v":"all","type":"create","when":"2024-10-29T22:08:58Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2v2sd3jk5xxs","to":"m2v02e8n2gwv2","type":"i_answer","when":"2024-10-29T23:25:08Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2v2szg6tv1f7","type":"i_answer_update","when":"2024-10-29T23:25:37Z"},{"anon":"no","uid":"m182ylsvjo43qg","to":"m2v02e8n2gwv2","type":"followup","when":"2024-10-30T18:12:03Z","cid":"m2w71l4cnrrk"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2v02e8n2gwv2","type":"feedback","when":"2024-10-30T21:06:24Z","cid":"m2wd9sx7dw71vz"}],"bucket_name":"Today","history":[{"anon":"no","uid":"m182ylsvjo43qg","subject":"Part 2 perplexity gets too low","created":"2024-10-29T22:08:58Z","content":"In part 2, when I set train_LM_loader&#39;s shuffle to False, I will get training perplexity to near 100 and test perplexity to near 190s, which is still lower than expected. When I set train_LM_loader&#39;s shuffle to True, training perplexity drops to 5 and test perplexity drops to near 10. I have set the mask for the decoder using torch.triu. What is the possible reason for this?"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":173,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-29T23:25:08Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-29T23:25:37Z","content":"<p>Have you checked your attention map when masking is applied? Correct implementation will show something like this</p>\n<p></p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fku2mpmjms7n645%2F40e8f1cc85b0a74c399f76551cc4af859ee1c6887ca892b0901fd887a8c44d05%2Fimage.png\" alt=\"image.png\" width=\"163\" height=\"163\" /></p>\n<p></p>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-29T23:25:08Z","content":"<md>Have you checked your attention map when masking is applied? Correct implementation will show something like this\n\n![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fku2mpmjms7n645%2F40e8f1cc85b0a74c399f76551cc4af859ee1c6887ca892b0901fd887a8c44d05%2Fimage.png)</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2v2sd3ebtmxr","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>My attention matrix looks like this. What is the possible reason for this weird-looking matrix? Thanks.</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fm182ylsvjo43qg%2F3a2783c77954f9c2bf247b337787976f89231ad38f4f80bff54eae2b1bb11ca9%2Fimage.png\" alt=\"image.png\" width=\"231\" height=\"183\" /></p>","created":"2024-10-30T18:12:03Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid":"m182ylsvjo43qg","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>You need to implement masked-self attention. Make sure your implementation correctly masks out future tokens to prevent the model from ‘seeing’ them during training and test. That's why the upper triangle part in the plot i posted is black (the weights for future tokens are 0.0s)</md>","created":"2024-10-30T21:06:24Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2wd9sx7dw71vz","updated":"2024-10-30T21:06:24Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":1,"id":"m2w71l4cnrrk","updated":"2024-10-30T21:06:24Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m2v02e8n2gwv2","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":3,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990305932,"default_anonymity":"no"},"error":null,"aid":"m3nyb534cvxuk"}