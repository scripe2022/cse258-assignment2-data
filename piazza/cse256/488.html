{"result":{"history_size":1,"folders":["other"],"nr":488,"data":{"embed_links":[]},"created":"2024-11-17T00:32:57Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m3kv4wqkenx5zq","v":"all","type":"create","when":"2024-11-17T00:32:57Z","uid_a":"a_0"},{"anon":"no","uid":"lmr7yhnqdjb6ul","data":"m3n9ucsix141j2","to":"m3kv4wqeh8g5zp","type":"i_answer","when":"2024-11-18T17:00:11Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Clarification needed for Quiz2 Q3","created":"2024-11-17T00:32:57Z","content":"<p>Hi all, can anyone explain what choice A want to express for Q3?</p>\n<p>3. The literature states that RLHF aligns LLMs with ``human values”.  What is true?</p>\n<p> A) In reality, the model outputs are aligned to a set of labelers&#39; preferences that were influenced, among others things, by the instructions they were given, the context in which they received them (as a paid job), and who they received them from. </p>"}],"type":"question","tags":["other","student"],"tag_good":[],"unique_views":111,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-18T17:00:11Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"lmr7yhnqdjb6ul","subject":"","created":"2024-11-18T17:00:11Z","content":"It asks if the LLMs&#39; outputs will be aligned with the labelers&#39; preference, the instructions given, etc."}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m3n9ucsdjpw1j1","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m3kv4wqeh8g5zp","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990480891,"default_anonymity":"no"},"error":null,"aid":"m3nyew331l066v"}