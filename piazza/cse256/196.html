{"result":{"history_size":4,"folders":["pa1"],"nr":196,"data":{"embed_links":[]},"created":"2024-10-19T07:04:38Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2ftcwy6v371ao","v":"private","type":"create","when":"2024-10-19T07:04:38Z","uid_a":"a_0"},{"anon":"stud","data":"m2ftf8tqeeq53n","v":"private","type":"update","when":"2024-10-19T07:06:27Z","uid_a":"a_0"},{"anon":"stud","data":"m2ftoydqbuz53k","v":"private","type":"update","when":"2024-10-19T07:14:00Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2ftra8pcn7x5","to":"m2ftcwxzsep1an","type":"i_answer","when":"2024-10-19T07:15:49Z"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m2ftcwxzsep1an","type":"followup","when":"2024-10-19T07:17:58Z","cid":"m2ftu27eayr54i"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2ftv8veaab6yl","type":"i_answer_update","when":"2024-10-19T07:18:54Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2fu92i8wfz6k5","type":"i_answer_update","when":"2024-10-19T07:29:39Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2fu9uja5zn7pe","type":"i_answer_update","when":"2024-10-19T07:30:15Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2fuesard737p7","type":"i_answer_update","when":"2024-10-19T07:34:05Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2fufbvez45t1","type":"i_answer_update","when":"2024-10-19T07:34:31Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2fuh6e9wqo3kp","type":"i_answer_update","when":"2024-10-19T07:35:57Z"},{"anon":"no","uid":"kfoqmke78c463e","data":"m2h6hcqwrrk5dj","v":"all","type":"update","when":"2024-10-20T05:59:47Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"kfoqmke78c463e","subject":"BPE merging","created":"2024-10-20T05:59:47Z","content":"<p>Hi, I have another question regarding BPE merging.</p>\n<p>Suppose I have a word &#39;hugs&#39;, and the indexer looks like [h,u,g,s,hu,ug,ugs,] , should I tokenize the word into &#39;h ugs&#39; or &#39;hu g s&#39;? It seems that &#39;h ugs&#39; makes more sense since it contains the longest meaningful subword, but if I choose to go from left to right over the indexer, &#39;hu&#39; will be the first to appear, making the word into &#39;hu g s&#39;. Which way of tokenization should be right? &#39;hu g s&#39; seems to break up the data into smaller pieces, and makes the final tokenized data larger that the previous one.</p>\n<p></p>\n<p>A more extreme situation will be [h,u,g,s,ug,ugs]. (h has the highest frequency) In this case the final outcome will be &#39;h u g s&#39;? That&#39;s very strange. </p>"},{"anon":"stud","uid_a":"a_0","subject":"BPE merging","created":"2024-10-19T07:14:00Z","content":"<p>Hi, I have another question regarding BPE merging.</p>\n<p>Suppose I have a word &#39;hugs&#39;, and the indexer looks like [h,u,g,s,hu,ug,ugs,] , should I tokenize the word into &#39;h ugs&#39; or &#39;hu g s&#39;? It seems that &#39;h ugs&#39; makes more sense since it contains the longest meaningful subword, but if I choose to go from left to right over the indexer, &#39;hu&#39; will be the first to appear, making the word into &#39;hu g s&#39;. Which way of tokenization should be right? &#39;hu g s&#39; seems to break up the data into smaller pieces, and makes the final tokenized data larger that the previous one.</p>\n<p></p>\n<p>A more extreme situation will be [h,u,g,s,ug,ugs]. (h has the highest frequency) In this case the final outcome will be &#39;h u g s&#39;? That&#39;s very strange. </p>"},{"anon":"stud","uid_a":"a_0","subject":"BPE merging","created":"2024-10-19T07:06:27Z","content":"<p>Hi, I have another question regarding BPE merging.</p>\n<p>Suppose I have a word &#39;hugs&#39;, and the indexer looks like [h,u,g,s,hu,ug,ugs,] , should I tokenize the word into &#39;h ugs&#39; or &#39;hu g s&#39;? It seems that &#39;h ugs&#39; makes more sense since it contains the longest meaningful subword, but if I choose to go from left to right over the indexer, &#39;hu&#39; will be the first to appear, making the word into &#39;hu g s&#39;. Which way of tokenization should be right? &#39;hu g s&#39; seems to break up the data into smaller pieces, and makes the final tokenized data larger that the previous one.</p>"},{"anon":"stud","uid_a":"a_0","subject":"BPE merging","created":"2024-10-19T07:04:38Z","content":"<p>Hi, I have another question regarding BPE merging.</p>\n<p>Suppose I have a word &#39;hugs&#39;, and the indexer looks like [h,u,g,s,hu,ug,ugs,] , should I tokenize the word into &#39;h ugs&#39; or &#39;hu g s&#39;? It seems that &#39;h ugs&#39; makes more sense since it contains the longest meaningful subword, but if I choose to go from left to right over the indexer, &#39;hu&#39; will be the first to appear, making the word into &#39;hu g s&#39;. Which way of tokenization should be right?</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":98,"children":[{"history_size":7,"folders":[],"data":{"embed_links":[]},"created":"2024-10-19T07:15:49Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T07:35:57Z","content":"<p>*Edited:<br />Based on your first example ([h,u,g,s,hu,ug,ugs]), you should tokenize &#34;hugs&#34; into &#34;hu&#34;, &#34;g&#34;, and &#34;s&#34;, following the left-to-right process, where &#34;hu&#34; is merged first since it appears earlier in the indexer. This follows the standard BPE algorithm where you <strong>merge the most frequent subword pairs step by step</strong>, even if it seems to result in smaller pieces.<br /><br />As for your second example ([h,u,g,s,ug,ugs]), the result should be &#34;h&#34; and &#34;ugs&#34;. <br /><br /></p>\n<p>For more details, please refer to the <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=2901s\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube tutorial</a>.</p>\n<p></p>\n<p>Hint:</p>\n<p>The initial vocabulary (e.g., individual characters) is not included in the merge list when applying/encoding BPE.</p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T07:34:31Z","content":"<p>*Edited:<br />Based on your first example ([h,u,g,s,hu,ug,ugs]), you should tokenize &#34;hugs&#34; into &#34;hu&#34;, &#34;g&#34;, and &#34;s’&#34;, following the left-to-right process, where &#34;hu&#34; is merged first since it appears earlier in the indexer. This follows the standard BPE algorithm where you <strong>merge the most frequent subword pairs step by step</strong>, even if it seems to result in smaller pieces.<br /><br />As for your second example ([h,u,g,s,ug,ugs]), the result should be &#34;h&#34; and &#34;ugs&#34;. <br /><br /></p>\n<p>For more details, please refer to the <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=2901s\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube tutorial</a>.</p>\n<p></p>\n<p>Hint:</p>\n<p>The initial vocabulary (e.g., individual characters) is not included in the merge list when applying/encoding BPE.</p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T07:34:05Z","content":"<p>*Edited:<br />Based on your first example ([h,u,g,s,hu,ug,ugs]), you should tokenize &#34;hugs&#34; into &#34;hu&#34;, &#34;g&#34;, and &#34;s’&#34;, following the left-to-right process, where &#34;hu&#34; is merged first since it appears earlier in the indexer. This follows the standard BPE algorithm where you <strong>merge the most frequent subword pairs step by step</strong>, even if it seems to result in smaller pieces.<br /><br />As for your second example ([h,u,g,s,ug,ugs]), the result should be &#34;h&#34; and &#34;ugs&#34;. <br /><br /></p>\n<p>For more details, please refer to the <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=2901s\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube tutorial</a>.</p>\n<p></p>\n<p>Hint:</p>\n<p>The initial vocabulary (e.g., individual characters) is not included in the merge list when applying BPE.</p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T07:30:15Z","content":"<p>*Edited:<br />Based on your first example, you should tokenize &#34;hugs&#34; into &#34;hu&#34; &#34;g&#34;, and &#34;s’&#34;, following the left-to-right process, where &#34;hu&#34; is merged first since it appears earlier in the indexer. This follows the standard BPE algorithm where you <strong>merge the most frequent subword pairs step by step</strong>, even if it seems to result in smaller pieces.<br /><br />As for your second example, the result should be &#34;h&#34; and &#34;ugs&#34;. <br /><br /></p>\n<p>For more details, please refer to the <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=2901s\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube tutorial</a>.</p>\n<p></p>\n<p>Hint:</p>\n<p>The initial vocabulary (e.g., individual characters) is not included in the merge list when applying BPE.</p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T07:29:39Z","content":"<p>*Edited:<br />Based on your first example, you should tokenize &#34;hugs&#34; into &#34;hu&#34; &#34;g&#34;, and &#34;s’&#34;, following the left-to-right process, where &#34;hu&#34; is merged first since it appears earlier in the indexer. This follows the standard BPE algorithm where you merge the most frequent subword pairs step by step, even if it seems to result in smaller pieces.<br /><br />As for your second example, the result should be &#34;h&#34; and &#34;ugs&#34;. <br /><br /></p>\n<p>For more details, please refer to the <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=2901s\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube tutorial</a>.</p>\n<p></p>\n<p>Hint:</p>\n<p>The initial vocabulary (e.g., individual characters) is not included in the merge list when applying BPE.</p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T07:18:54Z","content":"Based on your first example, you should tokenize ‘hugs’ into ‘hu g s’ following the left-to-right process, where ‘hu’ is merged first since it appears earlier in the indexer. This follows the standard BPE algorithm where you merge the most frequent subword pairs step by step, even if it seems to result in smaller pieces."},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T07:15:49Z","content":"Based on your example, you should tokenize ‘hugs’ into ‘hu g s’ following the left-to-right process, where ‘hu’ is merged first since it appears earlier in the indexer. This follows the standard BPE algorithm where you merge the most frequent subword pairs step by step, even if it seems to result in smaller pieces."}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2ftra8l4p3x4","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>Also, would you mind making this post public instead of private (you can choose to remain anonymous to your classmates)? Others may have similar questions, and it could be helpful for them as well. </p><br /><p>Please only send us private messages when personal information is shared.</p><br /><p>Thanks!</p>","created":"2024-10-19T07:17:58Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"no_answer":1,"id":"m2ftu27eayr54i","d-bucket":"Yesterday","updated":"2024-10-19T07:17:58Z","config":{}}],"tag_good_arr":[],"no_answer":0,"id":"m2ftcwxzsep1an","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990211008,"default_anonymity":"no"},"error":null,"aid":"m3ny93ubowva5"}