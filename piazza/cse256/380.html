{"result":{"history_size":2,"folders":["pa2"],"nr":380,"data":{"embed_links":[]},"created":"2024-11-04T18:26:08Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m33cqy0ziej1gd","v":"all","type":"create","when":"2024-11-04T18:26:08Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m33o05poxbj239","to":"m33cqy0foj31g9","type":"i_answer","when":"2024-11-04T23:41:13Z"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m33wdjyyb5nfw","type":"i_answer_update","when":"2024-11-05T03:35:35Z"},{"anon":"stud","uid_a":"a_1","data":"m37ppw35hr679w","v":"all","type":"update","when":"2024-11-07T19:40:18Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_1","subject":"Can we just to sanity check before training to test attention map implementation?","created":"2024-11-07T19:40:18Z","content":"<p>Follow up on @354:<br /><br /></p>\n<p>If the untrained attention heads are giving attention to padded tokens, is that fine? I am only doing sanity check on the untrained attention heads and I figured the model hasn&#39;t learned to ignore the padding.</p>\n<p></p>\n<p>Additionally, is it fine if the encoder/decoder doesn&#39;t return the attention maps/matrices? The way I have my code set up currently I can&#39;t find a good way to pass the attention matrix from an attention head to the multi head class to the encoder block to the encoder layers and then onto the final transformer model, so I am just creating a separate sanity testing class that does the embedding and creates the attention matrix in one separate class, and that is being passed as a model to the sanity check function. Is this acceptable?</p>\n<p>I will include the attention maps in the report.</p>\n<p></p>\n<p>Looking at the instructions given in the assignment, I assumed that the sanity check was just to check the calculation implementation and not for analysis. Also, I went off of the comments in <a href=\"/class/m119j4046iw6dq/post/282\">@282</a> and already made my entire setup without passing the attention matrices...</p>\n<p></p>\n<div>\n<div>\n<pre>\nPart 1.4: Sanity Checks\nSanity check your attention implementation using the helper function provided in utilities.py. This function will verify that the rows of the attention matrix sum to 1 and will also visualize the attention matrix. Include plots of attention matrices for one or two sentences in your report (you can choose any layer(s) and any head(s)). Discuss what you observe in these plots.</pre>\n</div>\n</div>"},{"anon":"stud","uid_a":"a_0","subject":"Can we just to sanity check before training to test attention map implementation?","created":"2024-11-04T18:26:08Z","content":"<p>Follow up on @354:<br /><br /></p>\n<p>If the untrained attention heads are giving attention to padded tokens, is that fine? I am only doing sanity check on the untrained attention heads and I figured the model hasn&#39;t learned to ignore the padding.</p>\n<p></p>\n<p>Additionally, is it fine if the encoder/decoder doesn&#39;t return the attention maps/matrices? The way I have my code set up currently I can&#39;t find a good way to pass the attention matrix from an attention head to the multi head class to the encoder block to the encoder layers and then onto the final transformer model, so I am just creating a separate sanity testing class that does the embedding and creates the attention matrix in one separate class, and that is being passed as a model to the sanity check function. Is this acceptable?</p>\n<p>I will include the attention maps in the report.</p>\n<p></p>\n<p>Looking at the instructions given in the assignment, I assumed that the sanity check was just to check the calculation implementation and not for analysis. Also, I went off of the comments in <a href=\"/class/m119j4046iw6dq/post/282\">@282</a> and already made my entire setup without passing the attention matrices...</p>\n<p></p>\n<div>\n<div>\n<pre>\nPart 1.4: Sanity Checks\nSanity check your attention implementation using the helper function provided in utilities.py. This function will verify that the rows of the attention matrix sum to 1 and will also visualize the attention matrix. Include plots of attention matrices for one or two sentences in your report (you can choose any layer(s) and any head(s)). Discuss what you observe in these plots.</pre>\n</div>\n</div>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":177,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-11-04T23:41:13Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-11-05T03:35:35Z","content":"<p>“Discuss what you observe in these plots.” means you need to do actual analysis with trained attention heads. Just saying ‘attention weights are random with the untrained model’ will not get full credits.</p>\n<p></p>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-04T23:41:13Z","content":"<md>\"Discuss what you observe in these plots.\" means you need to do actual analysis with trained  attention heads. Just saying 'attention weights are random with the untrained model' will not get full credits.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m33o05pbi65237","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m33cqy0foj31g9","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990384563,"default_anonymity":"no"},"error":null,"aid":"m3nyctrbna35bf"}