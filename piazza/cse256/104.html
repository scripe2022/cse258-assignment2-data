{"result":{"history_size":1,"folders":["pa1"],"nr":104,"data":{"embed_links":[]},"created":"2024-10-15T05:12:22Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m29zl4apisb2j8","v":"all","type":"create","when":"2024-10-15T05:12:22Z","uid_a":"a_0"},{"anon":"stud","to":"m29zl4ahcu22j7","type":"followup","when":"2024-10-15T05:23:23Z","cid":"m29zzag99neyi","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2a2e6yid33hp","to":"m29zl4ahcu22j7","type":"i_answer","when":"2024-10-15T06:30:57Z"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m29zl4ahcu22j7","type":"feedback","when":"2024-10-15T06:31:06Z","cid":"m2a2edlmzv33rc"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"How to split input into tokens present in the BPE vocabulary?","created":"2024-10-15T05:12:22Z","content":"<p>Hi,</p>\n<p></p>\n<p>I got the BPE embeddings from train.txt by combining all the sentences into one long list and running BPE. Now I need to tokenize the input on train or dev sentence by sentence (in batches of 16).</p>\n<p></p>\n<p>I&#39;m not sure how I tokenize the input sentence based on the existing vocab dictionary. Since I already have the vocabulary, do I need to compare each and every token in the vocabulary dictionary to every portion of the input sentence?</p>\n<p></p>\n<p>I can&#39;t split the input sentence like we did with BPE because that would split all the tokens present, but I don&#39;t understand how to tokenize it while leaving the input sentence together and returning the indices of the tokens in the dictionary?</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":164,"children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Also should we still include the UNK token in the embeddings if the given input sentence doesn&#39;t match any of the tokens in the vocabulary dictionary?Â ","created":"2024-10-15T05:23:23Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Yes.","created":"2024-10-15T06:31:06Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m2a2edlmzv33rc","updated":"2024-10-15T06:31:06Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m29zzag99neyi","updated":"2024-10-15T06:31:06Z","config":{"editor":"rte"}},{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-15T06:30:57Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-15T06:30:57Z","content":"Please take a look at the <a href=\"https://arxiv.org/pdf/1508.07909\" target=\"_blank\" rel=\"noopener noreferrer\">BPE paper</a> and this <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube tutorial </a>that explain how the BPE tokenizer works. These should help clarify how to tokenize input based on your existing vocabulary. Let us know if you still have questions after reviewing the materials. Thanks!"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2a2e6ybsk3hn","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m29zl4ahcu22j7","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990108439,"default_anonymity":"no"},"error":null,"aid":"m3ny6wp52yfb6"}