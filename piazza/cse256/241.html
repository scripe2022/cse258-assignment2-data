{"result":{"history_size":1,"folders":["pa2"],"nr":241,"data":{"embed_links":[]},"created":"2024-10-24T21:36:11Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2ntozp8kbl44n","v":"all","type":"create","when":"2024-10-24T21:36:11Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2nusvhmpl16ml","to":"m2ntozp1sk444m","type":"i_answer","when":"2024-10-24T22:07:12Z"},{"anon":"stud","to":"m2ntozp1sk444m","type":"followup","when":"2024-10-24T22:12:57Z","cid":"m2nv09i8tnp1h3","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2ntozp1sk444m","type":"feedback","when":"2024-10-25T01:44:53Z","cid":"m2o2ktr8bbh4t3"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Target vals for Decoder","created":"2024-10-24T21:36:11Z","content":"The decoder should output the probabilities of the next words in the sequence(so softmax after classifier), what are the target labels yb in this loop? They are not probabilities right? so how do I go about minimizing loss from this?<br />thanks!<img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fm182yqdoa2644c%2Fd6085137407034501004c77cffc6d042608a584fd0765943eadb3e250bf6b863%2FScreenshot_2024-10-24_at_2.35.05_PM.png\" alt=\"Screenshot_2024-10-24_at_2.35.05_PM.png\" />"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":130,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-24T22:07:12Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-24T22:07:12Z","content":"<md>Target label here is an index of vocab. For example, if our vocab size is 50000, this can be regarded 50000-class classification, so y ranges (0, 49999). Like we used the label 0 or 1 in the 2-class (binary) classification in PA1, this is |V|-class classification.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2nusvhh4tk6mk","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Okay i understand that,<div>But when i run my decoder on x, it outputs a tensor of probabilities, how do i go from there to computing cross entropy? Do i also call softmax on yb?</div><div>Thanks</div>","created":"2024-10-24T22:12:57Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>You don't need to call softmax if you use `torch.nn.CrossEntropyLoss`. Other details depend on how you design the model though. I think the lecture will cover more details soon.</md>","created":"2024-10-25T01:44:53Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2o2ktr8bbh4t3","updated":"2024-10-25T01:44:53Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":1,"id":"m2nv09i8tnp1h3","updated":"2024-10-25T01:44:53Z","config":{}}],"tag_good_arr":["m182ygrlgf83d9"],"no_answer":0,"id":"m2ntozp1sk444m","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990246491,"default_anonymity":"no"},"error":null,"aid":"m3ny9v7yb8c6vo"}