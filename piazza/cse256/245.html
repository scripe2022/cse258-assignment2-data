{"result":{"history_size":1,"folders":["pa2"],"nr":245,"data":{"embed_links":[]},"created":"2024-10-25T03:08:33Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2o5keidw774n2","v":"all","type":"create","when":"2024-10-25T03:08:33Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2odcgf8pyx2p0","to":"m2o5kei47tm4n1","type":"i_answer","when":"2024-10-25T06:46:19Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Should we train the encoder sentence by sentence when its being passed to the encoder layer?","created":"2024-10-25T03:08:33Z","content":"Is my understanding of how the encoder works correct? First we get the word embeddings, then get their positional embeddings. Next we sum the word and positional embeddings. Then, for each layer in the encoder layer ( which does the multi-head attention) , we pass in the summed tensors (i.e. what we get from layer 1, we pass to layer 2, what we from layer 2 pass to layer 3 ....). After getting result of these, we average the stacked tensors we got from the last layer and then pass to the linear classifier. Is my understanding of the encoding algorithm right?"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Rahul Sharma Nemmani","endorser":{},"admin":false,"photo":null,"id":"jml95ci0otv4w9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Siya Rajpal","endorser":{},"admin":false,"photo":null,"id":"kfo6q32ufu148m","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Madurya Suresh","endorser":{},"admin":false,"photo":null,"id":"m182yu2idsy4f6","photo_url":null,"us":false,"facebook_id":null},{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":140,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-25T06:46:19Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-25T06:46:19Z","content":"<md>Yes, I think you are on the right track.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2odcgf2pt52oz","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["jml95ci0otv4w9","m182ygrlgf83d9","kfo6q32ufu148m","m182yu2idsy4f6","kfsi52ar6572xo"],"no_answer":0,"id":"m2o5kei47tm4n1","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990251550,"default_anonymity":"no"},"error":null,"aid":"m3ny9z4ijjk4sk"}