{"result":{"history_size":1,"folders":["pa2"],"nr":281,"data":{"embed_links":[]},"created":"2024-10-27T20:48:51Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2s2bobzwxx6w3","v":"all","type":"create","when":"2024-10-27T20:48:51Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2sn05yci0d1nm","to":"m2s2bobskog6w2","type":"i_answer","when":"2024-10-28T06:27:46Z"},{"anon":"stud","to":"m2s2bobskog6w2","type":"followup","when":"2024-10-28T18:25:21Z","cid":"m2tcmyzkuhf7n1","uid_a":"a_1"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s2bobskog6w2","type":"feedback","when":"2024-10-29T04:41:00Z","cid":"m2tymph8dtf3i9"},{"anon":"stud","to":"m2s2bobskog6w2","type":"feedback","when":"2024-10-29T14:22:55Z","cid":"m2ujf23kg9c2kt","uid_a":"a_1"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m2s2bobskog6w2","type":"feedback","when":"2024-10-29T17:06:16Z","cid":"m2up94i8o3ks2"},{"anon":"stud","to":"m2s2bobskog6w2","type":"feedback","when":"2024-10-29T17:12:26Z","cid":"m2uph2i1t8o3q9","uid_a":"a_1"},{"anon":"stud","to":"m2s2bobskog6w2","type":"feedback","when":"2024-10-31T23:44:18Z","cid":"m2xycpn4qlf7g2","uid_a":"a_0"},{"anon":"stud","to":"m2s2bobskog6w2","type":"feedback","when":"2024-11-01T02:02:34Z","cid":"m2y3aj56g51575","uid_a":"a_2"},{"anon":"stud","to":"m2s2bobskog6w2","type":"feedback","when":"2024-11-04T05:22:09Z","cid":"m32kqqsk4d6fk","uid_a":"a_3"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"sanity check for sentences that doesn&#39;t have 32 tokens","created":"2024-10-27T20:48:51Z","content":"<p>To my understanding, the attention map that we should plot should look like a 32*32 grid, each dimension represents a vocabulary, and the cell value is the attention value. Is it possible that the sentence we choose doesn&#39;t have 32 tokens?</p>\n<p></p>\n<p>If it&#39;s possible,</p>\n<p>- If it has more than 32 tokens, should we just use the first 32 tokens in the sentence for the attention map?</p>\n<p>- If it has less than 32 tokens, there will be some dimensions in the attention map that doesn&#39;t correspond to any vocab. In this case, do we still need to output a 32*32 grid? or should we make the grid smaller and let the dimension be number of tokens in the sentence?</p>\n<p></p>\n<p>Please correct me if I have any misunderstanding. Thank you!</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":163,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-28T06:27:46Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-28T06:27:46Z","content":"<md>If it has more than 32 tokens, just use the first 32 tokens in the sentence,\n\nIf it has less than 32 tokens, the attention weight of the remaining is 0, so these tokens will appear black in the plot. You don't need to make the grid smaller.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2sn05y6vqp1nl","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"In the tutorial, there doesnâ€™t seem to be a mechanism to ensure that the attention weights of the pad token are set to zero, does there? To address this, we could either generate an attention mask in the transformer code to pass to the Head class or modify the SimpleTokenizer to return an attention mask as well. Is it necessary to ensure zero attention on pad tokens, or would non-zero attention on them be acceptable?","created":"2024-10-28T18:25:21Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>You don't need to generate an attention mask here. In `collate_batch`, sentences are padded and your multi-head attention will learn ignoring pad tokens through training.</md>","created":"2024-10-29T04:41:00Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2tymph8dtf3i9","updated":"2024-10-29T04:41:00Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Yes, that was also my expectation. However, the plots show some non-zero attention values on pad tokens, even though all sanity checks have passed and the required accuracy has been reached. So, the implementation itself should be correct.","created":"2024-10-29T14:22:55Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_1","children":[],"tag_good_arr":["m182ygrlgf83d9"],"id":"m2ujf23kg9c2kt","updated":"2024-10-31T22:47:12Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Can you share the plot on the private post?</md>","created":"2024-10-29T17:06:16Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m2up94i8o3ks2","updated":"2024-10-29T17:06:16Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Sure, I&#39;ve made a private post.","created":"2024-10-29T17:12:26Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m2uph2i1t8o3q9","updated":"2024-10-29T17:12:26Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>hi, I also got the same case, where there are some non-zero attention values on pad tokens, but all sanity checks have passed and the required accuracy has been reached. Is this an abnormal case? does this mean there is something wrong with my code?</p>\n<p></p>\n<p>Thank you!</p>","created":"2024-10-31T23:44:18Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m2xycpn4qlf7g2","updated":"2024-10-31T23:44:18Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I also saw non-zero attention in padding tokens (when testing on a single untrained attention head), would be keen to know what the issue here is.","created":"2024-11-01T02:02:34Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_2","children":[],"tag_good_arr":[],"id":"m2y3aj56g51575","updated":"2024-11-01T02:02:34Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<md>I am also meeting this issue that there are some non-zero attention values on pad token. Is there any update? Thanks.</md>","created":"2024-11-04T05:22:09Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_3","children":[],"tag_good_arr":["m182ygrlgf83d9"],"id":"m32kqqsk4d6fk","updated":"2024-11-04T06:44:14Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":1,"id":"m2tcmyzkuhf7n1","updated":"2024-11-04T05:22:09Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":0,"id":"m2s2bobskog6w2","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":5,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990288204,"default_anonymity":"no"},"error":null,"aid":"m3nyaren5akj4"}