{"result":{"history_size":2,"folders":["pa1"],"nr":51,"data":{"embed_links":[]},"created":"2024-10-11T20:20:34Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2569o5x8b27ng","v":"private","type":"create","when":"2024-10-11T20:20:34Z","uid_a":"a_0"},{"anon":"stud","data":"m258yk0j2wq64e","v":"all","type":"update","when":"2024-10-11T21:35:54Z","uid_a":"a_0"},{"anon":"stud","data":"m25bnhq4lie5mn","to":"m2569o5nbs7nf","type":"s_answer","when":"2024-10-11T22:51:17Z","uid_a":"a_1"},{"anon":"stud","to":"m2569o5nbs7nf","type":"followup","when":"2024-10-11T22:51:45Z","cid":"m25bo3f5vo06d5","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m25lexd68vr33r","to":"m2569o5nbs7nf","type":"i_answer","when":"2024-10-12T03:24:34Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m25lkht8lf422l","type":"i_answer_update","when":"2024-10-12T03:28:53Z"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m27tcgou97e2ey","type":"i_answer_update","when":"2024-10-13T16:42:08Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Confusion about what to do with the train and test file for DAN","created":"2024-10-11T21:35:54Z","content":"<p>Hi,</p>\n<p></p>\n<p>I have loaded the relativized embeddings into the DAN dataset class using:</p>\n<div>\n<div>embs = read_word_embeddings(&#34;data/glove.6B.50d-relativized.txt&#34;)</div>\n</div>\n<p></p>\n<p>Now I am unsure about how to handle training and testing datasets. In BOW, the countVectorizer was simply counting the occurrence of each word and storing it in a sparse matrix. It does this for both the training and testing datasets (lines 94-97 in main.py):</p>\n<div>\n<div>train_data = SentimentDatasetBOW(&#34;data/train.txt&#34;)</div>\n<div>dev_data = SentimentDatasetBOW(&#34;data/dev.txt&#34;)</div>\n<div>train_loader = DataLoader(train_data, batch_size=16, shuffle=True)</div>\n<div>test_loader = DataLoader(dev_data, batch_size=16, shuffle=False)</div>\n</div>\n<div></div>\n<div>1) How would the DAN dataset loader handle this? since we are already given the pretrained embeddings, what should it do with the training and testing dataset?</div>\n<div>The BOW example parses the test and train files (using read_sentiment_examples) and creates a sentiment object with a label and the words corresponding to that label (after cleaning up the sentence). But then it goes on to do the wordcount of the sentences, and I&#39;m not sure how the pretrained embeddings come into play here?</div>\n<div></div>\n<div>2) I know I&#39;m supposed to average the embeddings and then pass it into the NN. Does that mean doing the column-wise average of the relativized dataset? so the 50d one would create a 1x50 array of the embeddings mean, and the 300d one would create a 1x300 dimensional array?</div>\n<div><br />3) Finally, I am not sure what I need to do with the training and testing data in the DAN data loader? </div>\n<div></div>\n<div>I have been stuck on this for a while, trying to understand all the helper code and libraries provided but I am unable to proceed. Any help will be greatly appreciated :)</div>\n<div></div>"},{"anon":"stud","uid_a":"a_0","subject":"Confusion about what to do with the train and test file for DAN","created":"2024-10-11T20:20:34Z","content":"<p>Hi,</p>\n<p></p>\n<p>I have loaded the relativized embeddings into the DAN dataset class using:</p>\n<div>\n<div>embs = read_word_embeddings(&#34;data/glove.6B.50d-relativized.txt&#34;)</div>\n</div>\n<p></p>\n<p>Now I am unsure about how to handle training and testing datasets. In BOW, the countVectorizer was simply counting the occurrence of each word and storing it in a sparse matrix. It does this for both the training and testing datasets (lines 94-97 in main.py):</p>\n<div>\n<div>train_data = SentimentDatasetBOW(&#34;data/train.txt&#34;)</div>\n<div>dev_data = SentimentDatasetBOW(&#34;data/dev.txt&#34;)</div>\n<div>train_loader = DataLoader(train_data, batch_size=16, shuffle=True)</div>\n<div>test_loader = DataLoader(dev_data, batch_size=16, shuffle=False)</div>\n</div>\n<div></div>\n<div>1) How would the DAN dataset loader handle this? since we are already given the pretrained embeddings, what should it do with the training and testing dataset?</div>\n<div>The BOW example parses the test and train files (using read_sentiment_examples) and creates a sentiment object with a label and the words corresponding to that label (after cleaning up the sentence). But then it goes on to do the wordcount of the sentences, and I&#39;m not sure how the pretrained embeddings come into play here?</div>\n<div></div>\n<div>2) I know I&#39;m supposed to average the embeddings and then pass it into the NN. Does that mean doing the column-wise average of the relativized dataset? so the 50d one would create a 1x50 array of the embeddings mean, and the 300d one would create a 1x300 dimensional array?</div>\n<div><br />3) Finally, I am not sure what I need to do with the training and testing data in the DAN data loader? </div>\n<div></div>\n<div>I have been stuck on this for a while, trying to understand all the helper code and libraries provided but I am unable to proceed. Any help will be greatly appreciated :)</div>\n<div></div>"}],"type":"question","tags":["pa1","student"],"tag_good":[{"role":"student","name":"Sarvani Kunapareddy","endorser":{},"admin":false,"photo":"a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","id":"m182yor466x3zz","photo_url":"https://cdn-uploads.piazza.com/photos/m182yor466x3zz/a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","published":true,"us":false,"facebook_id":null},{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":179,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-11T22:51:17Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_1","subject":"","created":"2024-10-11T22:51:17Z","content":"I have been having the same question and have been wracking my head. Not sure what data should be used to train DAN. "}],"type":"s_answer","tag_endorse_arr":[],"children":[],"id":"m25bnhpyvra5mm","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"I have been having the same question and have been wracking my head. Not sure what data should be used to train DAN.","created":"2024-10-11T22:51:45Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"no_answer":1,"id":"m25bo3f5vo06d5","updated":"2024-10-11T22:51:45Z","config":{"editor":"rte"}},{"history_size":3,"folders":[],"data":{"embed_links":[]},"created":"2024-10-12T03:24:34Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Forrest Dai","endorser":{},"admin":false,"photo":null,"id":"lvdiwwr2smn3tq","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-13T16:42:08Z","content":"<p>Similar to BOW, for your DAN model, you will load the training data from <code>train.txt</code> and the test data from <code>dev.txt</code>. In DAN, instead of counting word occurrences like BOW, you will convert words to their corresponding embeddings, average them, and then pass them to the neural network.</p>\n<p></p>\n<p>If you&#39;re unsure how the DAN model works, you may want to refer to the <a target=\"_new\" href=\"https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf\" rel=\"noopener noreferrer\">DAN paper</a> for more details.</p>\n<p></p>\n<p>Hints:</p>\n<p>In Part 1, there are two phases of mapping:</p>\n<ol><li><b>Converting Words to Word Indices</b>: In this phase, you map each unique word in your vocabulary to a corresponding index.</li><li><b>Converting Word Indices to Word Embeddings</b>: In this phase, you take the word indices and convert them into their respective word embeddings. </li></ol>\n<p></p>\n<p>Also, @46 might be helpful.</p>\n<p></p>\n<p>If you&#39;re still having trouble, feel free to drop by the office hours next week. </p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-12T03:28:53Z","content":"<p>Similar to BOW, for your DAN model, you will load the training data from <code>train.txt</code> and the test data from <code>dev.txt</code>. In DAN, instead of counting word occurrences like BOW, you will convert words to their corresponding embeddings, average them, and then pass them to the neural network.</p>\n<p></p>\n<p>If you&#39;re unsure how the DAN model works, you may want to refer to the <a target=\"_new\" href=\"https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf\" rel=\"noopener noreferrer\">DAN paper</a> for more details.</p>\n<p></p>\n<p>Hints:</p>\n<p>In Part 1, there are two phases of mapping:</p>\n<ol><li><b>Converting Words to Word Indices</b>: In this phase, you map each unique word in your vocabulary to a corresponding index.</li><li><b>Converting Word Indices to Word Embeddings</b>: In this phase, you take the word indices and convert them into their respective word embeddings. </li></ol>\n<p></p>\n<p>Also, @46 might be helpful.</p>\n<p></p>\n<p>If you&#39;re still having trouble, feel free to drop by the office hours next week. </p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-12T03:24:34Z","content":"<p>Similar to BOW, for your DAN model, you will load the training data from <code>train.txt</code> and the test data from <code>dev.txt</code>. In DAN, instead of counting word occurrences like BOW, you will convert words to their corresponding embeddings, average them, and then pass them to the neural network.</p>\n<p></p>\n<p>If you&#39;re unsure how the DAN model works, you may want to refer to the <a target=\"_new\" href=\"https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf\" rel=\"noopener noreferrer\">DAN paper</a> for more details.</p>\n<p></p>\n<p>Hints:</p>\n<p>In Part 1, there are two phases of mapping:</p>\n<ol><li><b>Converting Words to Word Indices</b>: In this phase, you map each unique word in your vocabulary to a corresponding index.</li><li><b>Converting Word Indices to Word Embeddings</b>: In this phase, you take the word indices and convert them into their respective word embeddings. </li></ol>\n<p>If you&#39;re still having trouble, feel free to drop by the office hours next week. </p>"}],"type":"i_answer","tag_endorse_arr":["m182ygrlgf83d9","lvdiwwr2smn3tq","kfsi52ar6572xo"],"children":[],"id":"m25lexd1htc33q","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":["m182yor466x3zz","m182ygrlgf83d9","kfsi52ar6572xo"],"no_answer":0,"id":"m2569o5nbs7nf","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990055358,"default_anonymity":"no"},"error":null,"aid":"m3ny5rqqgimc"}