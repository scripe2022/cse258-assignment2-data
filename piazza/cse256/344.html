{"result":{"history_size":1,"folders":["pa2"],"nr":344,"data":{"embed_links":[]},"created":"2024-11-02T01:17:46Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2zh4r347e2l","v":"all","type":"create","when":"2024-11-02T01:17:46Z","uid_a":"a_0"},{"anon":"no","uid":"m182yt8a22e4cr","data":"m2zpnjqyo7y3j5","to":"m2zh4r2xawy2k","type":"s_answer","when":"2024-11-02T05:16:20Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Part 2 perplexity 6000&#43; for training","created":"2024-11-02T01:17:46Z","content":"<p>Hi,</p>\n<p></p>\n<p>I think I have implemented the decoder properly (including masked attention which is passing sanity check) and dropout(p=0.2) in places that Karpathy put. I am still getting perplexity in order of 6000-7000 for training.</p>\n<p></p>\n<p>Currently my transformer is taking in sentences, getting their positional and word embedding encoding, pass it through the decoder stack of 4 sequential decoders (masked attention &#43; ffwd with 100 hidden dims) and then passing it to a final linear layer to map dims from n_embd to vocab_size. This is then getting passed into the F.crossentropy() function just like Karpathy&#39;s tutorial. I&#39;m also doing pre-LN in this final layer as well as in the decoder residual addition layers [x &#43; layernorm(output)] similar to the encoder.</p>\n<p></p>\n<p>I have a few questions:</p>\n<p></p>\n<p>1. Are we only supposed to return the loss since the compute perplexity only needs loss? Karpathy also returns logits but it seems pointless here.</p>\n<p>2. Could there be an issue with computing loss within the forward function that may not be updating params correctly and causing such high perplexity?</p>\n<p>3. We don&#39;t have to do softmax because we are just passing the output of ln_head to the cross entropy function like the Karpathy video right?</p>\n<p>4. Are there any other suggestions or tips to get perplexity down?</p>\n<p>5. Also I think the dataset class is already shifting the targets by 1, so we don&#39;t need to do that ourselves right?<br /><br />My code is almost exactly the same as Karpathy&#39;s for the decoder. The perplexity pretty much seems to be stagnant for all iterations...</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":175,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-02T05:16:20Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Brian Dinh","endorser":{"kfbl4a30mii5kq":1604903364,"global":1606859635,"kfrjlybsasg1pl":1606859635,"khsqnoitk6s3bb":1611077751,"khsqj6nzi1h5sl":1611692722,"ktixnxijtxq13o":1633499067},"admin":false,"photo":null,"id":"kfshpjktxvu169","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"m182yt8a22e4cr","subject":"","created":"2024-11-02T05:16:20Z","content":"<md>1. My understanding of the PA is that we don't need to return the logits, only the CE loss.\n2. Since you're computing the CE loss with the `cross_entropy` function, I feel like it's unlikely that the CE loss is being miscomputed.\n3. I believe so—at least that's what I'm doing and it's been working for me.\n4. Some things that I needed to debug when I had very high perplexities:\n    - Check the structure of your feedforward components: how many layers, where you apply non-linearity, whether or not you use softmax, etc. Fixing them and/or just experimenting with different combinations sometimes made noticeable differences to the loss in my case.\n    - Make sure you're passing *all* of your LM's params to your optimizer. I was stuck with perplexities in the thousands for a long time because I was only passing in my decoder's params (I have separate classes for my decoder and LM, which meant that the feedforward params in my LM weren't being trained).\n5. I think you should be able to use the dataset class out of the box, with no other modifications to shift the targets—at least that's what I'm doing.</md>"}],"type":"s_answer","tag_endorse_arr":["kfsi52ar6572xo","kfshpjktxvu169"],"children":[],"id":"m2zpnjqtzmx3j4","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":0,"id":"m2zh4r2xawy2k","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990349036,"default_anonymity":"no"},"error":null,"aid":"m3nyc2cfqc15vv"}