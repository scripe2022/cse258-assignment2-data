{"result":{"history_size":1,"folders":["pa2"],"nr":373,"data":{"embed_links":[]},"created":"2024-11-04T06:27:29Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m32n2rjln474cj","v":"all","type":"create","when":"2024-11-04T06:27:29Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m33bm3gckft1pt","to":"m32n2rjglae4ci","type":"i_answer","when":"2024-11-04T17:54:22Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Question about data in Part 2","created":"2024-11-04T06:27:29Z","content":"According to the dataset class, y is 1 shift from x. Does that mean the output contains mostly the input and only one new word? If the model is only predicting one word, why not have a linear layer at the end to only predict one word instead of block-size words?"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":104,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-04T17:54:22Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-04T17:54:22Z","content":"<md>Yes, it's next token prediction given the previous context.\n\nI'm not sure if I understand your question correctly, its is for batch training to speed up and use GPU resource efficiently. This is also why we use causal masking to make model not see future tokens.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m33bm3g6hg81ps","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m32n2rjglae4ci","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990375676,"default_anonymity":"no"},"error":null,"aid":"m3nycmwe1x12zl"}