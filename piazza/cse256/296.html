{"result":{"history_size":2,"folders":["pa2"],"nr":296,"data":{"embed_links":[]},"created":"2024-10-29T18:24:16Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"no","uid":"ktyq6r4zhsm3ih","data":"m2us1fh0dtw4sx","v":"all","type":"create","when":"2024-10-29T18:24:16Z"},{"anon":"no","uid":"ktyq6r4zhsm3ih","data":"m2us5ldzlcb56m","v":"all","type":"update","when":"2024-10-29T18:27:30Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2v2zns4xb8107","to":"m2us1fgtork4sw","type":"i_answer","when":"2024-10-29T23:30:49Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"ktyq6r4zhsm3ih","subject":"Decoder output shape for CrossEntropyLoss","created":"2024-10-29T18:27:30Z","content":"<p>The output shape for the decoder is B x T x C (16,32,64). However, ground truth labels are of shape B x T (16, 32). Does this mean we need to add a FFNN on top of the decoder as well in order to do next token prediction?</p>\n<p><br />If this is the case, what should the output size be for the FFNN? Is it the entire vocabulary and then we softmax over it to get the highest probability word?</p>"},{"anon":"no","uid":"ktyq6r4zhsm3ih","subject":"Decoder output shape for CrossEntropyLoss","created":"2024-10-29T18:24:16Z","content":"The output shape for the decoder is B x T x C (16,32,64). However, ground truth labels are of shape B x T (16, 32). Does this mean we need to add a FFNN on top of the decoder as well in order to do next token prediction?"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Chia-Yuan Chang","endorser":{},"admin":false,"photo":null,"id":"lmuvcudy1qiwc","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":124,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-29T23:30:49Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Chia-Yuan Chang","endorser":{},"admin":false,"photo":null,"id":"lmuvcudy1qiwc","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-29T23:30:49Z","content":"<md>Yes, you need one nn.Linear() to get (16, 32, vocab_size) on the top of the decoder.</md>"}],"type":"i_answer","tag_endorse_arr":["lmuvcudy1qiwc"],"children":[],"id":"m2v2znrz8ok106","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["lmuvcudy1qiwc"],"no_answer":0,"id":"m2us1fgtork4sw","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990302136,"default_anonymity":"no"},"error":null,"aid":"m3nyb25n6nc24c"}