{"result":{"history_size":1,"folders":["pa2"],"nr":391,"data":{"embed_links":[]},"created":"2024-11-05T08:33:01Z","bucket_order":3,"no_answer_followup":2,"change_log":[{"anon":"stud","data":"m34701wj91354y","v":"all","type":"create","when":"2024-11-05T08:33:01Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m34pmzlmsmh6ys","to":"m34701wcumz54x","type":"i_answer","when":"2024-11-05T17:14:44Z"},{"anon":"stud","to":"m34701wcumz54x","type":"followup","when":"2024-11-05T22:27:48Z","cid":"m350tl104if5ds","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m34701wcumz54x","type":"feedback","when":"2024-11-05T22:32:57Z","cid":"m35107n9rvm1yp"},{"anon":"stud","to":"m34701wcumz54x","type":"feedback","when":"2024-11-05T22:36:19Z","cid":"m3514ja0abz5hw","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m34701wcumz54x","type":"feedback","when":"2024-11-05T22:52:32Z","cid":"m351peknl0i193"},{"anon":"stud","to":"m34701wcumz54x","type":"feedback","when":"2024-11-05T22:56:33Z","cid":"m351uka5vgf4dm","uid_a":"a_0"},{"anon":"stud","to":"m34701wcumz54x","type":"followup","when":"2024-11-06T10:52:22Z","cid":"m35rf3vh5zd4qm","uid_a":"a_1"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m34701wcumz54x","type":"feedback","when":"2024-11-06T17:28:04Z","cid":"m365jz35l6w4xg"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"High perplexity","created":"2024-11-05T08:33:01Z","content":"<p>Here is my perplexity:</p>\n<p>Epoch #100: train perplexity 1074.193<br />Epoch #200: train perplexity 548.469<br />Epoch #300: train perplexity 413.743<br />Epoch #400: train perplexity 300.042<br />Epoch #500: train perplexity 224.529<br />Obama perplexity 459.242<br />hbush perplexity 509.353<br />wbush perplexity 560.629</p>\n<p>It&#39;s a little over the expected perplexity. My multi-head attention block should be correct because my part 1 achieved 90&#43;% accuracy. I used pre-norm and drop_out=0.05. I also added the normalization layer before the final linear layer. I wonder what could I improve to meet the requirements.</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Matthew Omalley-Nichols","endorser":{},"admin":false,"photo":null,"id":"m182yjcbe9m3ke","photo_url":null,"us":false,"facebook_id":null},{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":138,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-05T17:14:44Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-05T17:14:44Z","content":"<md>I've seen several cases that make the performance a little bit off.\n\n1. Do you use Softmax and CrossEntropy at the same time ? If you use CrossEntropy, don't use Softmax\n\n2. Do you use 2 nn.Layer()s and ReLu for your LM_head ? Try single layer LM_head.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m34pmzlgu5m6yq","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"I found my model performs way better without any normalization in the multi-head component. Is this structure allowed?","created":"2024-11-05T22:27:48Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Yes. there is no normalization **inside** multi-head attention. Normalization should be before or after multi-head attention.</md>","created":"2024-11-05T22:32:57Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m35107n9rvm1yp","d-bucket":"Yesterday","updated":"2024-11-05T22:32:57Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Sorry for my wording and misunderstanding. My normalizations were in the Transformer. If I use no normalization at all (except for the one before LM_head), my encoder accuracy would drop to 80&#43;%, but my decoder would meet the required perplexity. Is it allowed to use no normalization in the transformer?","created":"2024-11-05T22:36:19Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m3514ja0abz5hw","d-bucket":"Yesterday","updated":"2024-11-05T22:36:19Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Have you tried pre-LayerNorm or post-LayerNorm() in @283 - 3 ?\n\nI think you need to include LayerNorm since it is correct implementation based on the 'Attention is all you need' paper.\n\nIts okay if you use pre-LayerNorm or post-LayerNorm whatever it works better for you. But I think you still need to include LayerNorm in the Transformer block.</md>","created":"2024-11-05T22:52:32Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m351peknl0i193","d-bucket":"Yesterday","updated":"2024-11-05T22:52:32Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I see. I found the reason. Pre-LayerNorm works better on the encoder and post-LayerNorm works better on the decoder. Thank you so much for your help!","created":"2024-11-05T22:56:33Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_0","children":[],"tag_good_arr":["kfsi52ar6572xo"],"id":"m351uka5vgf4dm","updated":"2024-11-06T18:15:35Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m350tl104if5ds","d-bucket":"Yesterday","updated":"2024-11-05T22:56:33Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"According to the specfication of part 2.1, we&#39;re supposed to implement a bi-layer feedforward network, right? Testing on my side shows that having a two-layer neural ffw network does cause the model to converge much slower - it consistently fails to reach to specified perplexity levels","created":"2024-11-06T10:52:22Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_1","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>In part 2, lm_head can be single-layer FFN</md>","created":"2024-11-06T17:28:04Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m365jz35l6w4xg","updated":"2024-11-06T17:28:04Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":1,"id":"m35rf3vh5zd4qm","updated":"2024-11-06T17:28:04Z","config":{"editor":"rte"}}],"tag_good_arr":["m182yjcbe9m3ke","kfsi52ar6572xo"],"no_answer":0,"id":"m34701wcumz54x","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990396014,"default_anonymity":"no"},"error":null,"aid":"m3nyd2lb53d3me"}