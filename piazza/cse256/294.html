{"result":{"history_size":1,"folders":["pa2"],"nr":294,"data":{"embed_links":[]},"created":"2024-10-29T17:20:33Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"no","uid":"ktyq6r4zhsm3ih","data":"m2upri4h22pe7","v":"all","type":"create","when":"2024-10-29T17:20:33Z"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m2upw0fwb9s4ds","to":"m2upri4a4die6","type":"i_answer","when":"2024-10-29T17:24:04Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"ktyq6r4zhsm3ih","subject":"Part 1 attention","created":"2024-10-29T17:20:33Z","content":"To clarify, for part 1 we can allow attention to look across the entire sentence but in part 2 it must be the case that we only look at preceding tokens?"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":161,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-29T17:24:04Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Sohail Lokhandwala","endorser":{},"admin":false,"photo":null,"id":"ktyq6r4zhsm3ih","photo_url":null,"us":false,"facebook_id":null},{"role":"student","name":"Chia-Yuan Chang","endorser":{},"admin":false,"photo":null,"id":"lmuvcudy1qiwc","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-29T17:24:04Z","content":"Yeah. This is because we use masked attention mechanism in the decoder part.Â "}],"type":"i_answer","tag_endorse_arr":["ktyq6r4zhsm3ih","lmuvcudy1qiwc"],"children":[],"id":"m2upw0frim04dr","config":{},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2upri4a4die6","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990299608,"default_anonymity":"no"},"error":null,"aid":"m3nyb07g4vx469"}