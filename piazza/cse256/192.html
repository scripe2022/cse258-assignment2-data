{"result":{"history_size":3,"folders":["pa1"],"nr":192,"data":{"embed_links":[]},"created":"2024-10-19T02:25:42Z","bucket_order":3,"no_answer_followup":2,"change_log":[{"anon":"stud","data":"m2fje6zg4hi57e","v":"private","type":"create","when":"2024-10-19T02:25:42Z","uid_a":"a_0"},{"anon":"stud","data":"m2fjjuwfaof7l2","v":"private","type":"update","when":"2024-10-19T02:30:06Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2fp5u3x5g13vc","to":"m2fje6za89m57d","type":"i_answer","when":"2024-10-19T05:07:10Z"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m2fje6za89m57d","type":"followup","when":"2024-10-19T05:07:22Z","cid":"m2fp63e34dk46k"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m2fpjxvoqx66do","type":"i_answer_update","when":"2024-10-19T05:18:08Z"},{"anon":"no","uid":"kfoqmke78c463e","data":"m2fpqhwkjmt5gz","v":"all","type":"update","when":"2024-10-19T05:23:14Z"},{"anon":"no","uid":"kfoqmke78c463e","to":"m2fje6za89m57d","type":"followup","when":"2024-10-19T05:25:36Z","cid":"m2fptjws9nw2bt"},{"anon":"no","uid":"kfoqmke78c463e","to":"m2fje6za89m57d","type":"feedback","when":"2024-10-19T05:30:45Z","cid":"m2fq06f721r488"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2fqco5hw1j60y","type":"i_answer_update","when":"2024-10-19T05:40:28Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"kfoqmke78c463e","subject":"Question: separate training and dev? and what to feed into embedding and skip gram model","created":"2024-10-19T05:23:14Z","content":"<p>Hi, I have some questions about BPE and skip gram model. </p>\n<p>1. Suppose I am using BPE and I do not have initial embedding, do I need to separate training data and dev data  (unlike what we did in Q1 where we can get indexer from glove) because I will use the indexer I get from training data to process the dev data, and possibly I need to handle UNK in the dev data?</p>\n<p></p>\n<p>2. In BPE, what should I feed into the embedding layer. In Q1, we are feeding batch of list of indices (size 16 * 52 matrix if I remember correctly taking padding into consideration) of each word (aka token). Then is that the same case for Q2, if we are feeding the indices of subwords, for each word, are we simply merging the indices or there still need some ordering. </p>\n<p></p>\n<p>If we have [&#39;l ove&#39;, &#39;y ou&#39;] corresponding [2, 3, 4, 5](0, 1 reserved for PAD and UNK), we will feed [[2,3], [4, 5]] or [2, 3, 4, 5]. The former one seems to be the method used in Youtube video, and the latter one seems to be the paper. Which one should I use?</p>\n<p></p>\n<p>3. For the skip gram model, based on the paper, it seems that the denominator also contains that dot product with itself (aka v_the * c_the) because it says &#39;W is the number of words&#39;. This is really counterintuitive, and why we want to have a situation where context IS the center?</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkfoqmke78c463e%2Fe5cc4815948b672981dbb0504b3dda52ea40be9bccecca783e1954769513a5f7%2Ftem.png\" alt=\"tem.png\" /></p>"},{"anon":"stud","uid_a":"a_0","subject":"Question: separate training and dev? and what to feed into embedding and skip gram model","created":"2024-10-19T02:30:06Z","content":"<p>Hi, I have some questions about BPE and skip gram model. </p>\n<p>1. Suppose I am using BPE and I do not have initial embedding, do I need to separate training data and dev data  (unlike what we did in Q1 where we can get indexer from glove) because I will use the indexer I get from training data to process the dev data, and possibly I need to handle UNK in the dev data?</p>\n<p></p>\n<p>2. In BPE, what should I feed into the embedding layer. In Q1, we are feeding batch of list of indices (size 16 * 52 matrix if I remember correctly taking padding into consideration) of each word (aka token). Then is that the same case for Q2, if we are feeding the indices of subwords, for each word, are we simply merging the indices or there still need some ordering. </p>\n<p></p>\n<p>If we have [&#39;l ove&#39;, &#39;y ou&#39;] corresponding [2, 3, 4, 5](0, 1 reserved for PAD and UNK), we will feed [[2,3], [4, 5]] or [2, 3, 4, 5]. The former one seems to be the method used in Youtube video, and the latter one seems to be the paper. Which one should I use?</p>\n<p></p>\n<p>3. For the skip gram model, based on the paper, it seems that the denominator also contains that dot product with itself (aka v_the * c_the) because it says &#39;W is the number of words&#39;. This is really counterintuitive, and why we want to have a situation where context IS the center?</p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkfoqmke78c463e%2Fe5cc4815948b672981dbb0504b3dda52ea40be9bccecca783e1954769513a5f7%2Ftem.png\" alt=\"tem.png\" /></p>"},{"anon":"stud","uid_a":"a_0","subject":"Question: separate training and dev? and what to feed into embedding","created":"2024-10-19T02:25:42Z","content":"<p>Hi, I have some questions about BPE.</p>\n<p>1. Suppose I am using BPE and I do not have initial embedding, do I need to separate training data and dev data  (unlike what we did in Q1 where we can get indexer from glove) because I will use the indexer I get from training data to process the dev data, and possibly I need to handle UNK in the dev data?</p>\n<p></p>\n<p>2. In BPE, what should I feed into the embedding layer. In Q1, we are feeding batch of list of indices (size 16 * 52 matrix if I remember correctly taking padding into consideration) of each word (aka token). Then is that the same case for Q2, if we are feeding the indices of subwords, for each word, are we simply merging the indices or there still need some ordering. </p>\n<p></p>\n<p>If we have [&#39;l ove&#39;, &#39;y ou&#39;] corresponding [2, 3, 4, 5](0, 1 reserved for PAD and UNK), we will feed [[2,3], [4, 5]] or [2, 3, 4, 5]. The former one seems to be the method used in Youtube video, and the latter one seems to be the paper. Which one should I use?</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":142,"children":[{"history_size":3,"folders":[],"data":{"embed_links":[]},"created":"2024-10-19T05:07:10Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T05:40:28Z","content":"<p>Great questions!</p>\n<p></p>\n<p>1. Yes, use the indexer from the training data to process the dev data, and handle <kbd>&lt;UNK&gt;</kbd> in the dev data.</p>\n<p></p>\n<p>2. You can feed subword indices similar to word indices. I used the latter format [2, 3, 4, 5], but either way is fine as long as the DAN model runs properly.</p>\n<p></p>\n<p>3. Repeating words can happen, like in &#34;NLP is very very interesting,&#34; where &#34;very&#34; could be both the center and context word.<br /><br />*Edited:</p>\n<p></p>\n<p>Regarding Part 2, according to the spec: &#34;Implement the Byte Pair Encoding (BPE) algorithm as discussed in class, and in Sennrich et al. [3] and train BPE on ‘train.txt’. Use the sub-word vocabulary generated by BPE in your DAN, instead of the word-level vocabulary.&#34;</p>\n<p><br />So please train BPE on ‘train.txt’ (also refer to question @49). </p>"},{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-19T05:18:08Z","content":"<p>Great questions!</p>\n<p></p>\n<p>1. Yes, use the indexer from the training data to process the dev data, and handle <kbd>&lt;UNK&gt;</kbd> in the dev data.</p>\n<p></p>\n<p>2. You can feed subword indices similar to word indices. I used the latter format [2, 3, 4, 5], but either way is fine as long as the DAN model runs properly.</p>\n<p></p>\n<p>3. Repeating words can happen, like in &#34;NLP is very very interesting,&#34; where &#34;very&#34; could be both the center and context word.</p>"},{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-19T05:07:10Z","content":"<p>Great questions!</p>\n<p></p>\n<p>1. Yes, use the indexer from the training data to process the dev data, and handle <kbd>&lt;UNK&gt;</kbd> in the dev data.</p>\n<p></p>\n<p>2. You can feed subword indices similar to word indices. I used the latter format [2, 3, 4, 5], but either way is fine as long as the DAN model runs properly.</p>\n<p></p>\n<p>3. Repeating words can happen, like in &#34;NLP is very very interesting,&#34; where &#34;very&#34; could be both the center and context word.</p>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2fp5u3towv3vb","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>Also, would you mind making this post public instead of private (you can choose to remain anonymous to your classmates)? Others may have similar questions, and it could be helpful for them as well. </p>\n<p>Please only send us private messages when personal information is shared.</p>\n<p>Thanks!</p>","created":"2024-10-19T05:07:22Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"no_answer":1,"id":"m2fp63e34dk46k","d-bucket":"Yesterday","updated":"2024-10-19T05:07:22Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"That is to say guys. REMEMBER TO SPLIT YOUR TRAINING AND DEV INTO TWO METHOD IN Q2. IT IS NOT THE SAME AS HOW YOU PROCESS THESE TWO IN ONE METHOD LIKE YOU DO IN Q1 BECAUSE YOU NO LONGER HAVE HANDS ON INDEXER. ","created":"2024-10-19T05:25:36Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid":"kfoqmke78c463e","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Or you really do not have to create two classes. Just remember the logic to process training and dev will be very different for Q2","created":"2024-10-19T05:30:45Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"kfoqmke78c463e","children":[],"tag_good_arr":[],"id":"m2fq06f721r488","d-bucket":"Yesterday","updated":"2024-10-19T05:30:45Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m2fptjws9nw2bt","d-bucket":"Yesterday","updated":"2024-10-19T05:30:45Z","config":{"editor":"rte"}}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":0,"id":"m2fje6za89m57d","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990205926,"default_anonymity":"no"},"error":null,"aid":"m3ny8zx5jur5kt"}