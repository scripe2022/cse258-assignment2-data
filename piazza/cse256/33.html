{"result":{"history_size":2,"folders":["pa1"],"nr":33,"data":{"embed_links":[]},"created":"2024-10-09T22:37:29Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m22ga1gkag92me","v":"private","type":"create","when":"2024-10-09T22:37:29Z","uid_a":"a_0"},{"anon":"no","uid":"m182yl5tu473ow","data":"m22gafm56jj35y","v":"all","type":"update","when":"2024-10-09T22:37:47Z"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m22ixb0xya33u0","to":"m22ga1gcgwd2md","type":"i_answer","when":"2024-10-09T23:51:34Z"}],"bucket_name":"Today","history":[{"anon":"no","uid":"m182yl5tu473ow","subject":"BOW Model and activation function each layer","created":"2024-10-09T22:37:47Z","content":"In BOW model given, there is only 1 activation function Relu for the first layer and the second layer is just a simple linear layer. In comparison, the DAN model described in the lecture notes has an activation function for each of the two layers. In general, I wonder how do we determine when to use activation functions when designing neural network?"},{"anon":"stud","uid_a":"a_0","subject":"BOW Model and activation function each layer","created":"2024-10-09T22:37:29Z","content":"In BOW model given, there is only 1 activation function Relu for the first layer and the second layer is just a simple linear layer. In comparison, the DAN model described in the lecture notes has an activation function for each of the two layers. In general, I wonder how do we determine when to use activation functions when designing neural network?"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":163,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-09T23:51:34Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-09T23:51:34Z","content":"In general, activation functions introduce non-linearity to neural networks, helping them learn more complex patterns. However, when and where to use them is case-dependent and often part of hyperparameter tuning. For PA1, checking the hyperparameter settings from the <a href=\"https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">DAN paper</a>Â might be a helpful starting point."}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m22ixb0qtei3ty","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m22ga1gcgwd2md","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990032635,"default_anonymity":"no"},"error":null,"aid":"m3ny5a7i46r7ht"}