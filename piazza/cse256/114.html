{"result":{"history_size":1,"folders":["pa1"],"nr":114,"data":{"embed_links":[]},"created":"2024-10-15T18:11:21Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m2arewchm9a6wp","v":"all","type":"create","when":"2024-10-15T18:11:21Z","uid_a":"a_0"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m2as1l6tpza5fn","to":"m2arewcapnt6wo","type":"i_answer","when":"2024-10-15T18:28:59Z"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m2as2lq0glv2ah","type":"i_answer_update","when":"2024-10-15T18:29:47Z"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m2arewcapnt6wo","type":"followup","when":"2024-10-15T18:33:09Z","cid":"m2as6xr3mkq489"},{"anon":"stud","to":"m2arewcapnt6wo","type":"feedback","when":"2024-10-17T22:20:25Z","cid":"m2dv6wnzfdm3o5","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m2arewcapnt6wo","type":"feedback","when":"2024-10-18T00:27:22Z","cid":"m2dzq5pdp5s1hq"},{"anon":"stud","to":"m2arewcapnt6wo","type":"feedback","when":"2024-10-18T16:47:49Z","cid":"m2eyr1fb1x1gk","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m2arewcapnt6wo","type":"feedback","when":"2024-10-19T00:01:57Z","cid":"m2fe9bnkauj5v2"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"PA1 3d","created":"2024-10-15T18:11:21Z","content":"<p>Hello, I am struggling a little on how to approach 3d. I think we have to maximize the dot product for pairs that occur together and minimize the dot product for pairs that don&#39;t. But I don&#39;t know how to arrive at concrete values.</p>\n<p>Any tips would be appreciated.<br />Thanks!</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":172,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-15T18:28:59Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-15T18:29:47Z","content":"In the question 3d, you are asked to provide a set of word and context vectors that nearly optimize the skip-gram objective. This means that you will need to find vector embeddings for both words and their contexts in such a way so that it assigns probabilities closer to the empirical distribution probability. It is somewhat similar to 3b. "},{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-15T18:28:59Z","content":"In the question 3d, you are asked to provide a set of word and context vectors that nearly optimize the skip-gram objective. This means that you will need to find vector embeddings for both words and their contexts in such a way so that it assigns probabilities close to the empirical distribution probability. It is somewhat similar to 3b. "}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2as1l6m1ou5fm","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"Perhaps try different settings for the context and word vectors for each word. Then follow similar steps as in 3b to check if those settings are optimal.<div><br /></div><div>Also, keep in mind that you only need to provide one set of both context and word vectors for each word that nearly optimizes the skip-gram objective.</div>","created":"2024-10-15T18:33:09Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid":"ln0md59uz9w3kd","children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Should we use the same context vectors that are given in part 1? so (0,1) for cat and dog and (1,0) for a and the? or do we assume both the context AND word vectors are NOT GIVEN?","created":"2024-10-17T22:20:25Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m2dv6wnzfdm3o5","updated":"2024-10-17T22:20:25Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<p>Not given.</p>\n<p></p>\n<p>In Part 3, Q1 (3a and 3b) and Q2 (3c and 3d) are separated.</p>","created":"2024-10-18T00:27:22Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m2dzq5pdp5s1hq","d-bucket":"Yesterday","updated":"2024-10-18T00:27:36Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Gotcha. And since we need to provide a sample word and context vector for each word to have the skip-gram be within 0.01 of the empirical distribution, can we just use the same context vectors from part 3a and then calculate the word vectors? or do the context vectors need to be completely new?","created":"2024-10-18T16:47:49Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m2eyr1fb1x1gk","d-bucket":"Yesterday","updated":"2024-10-18T16:47:49Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"You may use the same vectors from part 3a.","created":"2024-10-19T00:01:57Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"student","name":"Raunaq Sharma","endorser":{},"admin":false,"photo":null,"id":"m182ylcc6u33pc","photo_url":null,"us":false,"facebook_id":null}],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":["m182ylcc6u33pc"],"id":"m2fe9bnkauj5v2","updated":"2024-10-19T01:35:10Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m2as6xr3mkq489","updated":"2024-10-19T00:01:57Z","config":{}}],"tag_good_arr":[],"no_answer":0,"id":"m2arewcapnt6wo","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":4,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990118567,"default_anonymity":"no"},"error":null,"aid":"m3ny74ijkh95m0"}