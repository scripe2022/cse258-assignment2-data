{"result":{"history_size":3,"folders":["pa1"],"nr":146,"data":{"embed_links":[]},"created":"2024-10-17T01:33:08Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2cmmw0cs7l612","v":"all","type":"create","when":"2024-10-17T01:33:08Z","uid_a":"a_0"},{"anon":"stud","data":"m2cn6yd170y7g2","v":"all","type":"update","when":"2024-10-17T01:48:44Z","uid_a":"a_0"},{"anon":"stud","data":"m2cnrmmxl3q3ss","v":"all","type":"update","when":"2024-10-17T02:04:49Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2cqp8poac7m3","to":"m2cmmw04sdx611","type":"i_answer","when":"2024-10-17T03:26:56Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"BPE","created":"2024-10-17T02:04:49Z","content":"I understand that for BPE we need to use some text and use that as the training. I am using the training.txt which is all one big piece of text now. I have then gone through and encoded the characters and have been able to find the common pair. I next was able to merge and replace the common pair with a new token. However, I am a bit lost as to how this relates back to DAN. Do we create our own indexer and all of the tokens and new tokens to it? Do we take the encodings and embed them? Also in part 1 we were returning the label for __get_item__ I am a bit lost as to what we would do for get_item now? Any insight would be helpful!"},{"anon":"stud","uid_a":"a_0","subject":"BPE","created":"2024-10-17T01:48:44Z","content":"I understand that for BPE we need to use some text and use that as the training. I am using the training.txt which is all one big piece of text now. I have then gone through and encoded the characters and have been able to find the common pair. I next was able to merge and replace the common pair with a new token. However, I am a bit lost as to how this relates back to DAN. Do we create our own indexer and all of the tokens and new tokens to it? Do we take the encodings and embed them? Any insight would be helpful!"},{"anon":"stud","uid_a":"a_0","subject":"BPE","created":"2024-10-17T01:33:08Z","content":"I understand that for BPE we need to use some text and use that as the training. I am using the training.txt which is all one big piece of text now. I have then gone through and encoded the characters and have been able to find the common pair. I next was able to merge and replace the common pair with a new token. However, I am a bit lost as to how this relates back to DAN. Do we take the encodings and embed them? Any insight would be helpful! "}],"type":"question","tags":["pa1","student"],"tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":138,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-17T03:26:56Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-17T03:26:56Z","content":"<p>That sounds reasonable! There are two main parts to this:</p>\n<p></p>\n<ol><li>\n<p><strong>Training BPE to Get Subword Vocabulary</strong>: You’ve already done this by identifying common character pairs and merging them to form new tokens. This step is all about building a subword-level vocabulary from your training data.</p>\n</li><li>\n<p><strong>Integrating BPE into DAN</strong>: In Part 1, sentences were split by spaces (word-level). Now, after training the BPE, you need to use the BPE tokenizer to split sentences into subword units. Modify the parts of your code that originally split by spaces to use your BPE tokenizer instead.</p>\n</li></ol>\n<p>You&#39;ll still have two mappings similar to Part 1:</p>\n<p></p>\n<ul><li><strong>Map subwords to indices</strong>: Just like how you mapped words to indices before, now you&#39;ll map subwords to indices.</li><li><strong>Map indices to embeddings</strong>: Once you have subword indices, you map them to their (randomly initialized) embeddings.</li></ul>"}],"type":"i_answer","tag_endorse_arr":["kfsi52ar6572xo"],"children":[],"id":"m2cqp8pjy5mm1","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":0,"id":"m2cmmw04sdx611","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990156529,"default_anonymity":"no"},"error":null,"aid":"m3ny7xszv5x4fi"}