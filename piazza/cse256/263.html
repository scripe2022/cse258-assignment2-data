{"result":{"history_size":2,"folders":["pa2"],"nr":263,"data":{"embed_links":[]},"created":"2024-10-26T18:43:32Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2qieo73de56vt","v":"all","type":"create","when":"2024-10-26T18:43:32Z","uid_a":"a_0"},{"anon":"stud","data":"m2qio15fkfg4d5","v":"all","type":"update","when":"2024-10-26T18:50:49Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2qoxrveh485u1","to":"m2qieo6xi236vs","type":"i_answer","when":"2024-10-26T21:46:21Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Stack of N transformer layers","created":"2024-10-26T18:50:49Z","content":"<p>I am a bit confused here about how the stack of transformer layers works. </p>\n<p></p>\n<p>Here is what I have understood:<br />1. We have N identical transformer layers where each layer has two sub layers as described in the paper.</p>\n<p>2. We take the input (sum of token embeddings and positional emcodings) and pass it to each of these N transformer layers so that each layer can something different from the same input.<br />3. And then we stack the outputs of the identical layers. so now each output of dimension 16 x 32 x 64 will be stacked to create a new tensor of dimension 4 x 16 x 32 x 64. <br />4. We will average these outputs to get a 16 x 32 x 64 embedding output.</p>\n<p>5. We will pass this output to the feedword classifer.<br /><br />Is this correct??</p>"},{"anon":"stud","uid_a":"a_0","subject":"Stack of N transformer layers","created":"2024-10-26T18:43:32Z","content":"<p>I am a bit confused here about how the stack of transformer layers works. </p>\n<p></p>\n<p>Here is what I have understood:<br />1. We have N identical transformer layers where each layer has two sub layers as described in the paper.</p>\n<p>2. We take the input (sum of token embeddings and positional emcodings) and pass it to each of these N transformer layers so that each layer can something different from the same input.<br />3. And then we stack the outputs of the identical layers. so now each output of dimension 16 x 32 x 64 will be stacked to create a new tensor of dimension 16 x 4 x 32 x 64. <br />4. We will average these outputs to get a 16 x 32 x 64 embedding output.</p>\n<p>5. We will pass this output to the feedword classifer.<br /><br />Is this correct??</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Sean Hyatt Perry","endorser":{},"admin":false,"photo":null,"id":"kfo6qagh36n51j","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":105,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-26T21:46:21Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-26T21:46:21Z","content":"<md>At 2, you pass the sum of token embeddings and positional encodings to the first transformer layer and the output of the 1st layer is the input of 2nd layer. The output of 2nd layer is the input of 3rd layer and so on..\n\nalso see @245</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2qoxrv8wlb5u0","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["kfo6qagh36n51j"],"no_answer":0,"id":"m2qieo6xi236vs","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990273055,"default_anonymity":"no"},"error":null,"aid":"m3nyafpvw810y"}