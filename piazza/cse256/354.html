{"result":{"history_size":1,"folders":["pa2"],"nr":354,"data":{"embed_links":[]},"created":"2024-11-02T23:36:16Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m30sy2p5935180","v":"all","type":"create","when":"2024-11-02T23:36:16Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m30uvw9ggvm56m","to":"m30sy2oxox917z","type":"i_answer","when":"2024-11-03T00:30:33Z"},{"anon":"no","uid":"m182yt8a22e4cr","to":"m30sy2oxox917z","type":"followup","when":"2024-11-03T01:09:20Z","cid":"m30w9rhxejj23"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m30sy2oxox917z","type":"feedback","when":"2024-11-03T23:33:27Z","cid":"m328abbom7dic"},{"anon":"stud","to":"m30sy2oxox917z","type":"feedback","when":"2024-11-04T04:50:34Z","cid":"m32jm4a9yzj38m","uid_a":"a_1"},{"anon":"stud","uid_a":"a_2","to":"m30sy2oxox917z","type":"feedback","when":"2024-11-06T02:12:36Z","cid":"m358uopvf9874j"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"sanity check for attention map filled with random values","created":"2024-11-02T23:36:16Z","content":"<p>I know there are lots of discussion about the sanity check, but I still don&#39;t understand where we should do the sanity check.</p>\n<p></p>\n<p>In <a href=\"/class/m182y0mqvor2k7/post/282\">@282</a>, the instructor says,</p>\n<p>&#34;Don’t do sanity check in training.</p>\n<p>Sanity check is just for validating your implementation is correct or not. Just try once or twice before training to see your code passes or not.&#34;</p>\n<p></p>\n<p>Does this mean we should do the sanity check <strong>before training the model, but after creating the model instance</strong>? If so, why should we do it at that stage? Wouldn&#39;t the values in the attention map be random values? so there aren&#39;t any special interpretation we can explain for the report.</p>\n<p></p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":191,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-03T00:30:33Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-03T00:30:33Z","content":"<md>1. we should do the sanity check **before training the model**\n\n-> to check your implementation is valid or not. If you didn't pass the sanity check, which means your implementation is not correct, and there is no reason to train your model with wrong implementation.\n\n2. Don’t do sanity check in training.\n\n-> You don't need to do sanity check every time during training model. It is very unnecessary once you checked your implementation is correct.\n\n3. After you are done with training and your accuracy/perplexity look great, run sanity check **for analysis**.  You're right, the initial attention maps are random since the model is not trained at all. After training the model, you may discover some interesting patterns that show what attention is actually doing.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m30uvw99w7y56l","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<md>The PA instructions say to \"Discuss what you observe in these plots.\" Since the initial attention maps are random and have no observable patterns, is it OK if we only include post-training attention maps in our write-ups?</md>","created":"2024-11-03T01:09:20Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[{"role":"student","name":"Erica Cheng","endorser":{},"admin":false,"photo":null,"id":"m182ygrlgf83d9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Sarvani Kunapareddy","endorser":{},"admin":false,"photo":"a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","id":"m182yor466x3zz","photo_url":"https://cdn-uploads.piazza.com/photos/m182yor466x3zz/a108afd0-df22-4c82-a48b-4615ee8b0e26_200.jpg","published":true,"us":false,"facebook_id":null}],"uid":"m182yt8a22e4cr","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>For discussion, you need to analyze post-training attention maps. These is observable patterns with pre-training attention maps.</md>","created":"2024-11-03T23:33:27Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m328abbom7dic","updated":"2024-11-03T23:33:27Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"<p>If the untrained attention heads are giving attention to padded tokens, is that fine? I am only doing sanity check on the untrained attention heads and I figured the model hasn&#39;t learned to ignore the padding.</p>\n<p></p>\n<p>Additionally, is it fine if the encoder/decoder doesn&#39;t return the attention maps/matrices? The way I have my code set up currently I can&#39;t find a good way to pass the attention matrix from an attention head to the multi head class to the encoder block to the encoder layers and then onto the final transformer model, so I am just creating a separate sanity testing class that does the embedding and creates the attention matrix in one separate class, and that is being passed as a model to the sanity check function. Is this acceptable?</p>\n<p>I will include the attention maps in the report.</p>\n<p></p>\n<p>Looking at the instructions given in the assignment, I assumed that the sanity check was just to check the calculation implementation and not for analysis. Also, I went off of the comments in @282 and already made my entire setup without passing the attention matrices...</p>\n<p></p>\n<div>\n<div>\n<pre>\nPart 1.4: Sanity Checks\nSanity check your attention implementation using the helper function provided in utilities.py. This function will verify that the rows of the attention matrix sum to 1 and will also visualize the attention matrix. Include plots of attention matrices for one or two sentences in your report (you can choose any layer(s) and any head(s)). Discuss what you observe in these plots.</pre>\n</div>\n</div>","created":"2024-11-04T04:50:34Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m32jm4a9yzj38m","updated":"2024-11-04T04:50:34Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Just to clarify, there is no need to include the pre-training attention map in the report, we just need to run sanity check before training the model in the code itself?<br /><br /><br />","created":"2024-11-06T02:12:36Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[],"uid_a":"a_2","children":[],"tag_good_arr":[],"id":"m358uopvf9874j","updated":"2024-11-06T02:12:36Z","config":{"editor":"rte"}}],"tag_good_arr":["m182ygrlgf83d9","m182yor466x3zz"],"no_answer":1,"id":"m30w9rhxejj23","updated":"2024-11-06T02:12:36Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":0,"id":"m30sy2oxox917z","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":5,"num_favorites":4,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990357875,"default_anonymity":"no"},"error":null,"aid":"m3nyc961mfcsx"}