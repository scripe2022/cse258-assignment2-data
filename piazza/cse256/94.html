{"result":{"history_size":1,"folders":["pa1"],"nr":94,"data":{"embed_links":[]},"created":"2024-10-14T20:07:20Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m29g47gbn6q1sl","v":"all","type":"create","when":"2024-10-14T20:07:20Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m29ghudpqhh5t6","to":"m29g47g4gto1sk","type":"i_answer","when":"2024-10-14T20:17:56Z"},{"anon":"stud","to":"m29g47g4gto1sk","type":"followup","when":"2024-10-14T20:28:03Z","cid":"m29guugcgy01k7","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m29g47g4gto1sk","type":"feedback","when":"2024-10-14T20:29:41Z","cid":"m29gwyddwrj4en"},{"anon":"stud","to":"m29g47g4gto1sk","type":"feedback","when":"2024-10-14T20:30:30Z","cid":"m29gy06m6nc5ys","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m29g47g4gto1sk","type":"feedback","when":"2024-10-14T20:31:04Z","cid":"m29gyqg8jzj6z3"},{"anon":"stud","to":"m29g47g4gto1sk","type":"feedback","when":"2024-10-14T20:32:16Z","cid":"m29h0a2n9no1h8","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m29g47g4gto1sk","type":"feedback","when":"2024-10-14T20:34:00Z","cid":"m29h2i3qs5h2xi"},{"anon":"stud","to":"m29g47g4gto1sk","type":"feedback","when":"2024-10-15T20:54:13Z","cid":"m2ax8csitq747j","uid_a":"a_1"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m29g47g4gto1sk","type":"feedback","when":"2024-10-16T00:46:50Z","cid":"m2b5ji1cm4w62j"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Randomly initialize embeddings in part 2","created":"2024-10-14T20:07:20Z","content":"What does the writeup mean when it says randomly initialize embeddings for part 2? We have subword indices that we calculated in the __init__ method. I don&#39;t understand how we will use the torch.nn.embeddings for part 2. Can someone pls explain how this works?"}],"type":"question","tags":["pa1","student"],"tag_good":[{"role":"student","name":"Rahul Sharma Nemmani","endorser":{},"admin":false,"photo":null,"id":"jml95ci0otv4w9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":132,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-14T20:17:56Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-14T20:17:56Z","content":"<p>To understand how <code>torch.nn.Embedding</code> works, I recommend checking the PyTorch documentation <a target=\"_new\" href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\" rel=\"noopener noreferrer\">here</a>. It allows you to create a random embedding matrix that maps each index to a vector, which will be updated during training.</p>\n<p></p>\n<p>Let us know if you have further questions!</p>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m29ghudjhfd5t3","config":{"editor":"rte"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"So we map each sub word index to a vector and we get that vector to train in the fc layers that we created right?","created":"2024-10-14T20:28:03Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Sounds correct :)","created":"2024-10-14T20:29:41Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m29gwyddwrj4en","updated":"2024-10-14T20:29:41Z","config":{}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Okay, and those vectors are the vectors we are averaging in the case of the DAN model?","created":"2024-10-14T20:30:30Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m29gy06m6nc5ys","updated":"2024-10-14T20:30:30Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Yes.","created":"2024-10-14T20:31:04Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":[],"id":"m29gyqg8jzj6z3","updated":"2024-10-14T20:31:04Z","config":{}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Is the size of the vector technically a hyperparameter that we will have to tune?","created":"2024-10-14T20:32:16Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m29h0a2n9no1h8","updated":"2024-10-14T20:32:16Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"Yes, but since we are using 50d or 300d GloVe for 1a, it might be better to choose the same dimension for a better comparison.","created":"2024-10-14T20:34:00Z","bucket_order":5,"bucket_name":"This week","type":"feedback","tag_good":[{"role":"student","name":"Rahul Sharma Nemmani","endorser":{},"admin":false,"photo":null,"id":"jml95ci0otv4w9","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":["jml95ci0otv4w9"],"id":"m29h2i3qs5h2xi","updated":"2024-10-14T20:39:41Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Then how can I transfer the word to it&#39;s index without using the pretrained Glove embedding? Should I construct an indexer based on all training data by myself?","created":"2024-10-15T20:54:13Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m2ax8csitq747j","d-bucket":"Yesterday","updated":"2024-10-15T20:54:13Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"You may still use the GloVe file for the vocabulary, just be sure to randomly initialize the word embedding vectors instead of using the vectors provided in the GloVe file.Â ","created":"2024-10-16T00:46:50Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"student","name":"Brian Dinh","endorser":{"kfbl4a30mii5kq":1604903364,"global":1606859635,"kfrjlybsasg1pl":1606859635,"khsqnoitk6s3bb":1611077751,"khsqj6nzi1h5sl":1611692722,"ktixnxijtxq13o":1633499067},"admin":false,"photo":null,"id":"kfshpjktxvu169","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid":"ln0md59uz9w3kd","children":[],"tag_good_arr":["kfshpjktxvu169"],"id":"m2b5ji1cm4w62j","updated":"2024-10-16T01:07:39Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m29guugcgy01k7","updated":"2024-10-16T00:46:50Z","config":{"editor":"rte"}}],"tag_good_arr":["jml95ci0otv4w9"],"no_answer":0,"id":"m29g47g4gto1sk","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990097058,"default_anonymity":"no"},"error":null,"aid":"m3ny6nx0si14zi"}