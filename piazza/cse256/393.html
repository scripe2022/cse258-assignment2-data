{"result":{"history_size":1,"folders":["pa2"],"nr":393,"data":{"embed_links":[]},"created":"2024-11-05T08:44:35Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m347ex4kpsz7p6","v":"all","type":"create","when":"2024-11-05T08:44:35Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m34pt78eth2n","to":"m347ex4d2iu7p5","type":"i_answer","when":"2024-11-05T17:19:34Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Question about PAD token","created":"2024-11-05T08:44:35Z","content":"<p>I have two questions about handling the &lt;PAD&gt; token in the part1 encoder.</p>\n<p></p>\n<p>1. First, for sentences with a length of less than 32, the tokenizer pads them with &lt;PAD&gt; tokens to reach the full length. In my encoder implementation, I didn&#39;t apply masking to the &lt;PAD&gt; token, so it participates in the attention calculation. And the model doesn’t learn to ignore the &lt;PAD&gt; tokens, leading to non-zero attention scores for them, even though it passes the sanity check. Is it acceptable to display a post-training attention map where the &lt;PAD&gt; tokens have non-zero attention scores in the report?</p>\n<p></p>\n<p>2. Second, for the classifier input, we use the mean of the encoder’s output. Is it okay to average the entire [batch_size, 32, 64] tensor along dim=1 to get the mean vector no matter it is pad or not, or should I apply a mask to exclude the outputs corresponding to &lt;PAD&gt; positions?&#34;</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"unique_views":172,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-11-05T17:19:34Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-05T17:19:34Z","content":"<md>1. Yes it is possible. Because not all attention heads learn something useful or interpretable. This paper shows very interesting observations of the role of attention heads. Actually there are a lot of no-op heads, which means the heads actually do nothing (or randomly assign weights, assign weights which can not interpreted etc…)\n\nhttps://aclanthology.org/W19-4828.pdfIt is possible \n\n2. Is it okay to average the entire \\[batch\\_size, 32, 64\\] tensor along dim=1 to get the mean vector no matter it is pad or not -> Yes.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m34pt785o2m","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["kfsi52ar6572xo"],"no_answer":0,"id":"m347ex4d2iu7p5","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":2,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990398538,"default_anonymity":"no"},"error":null,"aid":"m3nyd4jkw2l41h"}