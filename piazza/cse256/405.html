{"result":{"history_size":4,"folders":["pa2"],"nr":405,"data":{"embed_links":[]},"created":"2024-11-06T02:05:21Z","bucket_order":3,"no_answer_followup":1,"change_log":[{"anon":"stud","data":"m358ld2t3sj1n3","v":"all","type":"create","when":"2024-11-06T02:05:21Z","uid_a":"a_0"},{"anon":"stud","data":"m358nekls074ks","v":"all","type":"update","when":"2024-11-06T02:06:56Z","uid_a":"a_0"},{"anon":"stud","data":"m358ph1szz7l5","v":"all","type":"update","when":"2024-11-06T02:08:33Z","uid_a":"a_0"},{"anon":"stud","data":"m358pumzk20ho","v":"all","type":"update","when":"2024-11-06T02:08:51Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m35d1afl59z2fi","to":"m358ld2mupp1n2","type":"i_answer","when":"2024-11-06T04:09:43Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m35d1i0330i2ru","type":"i_answer_update","when":"2024-11-06T04:09:52Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m35d2sx1n1i54p","type":"i_answer_update","when":"2024-11-06T04:10:53Z"},{"anon":"stud","to":"m358ld2mupp1n2","type":"followup","when":"2024-11-06T04:22:35Z","cid":"m35dhu2a3cq7pi","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:25:11Z","cid":"m35dl6xxob613m"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:26:16Z","cid":"m35dml1bi8t4aw","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:28:26Z","cid":"m35dpdb3v2f16n"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:32:02Z","cid":"m35du05qb9v432","uid_a":"a_0"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:34:21Z","cid":"m35dwz2p59u705","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:35:03Z","cid":"m35dxvoppny1y4"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:35:28Z","cid":"m35dyeu0hnc31f"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:36:59Z","cid":"m35e0cww8g82em","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:40:16Z","cid":"m35e4ksirky645"},{"anon":"no","uid":"ku2mpmjms7n645","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:40:58Z","cid":"m35e5hbk7v9e"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:44:25Z","cid":"m35e9x2wni54b2","uid_a":"a_0"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T04:54:38Z","cid":"m35en2e7mlg14t","uid_a":"a_0"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T05:39:33Z","cid":"m35g8tckpqx5wj","uid_a":"a_0"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T06:41:19Z","cid":"m35ig8ycutp7ky","uid_a":"a_1"},{"anon":"stud","to":"m358ld2mupp1n2","type":"feedback","when":"2024-11-06T07:01:16Z","cid":"m35j5x5m4aa593","uid_a":"a_0"},{"anon":"stud","data":"m36cxc0yz4s3nc","to":"m358ld2mupp1n2","type":"s_answer","when":"2024-11-06T20:54:24Z","uid_a":"a_0"},{"anon":"stud","uid_a":"a_0","data":"m36cy59ggjv4ng","type":"s_answer_update","when":"2024-11-06T20:55:02Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"High train perplexity","created":"2024-11-06T02:08:51Z","content":"My implementation for the decoder model achieves within the threshold for the test datasets but gets 200-300 train perplexity. Should I change my architecture to achieve lower train perplexity or can I get full marks reporting on why the train perplexity is high?"},{"anon":"stud","uid_a":"a_0","subject":"High train perplexity","created":"2024-11-06T02:08:33Z","content":"My implementation for the decoder model achieves within the threshold for the test datasets but gets 200-280 train perplexity. Should I change my architecture to achieve lower train perplexity or can I get full marks reporting on why the train perplexity is high?"},{"anon":"stud","uid_a":"a_0","subject":"High train perplexity","created":"2024-11-06T02:06:56Z","content":"My implementation for the decoder model achieves within the threshold for the test datasets but gets 200-250 train perplexity. Should I change my architecture to achieve lower train perplexity or can I get full marks reporting on why the train perplexity is high?"},{"anon":"stud","uid_a":"a_0","subject":"High train perplexity","created":"2024-11-06T02:05:21Z","content":"My implementation for the decoder model achieves within the threshold for the test datasets but gets high 200 to low 300 training perplexity. Should I change my architecture to achieve lower train perplexity or can I just report on why the train perplexity is higher than the threshold?"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Matthew Omalley-Nichols","endorser":{},"admin":false,"photo":null,"id":"m182yjcbe9m3ke","photo_url":null,"us":false,"facebook_id":null}],"unique_views":160,"children":[{"history_size":3,"folders":[],"data":{"embed_links":[]},"created":"2024-11-06T04:09:43Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-06T04:10:53Z","content":"<md>That could be not a full-credit. \n\nI’ve seen several cases that make the performance a little bit off.\n\n1.  Do you use Softmax and CrossEntropy at the same time ? If you use CrossEntropy, don’t use Softmax\n    \n2.  Do you use 2 nn.Layer()s and ReLu for your LM\\_head ? Try single layer LM\\_head.\n\nsee @391</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-06T04:09:52Z","content":"<md>I’ve seen several cases that make the performance a little bit off.\n\n1.  Do you use Softmax and CrossEntropy at the same time ? If you use CrossEntropy, don’t use Softmax\n    \n2.  Do you use 2 nn.Layer()s and ReLu for your LM\\_head ? Try single layer LM\\_head.\n\nsee @391</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-06T04:09:43Z","content":"<md>I’ve seen several cases that make the performance a little bit off.\n\n1.  Do you use Softmax and CrossEntropy at the same time ? If you use CrossEntropy, don’t use Softmax\n    \n2.  Do you use 2 nn.Layer()s and ReLu for your LM\\_head ? Try single layer LM\\_head.\n\nsee @391</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m35d1afgxbg2fg","config":{"editor":"md"},"is_tag_endorse":false},{"anon":"stud","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"I am only using crossentropy and a single LM head. I consistently get ~220 perplexity at 500 iterations.","created":"2024-11-06T04:22:35Z","bucket_order":4,"bucket_name":"Yesterday","type":"followup","tag_good":[],"uid_a":"a_0","children":[{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>What about learning rate?</md>","created":"2024-11-06T04:25:11Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m35dl6xxob613m","d-bucket":"Yesterday","updated":"2024-11-06T04:25:11Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I am using the default hyperparameters (learning rate of 1e-3).","created":"2024-11-06T04:26:16Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m35dml1bi8t4aw","d-bucket":"Yesterday","updated":"2024-11-06T04:26:16Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>how do you reshape your output of lm_head ? I mean, when you use CrossEntorpyLoss(x, y), what is the shape of x?</md>","created":"2024-11-06T04:28:26Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m35dpdb3v2f16n","d-bucket":"Yesterday","updated":"2024-11-06T04:28:26Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I am flattening x and y to be [512, 5755] and [512]","created":"2024-11-06T04:32:02Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m35du05qb9v432","d-bucket":"Yesterday","updated":"2024-11-06T04:32:02Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Where B = 16, T = 32, and B x T = 512.","created":"2024-11-06T04:34:21Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m35dwz2p59u705","d-bucket":"Yesterday","updated":"2024-11-06T04:34:21Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Yeah that seems correct. Sorry it's hard to see what's the problem given this information.</md>","created":"2024-11-06T04:35:03Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m35dxvoppny1y4","d-bucket":"Yesterday","updated":"2024-11-06T04:35:03Z","config":{"editor":"md"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Are you using pre-Norm or post-Norm?</md>","created":"2024-11-06T04:35:28Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m35dyeu0hnc31f","d-bucket":"Yesterday","updated":"2024-11-06T04:35:28Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I have tried None, Pre, Post, and both and I am still in the 205-230 range.","created":"2024-11-06T04:36:59Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m35e0cww8g82em","d-bucket":"Yesterday","updated":"2024-11-06T04:36:59Z","config":{"editor":"rte"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>I see, I think we need to see your code then. If the implementation is correct, we will not deduct any points because of the train perplexity.</md>","created":"2024-11-06T04:40:16Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m35e4ksirky645","d-bucket":"Yesterday","updated":"2024-11-06T04:40:16Z","config":{"editor":"md"}},{"anon":"no","folders":[],"data":{"embed_links":null},"subject":"<md>Are you using eval_iters=200 or 100?</md>","created":"2024-11-06T04:40:58Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid":"ku2mpmjms7n645","children":[],"tag_good_arr":[],"id":"m35e5hbk7v9e","d-bucket":"Yesterday","updated":"2024-11-06T04:41:39Z","config":{"editor":"md"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I am using eval_iters=200 for the test sets but am checking perplexity on the train set with eval_interval=100.","created":"2024-11-06T04:44:25Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m35e9x2wni54b2","d-bucket":"Yesterday","updated":"2024-11-06T04:44:25Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"There was an erroneous addition from a previous version of my project in my attention function. I am now consistently getting under 200. Thank you for your help","created":"2024-11-06T04:54:38Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m35en2e7mlg14t","d-bucket":"Yesterday","updated":"2024-11-06T04:54:38Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Actually, the solution was changing the module initialization to be based on the normal distribution instead of a custom one.","created":"2024-11-06T05:39:33Z","bucket_order":3,"bucket_name":"Today","type":"feedback","tag_good":[{"role":"ta","name":"Bosung Kim","endorser":{},"admin":true,"photo":null,"id":"ku2mpmjms7n645","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Yutian Shi","endorser":{},"admin":false,"photo":null,"id":"kfsi52ar6572xo","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_0","children":[],"tag_good_arr":["ku2mpmjms7n645","kfsi52ar6572xo"],"id":"m35g8tckpqx5wj","updated":"2024-11-06T18:06:14Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I also encountered the the similar problem, could you elaborate more on &#34;changing the module initialization to be based on normal distribution&#34;? Which part are you talking about?","created":"2024-11-06T06:41:19Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_1","children":[],"tag_good_arr":[],"id":"m35ig8ycutp7ky","d-bucket":"Yesterday","updated":"2024-11-06T06:41:19Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"I used nn.init to initialize the parameters of my model using a normal distribution with a standard deviation of 0.02.","created":"2024-11-06T07:01:16Z","bucket_order":4,"bucket_name":"Yesterday","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m35j5x5m4aa593","d-bucket":"Yesterday","updated":"2024-11-06T07:01:16Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m35dhu2a3cq7pi","d-bucket":"Yesterday","updated":"2024-11-06T07:01:16Z","config":{"editor":"rte"}},{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-11-06T20:54:24Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"","created":"2024-11-06T20:55:02Z","content":"For reference, I fixed this by using nn.init to initialize my transformer&#39;s weights with a normal distribution with a standard deviation of 0.02"},{"anon":"stud","uid_a":"a_0","subject":"","created":"2024-11-06T20:54:24Z","content":"For reference, I fixed this by using nn.init to initialize my transformer&#39;s weights with a normal distribution as done in &#34;General Adversial Networks&#34; by GoodFellow et al."}],"type":"s_answer","tag_endorse_arr":[],"children":[],"id":"m36cxc0vhhw3nb","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":["m182yjcbe9m3ke"],"no_answer":0,"id":"m358ld2mupp1n2","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990409931,"default_anonymity":"no"},"error":null,"aid":"m3nyddbzgj5282"}