{"result":{"history_size":1,"folders":["pa2"],"nr":251,"data":{"embed_links":[]},"created":"2024-10-25T20:48:51Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2p7fyt6ueg6ko","v":"all","type":"create","when":"2024-10-25T20:48:51Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2plytemslf6yy","to":"m2p7fyswi1i6kn","type":"i_answer","when":"2024-10-26T03:35:25Z"},{"anon":"no","uid":"lmpga1m6ftd62m","data":"m2pr1zauacq37v","type":"i_answer_update","when":"2024-10-26T05:57:51Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Confused on the batch size","created":"2024-10-25T20:48:51Z","content":"Do we send the entire batch to the attention heads or do we send one sentence at a time? Our batch size is 16, which means we are sending 16 sentences into the Transformer encoder. Now, do we send the entire batch to the Multi-head attention or do we send one by one?"}],"type":"question","tags":["pa2","student"],"tag_good":[{"role":"student","name":"Rahul Sharma Nemmani","endorser":{},"admin":false,"photo":null,"id":"jml95ci0otv4w9","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Madurya Suresh","endorser":{},"admin":false,"photo":null,"id":"m182yu2idsy4f6","photo_url":null,"us":false,"facebook_id":null}],"unique_views":113,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-10-26T03:35:25Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"lmpga1m6ftd62m","subject":"","created":"2024-10-26T05:57:51Z","content":"<md>Do not send one by one. Send 16 sentences at once, which means your input tensors are 3-D tensors with a shape of  \n(16, max_sequence_length (=block_size) , word_embedding_dim_size).\n=> (16, 32, 64) following the hyper parameters in main.py\n\nNote that, the initial input tensor (before word embedding) looks like (16, 32)\nand pass this to word_embedding, you will get (16, 32, 64)\nthen this tensor is the input of Transformer encoder.</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-26T03:35:25Z","content":"<md>Do not send one by one. Send 16 sentences at once, which means your input tensors are 3-D tensors with a shape of  \n(16, max_sequence_length (=block_size) , word_embedding_dim_size).\n=> (16, 32, 64) following the hyper parameters in main.py\n\nNote that, the initial input tensor (before word embedding) looks like (16, 32)\nand pass this to word_embedding, you will get (16, 32, 64)\nthen this tensor is the input of Transformer encoder.</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2plytegv7o6yx","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":["jml95ci0otv4w9","m182yu2idsy4f6"],"no_answer":0,"id":"m2p7fyswi1i6kn","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990259161,"default_anonymity":"no"},"error":null,"aid":"m3nya4zxtbz7dt"}