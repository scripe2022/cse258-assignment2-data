{"result":{"history_size":2,"folders":["pa2"],"nr":415,"data":{"embed_links":[]},"created":"2024-11-06T19:04:32Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m36901nreof37k","v":"all","type":"create","when":"2024-11-06T19:04:32Z","uid_a":"a_0"},{"anon":"stud","data":"m36960hsbdf55","v":"all","type":"update","when":"2024-11-06T19:09:11Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m36kkkxv88p5fj","to":"m36901njneu37i","type":"i_answer","when":"2024-11-07T00:28:26Z"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m36kl4amsc661k","type":"i_answer_update","when":"2024-11-07T00:28:51Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"PA2: Doubt in AliBi","created":"2024-11-06T19:09:11Z","content":"<p>Hello I have read the paper and implemented the AliBi PE as the equation in the paper:<br /><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Flmp3u162ds72qp%2Ff53dc80c6c7d25e9ba9ae8c97bcfd1db84cc1d2195c78e3842162f12baccd076%2Fimage.png\" alt=\"image.png\" /><br /><br />The public implementation however does:<br />```</p>\n<p>        maxpos = args.tokens_per_sample<br />        attn_heads = args.decoder_attention_heads<br />        self.slopes = torch.Tensor(get_slopes(attn_heads))<br />        self.alibi = self.slopes.unsqueeze(1).unsqueeze(1) * torch.arange(maxpos).unsqueeze(0).unsqueeze(0).expand(attn_heads, -1, -1)<br />        self.alibi = self.alibi.view(attn_heads, 1, maxpos)<br />        self.alibi = self.alibi.repeat(args.max_tokens//maxpos, 1, 1)  # batch_size, 1, 1</p>\n<p>```<br />The outputs of my implementation and public implementation match when:<br />public_implementation = -1 * my_implementation.flip(-1) <br /><br />Essentially the public implementation is same as m * [0,1,...(i-1)] from what I understand (I could be very much off).<br /><br />i.e. If we reverse the arrange in my implementation and remove the -1 multiplier.<br />However, I believe my implementation is accurate to the paper?<br /><br />Am I doing/understanding something wrong?</p>"},{"anon":"stud","uid_a":"a_0","subject":"PA2: Doubt in AliBi","created":"2024-11-06T19:04:32Z","content":"<p>Hello I have read the paper and implemented the AliBi PE as the equation in the paper:<br /><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Flmp3u162ds72qp%2Ff53dc80c6c7d25e9ba9ae8c97bcfd1db84cc1d2195c78e3842162f12baccd076%2Fimage.png\" alt=\"image.png\" /><br /><br />The public implementation however does:<br />```</p>\n<p>        maxpos = args.tokens_per_sample<br />        attn_heads = args.decoder_attention_heads<br />        self.slopes = torch.Tensor(get_slopes(attn_heads))<br />        self.alibi = self.slopes.unsqueeze(1).unsqueeze(1) * torch.arange(maxpos).unsqueeze(0).unsqueeze(0).expand(attn_heads, -1, -1)<br />        self.alibi = self.alibi.view(attn_heads, 1, maxpos)<br />        self.alibi = self.alibi.repeat(args.max_tokens//maxpos, 1, 1)  # batch_size, 1, 1</p>\n<p>```<br />The outputs of my implementation and public implementation match when:<br />public_implementation = -1 * my_implementation.flip(-1) <br />i.e. If we reverse the arrange in my implementation and remove the -1 multiplier.<br />However, I believe my implementation is accurate to the paper?<br /><br />Am I doing/understanding something wrong?</p>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":127,"children":[{"history_size":2,"folders":[],"data":{"embed_links":[]},"created":"2024-11-07T00:28:26Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-07T00:28:51Z","content":"<md>Hi, sorry we can't confirm if your implementation is correct or not given this information only. \n\nBut it seems flipping and * -1 will give same effects since what AliBi actually does is adding bias that increases linearly with the distance between the tokens? But as mentioned, not sure your implementation is correct or not.</md>"},{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-11-07T00:28:26Z","content":"<md>Hi, sorry we can't confirm if your implementation is correct or not given this information only. \n\nBut it seems flipping and * -1 will give same effects since what AliBi actually does is adding bias that increases linearly with the distance between the tokens?</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m36kkkxpd505fh","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m36901njneu37i","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990421309,"default_anonymity":"no"},"error":null,"aid":"m3nydm40hz05xj"}