{"result":{"history_size":1,"folders":["pa2"],"nr":247,"data":{"embed_links":[]},"created":"2024-10-25T18:41:28Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2p2w5pgvsdr","v":"all","type":"create","when":"2024-10-25T18:41:28Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2p3cifobyz1aj","to":"m2p2w5p9lidq","type":"i_answer","when":"2024-10-25T18:54:11Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Part 1.1","created":"2024-10-25T18:41:28Z","content":"In part 1.1, we need both word embedding and position embedding. Absolute positional embedding is for position embedding? And which method should we use to get the word embedding?Â "}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":166,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-25T18:54:11Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Yueqi Wu","endorser":{},"admin":false,"photo":null,"id":"m182yn6qh6g3ui","photo_url":null,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-25T18:54:11Z","content":"<md>Yes, use Absolute positional embedding.\n\nFor the word embedding, use `torch.nn.Embedding()` which gives you randomly initialized word vectors.</md>"}],"type":"i_answer","tag_endorse_arr":["m182yn6qh6g3ui"],"children":[],"id":"m2p3cifgc4e1ai","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2p2w5p9lidq","config":{"has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990254111,"default_anonymity":"no"},"error":null,"aid":"m3nya13mryba8"}