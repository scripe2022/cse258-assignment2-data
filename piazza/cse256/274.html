{"result":{"history_size":2,"folders":["pa2"],"nr":274,"data":{"embed_links":[]},"created":"2024-10-27T17:48:07Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2rvv8xb7kc4tb","v":"all","type":"create","when":"2024-10-27T17:48:07Z","uid_a":"a_0"},{"anon":"stud","data":"m2rvw334tq6684","v":"all","type":"update","when":"2024-10-27T17:48:46Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2s2p1h49na1gv","to":"m2rvv8x4tu74t9","type":"i_answer","when":"2024-10-27T20:59:15Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Decoder input output","created":"2024-10-27T17:48:46Z","content":"The LanguageModelingDataset returns the input sequence and the right shifted input sequence. According to the diagram, there is a token embedding applied to the input and after adding it with the positional encoding, we pass it to the encoder. Then do we have to use the output (the right shifted sequence) and use token embedding on it and add it with the positional encoding before passing it to the decoder? Do the token embeddings have to be the same in this case?"},{"anon":"stud","uid_a":"a_0","subject":"Decoder input output","created":"2024-10-27T17:48:07Z","content":"The LanguageModelingDataset returns the input sequence and the right shifted input sequence. According to the diagram, there is a token embedding applied to the input and after adding it with the positional encoding, we pass it to the encoder. Then do we have to use the output (the right shifted sequence) and use token embedding on it and add it with the positional encoding before passing it to the decoder?"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":136,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-27T20:59:15Z","bucket_order":3,"tag_endorse":[{"role":"student","name":"Rami Altai","endorser":{},"admin":false,"photo":null,"id":"lml4g3u9lv41ee","photo_url":null,"us":false,"facebook_id":null}],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-27T20:59:15Z","content":"<md>In Part 2, you only use Decoder. Encoder is not used in Part 2.\n\nThe decoder is a transformer decoder-only model, NOT the decoder of Transformer encoder-decoder model. So the decoder does not need cross attention in this project.\n\nThis might sound confusing, but this image could be helpful:\n![Decoder-only transformers are just the decoder portion of the transformer architecture! However, the cross attention portion of the decoder is removed due to the lack of an encoder. This is because we can’t attend to an encoder that doesn’t exist! [3/6] ](https://pbs.twimg.com/media/FsQH_gTaQAAL1_7.jpg)\n\nalso i think this post would be helpful:\nhttps://en.rattibha.com/thread/1640446114519474176</md>"}],"type":"i_answer","tag_endorse_arr":["lml4g3u9lv41ee"],"children":[],"id":"m2s2p1gwmc1gu","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2rvv8x4tu74t9","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":1,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990281903,"default_anonymity":"no"},"error":null,"aid":"m3nyamjmz2v2o"}