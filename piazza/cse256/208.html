{"result":{"history_size":1,"folders":["pa1"],"nr":208,"data":{"embed_links":[]},"created":"2024-10-20T04:41:27Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2h3omo0saq2el","v":"all","type":"create","when":"2024-10-20T04:41:27Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2h7dwnzikn19u","to":"m2h3omntatv2ek","type":"i_answer","when":"2024-10-20T06:25:05Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Embedding Size for BPE","created":"2024-10-20T04:41:27Z","content":"Hi, do we need to maintain the same embedding dimensions in the embedding layer for the BPE model as well? Or can we change the number of dimensions as compared to the previous questions?"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":161,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-20T06:25:05Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-20T06:25:05Z","content":"It would be ideal to keep the same embedding dimensions and model settings for the BPE model as well. This allows for a better comparison between word-level and sub-word tokenization with the other settings remaining consistent."}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2h7dwnrlhh19t","config":{"editor":"rte"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2h3omntatv2ek","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990222409,"default_anonymity":"no"},"error":null,"aid":"m3ny9cn2gp062e"}