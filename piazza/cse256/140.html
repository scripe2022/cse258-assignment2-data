{"result":{"history_size":2,"folders":["pa1"],"nr":140,"data":{"embed_links":[]},"created":"2024-10-16T23:23:15Z","bucket_order":3,"no_answer_followup":2,"change_log":[{"anon":"stud","data":"m2chzuzg81r2g","v":"private","type":"create","when":"2024-10-16T23:23:15Z","uid_a":"a_0"},{"anon":"no","uid":"ln0md59uz9w3kd","data":"m2cj1wdcx5q1hn","to":"m2chzuz8uf12d","type":"i_answer","when":"2024-10-16T23:52:50Z"},{"anon":"no","uid":"ln0md59uz9w3kd","to":"m2chzuz8uf12d","type":"followup","when":"2024-10-16T23:53:23Z","cid":"m2cj2m3js5j2ak"},{"anon":"stud","to":"m2chzuz8uf12d","type":"feedback","when":"2024-10-17T01:07:04Z","cid":"m2clpdoxjju2wk","uid_a":"a_0"},{"anon":"stud","data":"m2clq18ak5c3of","v":"all","type":"update","when":"2024-10-17T01:07:35Z","uid_a":"a_0"},{"anon":"stud","to":"m2chzuz8uf12d","type":"feedback","when":"2024-10-17T03:19:05Z","cid":"m2cqf5h5oie4px","uid_a":"a_0"},{"anon":"no","uid":"m182yt8a22e4cr","to":"m2chzuz8uf12d","type":"followup","when":"2024-10-17T06:15:08Z","cid":"m2cwpjowsdl4f2"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Assignment 1 part 1a","created":"2024-10-17T01:07:35Z","content":"<p>Hi,<br /><br />In part 1a) when i am trying to use get_initialized_embedding_layer, I am getting the following error,<br /><br />&#34;RuntimeError: Expected tensor for argument #1 &#39;indices&#39; to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)&#34;<br /><br />I have defined a custom collate_fn to handle padding by setting the padding value as &lt;PAD&gt; and passing it to the data loader function.</p>\n<p>I have used print statements to debug it too, the input to DAN is showing as float object, and i have appropriately set the indices and label tensors as long.<br />Thanks!</p>"},{"anon":"stud","uid_a":"a_0","subject":"Assignment 1 part 1a","created":"2024-10-16T23:23:15Z","content":"<p>Hi,<br /><br />In part 1a) when i am trying to use get_initialized_embedding_layer, I am getting the following error,<br /><br />&#34;RuntimeError: Expected tensor for argument #1 &#39;indices&#39; to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)&#34;<br /><br />I have defined a custom collate_fn to handle padding by setting the padding value as &lt;PAD&gt; and passing it to the data loader function.<br /></p>\n<p>I have used print statements to debug it too, the input to DAN is showing as float object, and i have appropriately set the indices and label tensors as long.<br />Thanks!</p>"}],"type":"question","tags":["pa1","student"],"tag_good":[],"unique_views":129,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-16T23:52:50Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ln0md59uz9w3kd","subject":"","created":"2024-10-16T23:52:50Z","content":"Perhaps try converting float to int or long inside the forward function."}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2cj1wd8p1a1hm","config":{},"is_tag_endorse":false},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<p>Also, would you mind making this post public instead of private (you can choose to remain anonymous to your classmates)? Others may have similar questions, and it could be helpful for them as well. </p><br /><p> </p><br /><p>Please only send us private messages when personal information is shared.</p><br /><p> </p><br /><p>Thanks!</p>","created":"2024-10-16T23:53:23Z","bucket_order":6,"bucket_name":"Last week","type":"followup","tag_good":[],"uid":"ln0md59uz9w3kd","children":[{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Thanks! Also i tried to run the model with get_initialized_embedding_layer and it worked perfectly with dev accuracy above 77%, can i implement it that way or do i need get_initialized_embedding_layer?","created":"2024-10-17T01:07:04Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[],"uid_a":"a_0","children":[],"tag_good_arr":[],"id":"m2clpdoxjju2wk","updated":"2024-10-17T01:07:04Z","config":{"editor":"rte"}},{"anon":"stud","folders":[],"data":{"embed_links":null},"subject":"Nevermind, i was able to resolve this!","created":"2024-10-17T03:19:05Z","bucket_order":6,"bucket_name":"Last week","type":"feedback","tag_good":[{"role":"ta","name":"Po-Chun Wu","endorser":{},"admin":true,"photo":null,"id":"ln0md59uz9w3kd","photo_url":null,"published":true,"us":false,"facebook_id":null}],"uid_a":"a_0","children":[],"tag_good_arr":["ln0md59uz9w3kd"],"id":"m2cqf5h5oie4px","updated":"2024-10-17T03:30:39Z","config":{"editor":"rte"}}],"tag_good_arr":[],"no_answer":1,"id":"m2cj2m3js5j2ak","updated":"2024-10-17T03:19:05Z","config":{}},{"anon":"no","folders":[],"data":{"embed_links":null},"no_upvotes":0,"subject":"<md>I'll second that the instructor answer works. For some reason that I'm not entirely sure of, sometimes the input vector `x` to the `forward` function ends up being a tensor of floats, at least in my code. The embedding layer can only handle ints or longs, which is why that error occurs. I just cast `x` to a tensor of ints at the start of the `forward` function to get around this.</md>","created":"2024-10-17T06:15:08Z","bucket_order":3,"bucket_name":"Today","type":"followup","tag_good":[{"role":"ta","name":"Po-Chun Wu","endorser":{},"admin":true,"photo":null,"id":"ln0md59uz9w3kd","photo_url":null,"published":true,"us":false,"facebook_id":null},{"role":"student","name":"Don Le","endorser":{},"admin":false,"photo":null,"id":"kfohh6rkd2u74r","photo_url":null,"us":false,"facebook_id":null}],"uid":"m182yt8a22e4cr","children":[],"tag_good_arr":["ln0md59uz9w3kd","kfohh6rkd2u74r"],"no_answer":1,"id":"m2cwpjowsdl4f2","updated":"2024-10-20T20:54:58Z","config":{"editor":"md"}}],"tag_good_arr":[],"no_answer":0,"id":"m2chzuz8uf12d","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":3,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990150176,"default_anonymity":"no"},"error":null,"aid":"m3ny7swiltq6v2"}