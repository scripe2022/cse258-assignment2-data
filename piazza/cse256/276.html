{"result":{"history_size":1,"folders":["pa2"],"nr":276,"data":{"embed_links":[]},"created":"2024-10-27T18:35:37Z","bucket_order":3,"no_answer_followup":0,"change_log":[{"anon":"stud","data":"m2rxkbyefeeqo","v":"all","type":"create","when":"2024-10-27T18:35:37Z","uid_a":"a_0"},{"anon":"no","uid":"ku2mpmjms7n645","data":"m2s2r5chq7e4xz","to":"m2rxkby8rsvqn","type":"i_answer","when":"2024-10-27T21:00:53Z"}],"bucket_name":"Today","history":[{"anon":"stud","uid_a":"a_0","subject":"Confusion about the Decoder","created":"2024-10-27T18:35:37Z","content":"<p>I have a few questions about the implementation of part 2:<br /><br /></p>\n<ol><li>Do we have to train the encoder and the decoder together in part 2?</li><li>Andrej Karpathy&#39;s code does not use the encoder output for cross attention, but we are supposed to use the encoder output as the keys and value frmo cross attention in the decoder, right?</li><li>Are we supposed to use different inputs to the encoder and decoder as given the transformer architecture diagram?</li></ol>"}],"type":"question","tags":["pa2","student"],"tag_good":[],"unique_views":119,"children":[{"history_size":1,"folders":[],"data":{"embed_links":[]},"created":"2024-10-27T21:00:53Z","bucket_order":3,"tag_endorse":[],"bucket_name":"Today","history":[{"anon":"no","uid":"ku2mpmjms7n645","subject":"","created":"2024-10-27T21:00:53Z","content":"<md>@274</md>"}],"type":"i_answer","tag_endorse_arr":[],"children":[],"id":"m2s2r5cb2y84xy","config":{"editor":"md"},"is_tag_endorse":false}],"tag_good_arr":[],"no_answer":0,"id":"m2rxkby8rsvqn","config":{"editor":"rte","has_emails_sent":1},"status":"active","drafts":{},"request_instructor":0,"request_instructor_me":false,"bookmarked":2,"num_favorites":0,"my_favorite":false,"is_bookmarked":false,"is_tag_good":false,"q_edits":[],"i_edits":[],"s_edits":[],"t":1731990284417,"default_anonymity":"no"},"error":null,"aid":"m3nyaohhov826y"}